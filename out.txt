2025-05-06 13:47:13 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
usage: corenet-eval [-h] [--common.s3.endpoint-url COMMON.S3.ENDPOINT_URL]
                    [--common.s3.aws-access-key-id COMMON.S3.AWS_ACCESS_KEY_ID]
                    [--common.s3.aws-secret-access-key COMMON.S3.AWS_SECRET_ACCESS_KEY]
                    [--common.s3.aws-session-token COMMON.S3.AWS_SESSION_TOKEN]
                    [--common.s3.multipart-chunksize COMMON.S3.MULTIPART_CHUNKSIZE]
                    [--dataset.root-train DATASET.ROOT_TRAIN]
                    [--dataset.pcap-collate-mode DATASET.PCAP_COLLATE_MODE]
                    [--dataset.root-val DATASET.ROOT_VAL]
                    [--dataset.root-test DATASET.ROOT_TEST]
                    [--dataset.disable-val] [--dataset.name DATASET.NAME]
                    [--dataset.category DATASET.CATEGORY]
                    [--dataset.percentage-of-samples DATASET.PERCENTAGE_OF_SAMPLES]
                    [--dataset.sample-selection-random-seed DATASET.SAMPLE_SELECTION_RANDOM_SEED]
                    [--dataset.train-batch-size0 DATASET.TRAIN_BATCH_SIZE0]
                    [--dataset.val-batch-size0 DATASET.VAL_BATCH_SIZE0]
                    [--dataset.eval-batch-size0 DATASET.EVAL_BATCH_SIZE0]
                    [--dataset.workers DATASET.WORKERS]
                    [--dataset.persistent-workers] [--dataset.pin-memory]
                    [--dataset.prefetch-factor DATASET.PREFETCH_FACTOR]
                    [--dataset.padding-index DATASET.PADDING_INDEX]
                    [--dataset.text-vocab-size DATASET.TEXT_VOCAB_SIZE]
                    [--dataset.text-context-length DATASET.TEXT_CONTEXT_LENGTH]
                    [--evaluation.segmentation.apply-color-map]
                    [--evaluation.segmentation.save-overlay-rgb-pred]
                    [--evaluation.segmentation.save-masks]
                    [--evaluation.segmentation.overlay-mask-weight EVALUATION.SEGMENTATION.OVERLAY_MASK_WEIGHT]
                    [--evaluation.segmentation.mode {single_image,image_folder,validation_set}]
                    [--evaluation.segmentation.path EVALUATION.SEGMENTATION.PATH]
                    [--evaluation.segmentation.resize-input-images]
                    [--evaluation.segmentation.resize-input-images-fixed-size]
                    [--dataset.clips-per-video DATASET.CLIPS_PER_VIDEO]
                    [--dataset.n-frames-per-clip DATASET.N_FRAMES_PER_CLIP]
                    [--dataset.num-samples-per-category DATASET.NUM_SAMPLES_PER_CATEGORY]
                    [--evaluation.detection.save-overlay-boxes]
                    [--evaluation.detection.mode {single_image,image_folder,validation_set}]
                    [--evaluation.detection.path EVALUATION.DETECTION.PATH]
                    [--evaluation.detection.num-classes EVALUATION.DETECTION.NUM_CLASSES]
                    [--evaluation.detection.resize-input-images]
                    [--dataset.language-modeling.sequence-length DATASET.LANGUAGE_MODELING.SEQUENCE_LENGTH]
                    [--dataset.language-modeling.min-tokens-per-text DATASET.LANGUAGE_MODELING.MIN_TOKENS_PER_TEXT]
                    [--dataset.language-modeling.min-characters-per-text DATASET.LANGUAGE_MODELING.MIN_CHARACTERS_PER_TEXT]
                    [--dataset.language-modeling.shuffle-data]
                    [--dataset.language-modeling.random-seed DATASET.LANGUAGE_MODELING.RANDOM_SEED]
                    [--dataset.multi-modal-img-text.zero-shot-img-cls-dataset-name DATASET.MULTI_MODAL_IMG_TEXT.ZERO_SHOT_IMG_CLS_DATASET_NAME]
                    [--dataset.multi-modal-img-text.context-length DATASET.MULTI_MODAL_IMG_TEXT.CONTEXT_LENGTH]
                    [--dataset.multi-modal-img-text.padding-index DATASET.MULTI_MODAL_IMG_TEXT.PADDING_INDEX]
                    [--dataset.multi-modal-img-text.trunc-seq-len]
                    [--dataset.speech-commands-v2.mixup]
                    [--dataset.imagenet-v2.split {matched_frequency,threshold_0.7,top_images}]
                    [--dataset.wordnet-tagged-classification.vocab-file DATASET.WORDNET_TAGGED_CLASSIFICATION.VOCAB_FILE]
                    [--dataset.wordnet-tagged-classification.metadata-file DATASET.WORDNET_TAGGED_CLASSIFICATION.METADATA_FILE]
                    [--dataset.wordnet-tagged-classification.vocab-size DATASET.WORDNET_TAGGED_CLASSIFICATION.VOCAB_SIZE]
                    [--dataset.detection.no-background-id]
                    [--dataset.detection.coco-mask-rcnn.use-lsj-aug]
                    [--dataset.language-modeling.commonsense-170k.path DATASET.LANGUAGE_MODELING.COMMONSENSE_170K.PATH]
                    [--dataset.language-modeling.general-lm.train-data-info DATASET.LANGUAGE_MODELING.GENERAL_LM.TRAIN_DATA_INFO [DATASET.LANGUAGE_MODELING.GENERAL_LM.TRAIN_DATA_INFO ...]]
                    [--dataset.language-modeling.general-lm.val-data-info DATASET.LANGUAGE_MODELING.GENERAL_LM.VAL_DATA_INFO [DATASET.LANGUAGE_MODELING.GENERAL_LM.VAL_DATA_INFO ...]]
                    [--dataset.language-modeling.general-lm.test-data-info DATASET.LANGUAGE_MODELING.GENERAL_LM.TEST_DATA_INFO [DATASET.LANGUAGE_MODELING.GENERAL_LM.TEST_DATA_INFO ...]]
                    [--dataset.language-modeling.general-lm.data-state DATASET.LANGUAGE_MODELING.GENERAL_LM.DATA_STATE [DATASET.LANGUAGE_MODELING.GENERAL_LM.DATA_STATE ...]]
                    [--dataset.language-modeling.general-lm.reader-chunk-size DATASET.LANGUAGE_MODELING.GENERAL_LM.READER_CHUNK_SIZE]
                    [--dataset.language-modeling.general-lm.document-split-size DATASET.LANGUAGE_MODELING.GENERAL_LM.DOCUMENT_SPLIT_SIZE]
                    [--dataset.language-modeling.general-lm.data-state-save-interval DATASET.LANGUAGE_MODELING.GENERAL_LM.DATA_STATE_SAVE_INTERVAL]
                    [--dataset.multi-modal-img-text.img-text-tar.metadata-file DATASET.MULTI_MODAL_IMG_TEXT.IMG_TEXT_TAR.METADATA_FILE]
                    [--dataset.pascal.use-coco-data]
                    [--dataset.pascal.coco-root-dir DATASET.PASCAL.COCO_ROOT_DIR]
                    [--model.text.name MODEL.TEXT.NAME]
                    [--model.text.padding-index MODEL.TEXT.PADDING_INDEX]
                    [--model.text.context-length MODEL.TEXT.CONTEXT_LENGTH]
                    [--model.text.vocab-size MODEL.TEXT.VOCAB_SIZE]
                    [--model.text.transformer.model-dim MODEL.TEXT.TRANSFORMER.MODEL_DIM]
                    [--model.text.transformer.no-scale-embedding]
                    [--model.text.transformer.no-pos-embedding]
                    [--model.text.transformer.embed-dropout MODEL.TEXT.TRANSFORMER.EMBED_DROPOUT]
                    [--model.text.transformer.n-transformer-layers MODEL.TEXT.TRANSFORMER.N_TRANSFORMER_LAYERS]
                    [--model.text.transformer.n-heads-per-layer MODEL.TEXT.TRANSFORMER.N_HEADS_PER_LAYER [MODEL.TEXT.TRANSFORMER.N_HEADS_PER_LAYER ...]]
                    [--model.text.transformer.ffn-multiplier-per-layer MODEL.TEXT.TRANSFORMER.FFN_MULTIPLIER_PER_LAYER [MODEL.TEXT.TRANSFORMER.FFN_MULTIPLIER_PER_LAYER ...]]
                    [--model.text.transformer.attn-dropout MODEL.TEXT.TRANSFORMER.ATTN_DROPOUT]
                    [--model.text.transformer.ffn-dropout MODEL.TEXT.TRANSFORMER.FFN_DROPOUT]
                    [--model.text.transformer.dropout MODEL.TEXT.TRANSFORMER.DROPOUT]
                    [--model.text.transformer.norm-layer MODEL.TEXT.TRANSFORMER.NORM_LAYER]
                    [--model.text.transformer.sinusoidal-pos-emb]
                    [--model.text.transformer.causal-masking]
                    [--model.text.transformer.classes-per-split-zero-shot MODEL.TEXT.TRANSFORMER.CLASSES_PER_SPLIT_ZERO_SHOT]
                    [--model.image-projection-head.name MODEL.IMAGE_PROJECTION_HEAD.NAME]
                    [--model.image-projection-head.lr-multiplier MODEL.IMAGE_PROJECTION_HEAD.LR_MULTIPLIER]
                    [--model.image-projection-head.attention-pool-nchw2nc.num-pos-embeddings MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NUM_POS_EMBEDDINGS]
                    [--model.image-projection-head.attention-pool-nchw2nc.use-sinusoidal-pos-embeddings]
                    [--model.image-projection-head.attention-pool-nchw2nc.num-attn-heads MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NUM_ATTN_HEADS]
                    [--model.image-projection-head.attention-pool-nchw2nc.no-feature-normalization]
                    [--model.image-projection-head.attention-pool-nchw2nc.use-pytorch-mha]
                    [--model.image-projection-head.global-pool-nchw2nc.no-feature-normalization]
                    [--model.image-projection-head.global-pool-nchw2nc.identity-if-same-size]
                    [--model.image-projection-head.simple-projection-nc2nc.no-feature-normalization]
                    [--model.image-projection-head.simple-projection-nc2nc.identity-if-same-size]
                    [--model.resume-exclude-scopes MODEL.RESUME_EXCLUDE_SCOPES]
                    [--model.ignore-missing-scopes MODEL.IGNORE_MISSING_SCOPES]
                    [--model.rename-scopes-map MODEL.RENAME_SCOPES_MAP]
                    [--model.freeze-modules MODEL.FREEZE_MODULES]
                    [--model.activation-checkpointing]
                    [--model.lora.config MODEL.LORA.CONFIG]
                    [--model.lora.use-lora]
                    [--model.audio-classification.name MODEL.AUDIO_CLASSIFICATION.NAME]
                    [--model.audio-classification.pretrained MODEL.AUDIO_CLASSIFICATION.PRETRAINED]
                    [--model.classification.byteformer.dropout MODEL.CLASSIFICATION.BYTEFORMER.DROPOUT]
                    [--model.classification.byteformer.stochastic-dropout MODEL.CLASSIFICATION.BYTEFORMER.STOCHASTIC_DROPOUT]
                    [--model.classification.byteformer.norm-layer {group_norm,layer_norm,layer_norm_nchw,layer_norm_2d,layer_norm_fp32,instance_norm_2d,instance_norm,instance_norm_1d,batch_norm_2d,batch_norm,batch_norm_fp32,batch_norm_1d,batch_norm_3d,sync_batch_norm,sync_batch_norm_fp32,rms_norm}]
                    [--model.classification.byteformer.sinusoidal-pos-emb]
                    [--model.classification.byteformer.use-pytorch-mha]
                    [--model.classification.byteformer.mode {tiny,small,base,huge}]
                    [--model.classification.byteformer.vocab-size MODEL.CLASSIFICATION.BYTEFORMER.VOCAB_SIZE]
                    [--model.classification.byteformer.max-num-tokens MODEL.CLASSIFICATION.BYTEFORMER.MAX_NUM_TOKENS]
                    [--model.classification.byteformer.conv-kernel-size MODEL.CLASSIFICATION.BYTEFORMER.CONV_KERNEL_SIZE]
                    [--model.classification.byteformer.window-sizes [MODEL.CLASSIFICATION.BYTEFORMER.WINDOW_SIZES ...]]
                    [--model.classification.byteformer.window-shifts [MODEL.CLASSIFICATION.BYTEFORMER.WINDOW_SHIFTS ...]]
                    [--model.classification.byteformer.downsample [MODEL.CLASSIFICATION.BYTEFORMER.DOWNSAMPLE ...]]
                    [--model.classification.byteformer.padding-index MODEL.CLASSIFICATION.BYTEFORMER.PADDING_INDEX]
                    [--model.classification.byteformer.dummy-input-token-length MODEL.CLASSIFICATION.BYTEFORMER.DUMMY_INPUT_TOKEN_LENGTH]
                    [--model.classification.classifier-dropout MODEL.CLASSIFICATION.CLASSIFIER_DROPOUT]
                    [--model.classification.name MODEL.CLASSIFICATION.NAME]
                    [--model.classification.n-classes MODEL.CLASSIFICATION.N_CLASSES]
                    [--model.classification.pretrained MODEL.CLASSIFICATION.PRETRAINED]
                    [--model.classification.freeze-batch-norm]
                    [--model.classification.activation.name MODEL.CLASSIFICATION.ACTIVATION.NAME]
                    [--model.classification.activation.inplace]
                    [--model.classification.activation.neg-slope MODEL.CLASSIFICATION.ACTIVATION.NEG_SLOPE]
                    [--model.classification.finetune-pretrained-model]
                    [--model.classification.n-pretrained-classes MODEL.CLASSIFICATION.N_PRETRAINED_CLASSES]
                    [--model.classification.gradient-checkpointing]
                    [--model.classification.enable-layer-wise-lr-decay]
                    [--model.classification.layer-wise-lr-decay-rate MODEL.CLASSIFICATION.LAYER_WISE_LR_DECAY_RATE]
                    [--model.classification.efficientnet.mode {b0,b1,b2,b3,b4,b5,b6,b7}]
                    [--model.classification.efficientnet.stochastic-depth-prob MODEL.CLASSIFICATION.EFFICIENTNET.STOCHASTIC_DEPTH_PROB]
                    [--model.classification.fastvit.variant MODEL.CLASSIFICATION.FASTVIT.VARIANT]
                    [--model.classification.fastvit.inference-mode MODEL.CLASSIFICATION.FASTVIT.INFERENCE_MODE]
                    [--model.classification.fastvit.dropout MODEL.CLASSIFICATION.FASTVIT.DROPOUT]
                    [--model.classification.fastvit.drop-path MODEL.CLASSIFICATION.FASTVIT.DROP_PATH]
                    [--model.classification.fastvit.use-layer-scale MODEL.CLASSIFICATION.FASTVIT.USE_LAYER_SCALE]
                    [--model.classification.fastvit.layer-scale-init-value MODEL.CLASSIFICATION.FASTVIT.LAYER_SCALE_INIT_VALUE]
                    [--model.classification.mobilenetv1.width-multiplier MODEL.CLASSIFICATION.MOBILENETV1.WIDTH_MULTIPLIER]
                    [--model.classification.mobilenetv2.width-multiplier MODEL.CLASSIFICATION.MOBILENETV2.WIDTH_MULTIPLIER]
                    [--model.classification.mobilenetv3.mode {small,large}]
                    [--model.classification.mobilenetv3.width-multiplier MODEL.CLASSIFICATION.MOBILENETV3.WIDTH_MULTIPLIER]
                    [--model.classification.mobileone.variant MODEL.CLASSIFICATION.MOBILEONE.VARIANT]
                    [--model.classification.mobileone.inference-mode MODEL.CLASSIFICATION.MOBILEONE.INFERENCE_MODE]
                    [--model.classification.mit.mode {xx_small,x_small,small}]
                    [--model.classification.mit.attn-dropout MODEL.CLASSIFICATION.MIT.ATTN_DROPOUT]
                    [--model.classification.mit.ffn-dropout MODEL.CLASSIFICATION.MIT.FFN_DROPOUT]
                    [--model.classification.mit.dropout MODEL.CLASSIFICATION.MIT.DROPOUT]
                    [--model.classification.mit.transformer-norm-layer MODEL.CLASSIFICATION.MIT.TRANSFORMER_NORM_LAYER]
                    [--model.classification.mit.no-fuse-local-global-features]
                    [--model.classification.mit.conv-kernel-size MODEL.CLASSIFICATION.MIT.CONV_KERNEL_SIZE]
                    [--model.classification.mit.head-dim MODEL.CLASSIFICATION.MIT.HEAD_DIM]
                    [--model.classification.mit.number-heads MODEL.CLASSIFICATION.MIT.NUMBER_HEADS]
                    [--model.classification.mitv2.attn-dropout MODEL.CLASSIFICATION.MITV2.ATTN_DROPOUT]
                    [--model.classification.mitv2.ffn-dropout MODEL.CLASSIFICATION.MITV2.FFN_DROPOUT]
                    [--model.classification.mitv2.dropout MODEL.CLASSIFICATION.MITV2.DROPOUT]
                    [--model.classification.mitv2.width-multiplier MODEL.CLASSIFICATION.MITV2.WIDTH_MULTIPLIER]
                    [--model.classification.mitv2.attn-norm-layer MODEL.CLASSIFICATION.MITV2.ATTN_NORM_LAYER]
                    [--model.classification.regnet.mode MODEL.CLASSIFICATION.REGNET.MODE]
                    [--model.classification.regnet.stochastic-depth-prob MODEL.CLASSIFICATION.REGNET.STOCHASTIC_DEPTH_PROB]
                    [--model.classification.regnet.stem-width MODEL.CLASSIFICATION.REGNET.STEM_WIDTH]
                    [--model.classification.resnet.depth MODEL.CLASSIFICATION.RESNET.DEPTH]
                    [--model.classification.resnet.dropout MODEL.CLASSIFICATION.RESNET.DROPOUT]
                    [--model.classification.resnet.stochastic-depth-prob MODEL.CLASSIFICATION.RESNET.STOCHASTIC_DEPTH_PROB]
                    [--model.classification.resnet.se-resnet]
                    [--model.classification.swin.mode MODEL.CLASSIFICATION.SWIN.MODE]
                    [--model.classification.swin.stochastic-depth-prob MODEL.CLASSIFICATION.SWIN.STOCHASTIC_DEPTH_PROB]
                    [--model.classification.swin.extract-end-point-format {nchw,nhwc}]
                    [--model.classification.vit.mode {tiny,small,base,large,huge}]
                    [--model.classification.vit.dropout MODEL.CLASSIFICATION.VIT.DROPOUT]
                    [--model.classification.vit.stochastic-dropout MODEL.CLASSIFICATION.VIT.STOCHASTIC_DROPOUT]
                    [--model.classification.vit.norm-layer MODEL.CLASSIFICATION.VIT.NORM_LAYER]
                    [--model.classification.vit.sinusoidal-pos-emb]
                    [--model.classification.vit.no-cls-token]
                    [--model.classification.vit.use-simple-fpn]
                    [--model.classification.vit.use-flash-attention]
                    [--model.detection.name MODEL.DETECTION.NAME]
                    [--model.detection.n-classes MODEL.DETECTION.N_CLASSES]
                    [--model.detection.pretrained MODEL.DETECTION.PRETRAINED]
                    [--model.detection.output-stride MODEL.DETECTION.OUTPUT_STRIDE]
                    [--model.detection.replace-stride-with-dilation]
                    [--model.detection.freeze-batch-norm]
                    [--model.detection.mask-rcnn.backbone-projection-channels MODEL.DETECTION.MASK_RCNN.BACKBONE_PROJECTION_CHANNELS]
                    [--model.detection.mask-rcnn.backbone-lr-multiplier MODEL.DETECTION.MASK_RCNN.BACKBONE_LR_MULTIPLIER]
                    [--model.detection.mask-rcnn.output-strides MODEL.DETECTION.MASK_RCNN.OUTPUT_STRIDES [MODEL.DETECTION.MASK_RCNN.OUTPUT_STRIDES ...]]
                    [--model.detection.mask-rcnn.anchor-sizes MODEL.DETECTION.MASK_RCNN.ANCHOR_SIZES [MODEL.DETECTION.MASK_RCNN.ANCHOR_SIZES ...]]
                    [--model.detection.mask-rcnn.aspect-ratio MODEL.DETECTION.MASK_RCNN.ASPECT_RATIO [MODEL.DETECTION.MASK_RCNN.ASPECT_RATIO ...]]
                    [--model.detection.mask-rcnn.bbox-head-fm-size MODEL.DETECTION.MASK_RCNN.BBOX_HEAD_FM_SIZE]
                    [--model.detection.mask-rcnn.mask-head-fm-size MODEL.DETECTION.MASK_RCNN.MASK_HEAD_FM_SIZE]
                    [--model.detection.mask-rcnn.representation-size MODEL.DETECTION.MASK_RCNN.REPRESENTATION_SIZE]
                    [--model.detection.mask-rcnn.box-fm-size-conv-layer MODEL.DETECTION.MASK_RCNN.BOX_FM_SIZE_CONV_LAYER [MODEL.DETECTION.MASK_RCNN.BOX_FM_SIZE_CONV_LAYER ...]]
                    [--model.detection.mask-rcnn.mask-fm-size-conv-layer MODEL.DETECTION.MASK_RCNN.MASK_FM_SIZE_CONV_LAYER [MODEL.DETECTION.MASK_RCNN.MASK_FM_SIZE_CONV_LAYER ...]]
                    [--model.detection.mask-rcnn.mask-dilation MODEL.DETECTION.MASK_RCNN.MASK_DILATION]
                    [--model.detection.mask-rcnn.rpn-pre-nms-top-n-train MODEL.DETECTION.MASK_RCNN.RPN_PRE_NMS_TOP_N_TRAIN]
                    [--model.detection.mask-rcnn.rpn-pre-nms-top-n-test MODEL.DETECTION.MASK_RCNN.RPN_PRE_NMS_TOP_N_TEST]
                    [--model.detection.mask-rcnn.rpn-post-nms-top-n-train MODEL.DETECTION.MASK_RCNN.RPN_POST_NMS_TOP_N_TRAIN]
                    [--model.detection.mask-rcnn.rpn-post-nms-top-n-test MODEL.DETECTION.MASK_RCNN.RPN_POST_NMS_TOP_N_TEST]
                    [--model.detection.mask-rcnn.rpn-nms-thresh MODEL.DETECTION.MASK_RCNN.RPN_NMS_THRESH]
                    [--model.detection.mask-rcnn.rpn-fg-iou-thresh MODEL.DETECTION.MASK_RCNN.RPN_FG_IOU_THRESH]
                    [--model.detection.mask-rcnn.rpn-bg-iou-thresh MODEL.DETECTION.MASK_RCNN.RPN_BG_IOU_THRESH]
                    [--model.detection.mask-rcnn.rpn-batch-size-per-image MODEL.DETECTION.MASK_RCNN.RPN_BATCH_SIZE_PER_IMAGE]
                    [--model.detection.mask-rcnn.rpn-positive-fraction MODEL.DETECTION.MASK_RCNN.RPN_POSITIVE_FRACTION]
                    [--model.detection.mask-rcnn.rpn-score-thresh MODEL.DETECTION.MASK_RCNN.RPN_SCORE_THRESH]
                    [--model.detection.mask-rcnn.box-score-thresh MODEL.DETECTION.MASK_RCNN.BOX_SCORE_THRESH]
                    [--model.detection.mask-rcnn.box-nms-thresh MODEL.DETECTION.MASK_RCNN.BOX_NMS_THRESH]
                    [--model.detection.mask-rcnn.box-detections-per-img MODEL.DETECTION.MASK_RCNN.BOX_DETECTIONS_PER_IMG]
                    [--model.detection.mask-rcnn.box-fg-iou-thresh MODEL.DETECTION.MASK_RCNN.BOX_FG_IOU_THRESH]
                    [--model.detection.mask-rcnn.box-bg-iou-thresh MODEL.DETECTION.MASK_RCNN.BOX_BG_IOU_THRESH]
                    [--model.detection.mask-rcnn.box-batch-size-per-image MODEL.DETECTION.MASK_RCNN.BOX_BATCH_SIZE_PER_IMAGE]
                    [--model.detection.mask-rcnn.box-positive-fraction MODEL.DETECTION.MASK_RCNN.BOX_POSITIVE_FRACTION]
                    [--model.detection.mask-rcnn.norm-layer MODEL.DETECTION.MASK_RCNN.NORM_LAYER]
                    [--model.detection.mask-rcnn.disable-fpn]
                    [--model.detection.ssd.anchors-aspect-ratio MODEL.DETECTION.SSD.ANCHORS_ASPECT_RATIO [MODEL.DETECTION.SSD.ANCHORS_ASPECT_RATIO ...]]
                    [--model.detection.ssd.output-strides MODEL.DETECTION.SSD.OUTPUT_STRIDES [MODEL.DETECTION.SSD.OUTPUT_STRIDES ...]]
                    [--model.detection.ssd.proj-channels MODEL.DETECTION.SSD.PROJ_CHANNELS [MODEL.DETECTION.SSD.PROJ_CHANNELS ...]]
                    [--model.detection.ssd.min-box-size MODEL.DETECTION.SSD.MIN_BOX_SIZE]
                    [--model.detection.ssd.max-box-size MODEL.DETECTION.SSD.MAX_BOX_SIZE]
                    [--model.detection.ssd.center-variance MODEL.DETECTION.SSD.CENTER_VARIANCE]
                    [--model.detection.ssd.size-variance MODEL.DETECTION.SSD.SIZE_VARIANCE]
                    [--model.detection.ssd.iou-threshold MODEL.DETECTION.SSD.IOU_THRESHOLD]
                    [--model.detection.ssd.conf-threshold MODEL.DETECTION.SSD.CONF_THRESHOLD]
                    [--model.detection.ssd.top-k MODEL.DETECTION.SSD.TOP_K]
                    [--model.detection.ssd.objects-per-image MODEL.DETECTION.SSD.OBJECTS_PER_IMAGE]
                    [--model.detection.ssd.nms-iou-threshold MODEL.DETECTION.SSD.NMS_IOU_THRESHOLD]
                    [--model.detection.ssd.fpn-out-channels MODEL.DETECTION.SSD.FPN_OUT_CHANNELS]
                    [--model.detection.ssd.use-fpn]
                    [--model.language-modeling.name MODEL.LANGUAGE_MODELING.NAME]
                    [--model.language-modeling.pretrained MODEL.LANGUAGE_MODELING.PRETRAINED]
                    [--model.language-modeling.general-gpt.model-name {gpt-test,gpt-1_3B,OpenELM-270M,OpenELM-450M,OpenELM-1_1B,OpenELM-3B}]
                    [--model.language-modeling.general-gpt.max-context-length MODEL.LANGUAGE_MODELING.GENERAL_GPT.MAX_CONTEXT_LENGTH]
                    [--model.language-modeling.general-gpt.vocab-size MODEL.LANGUAGE_MODELING.GENERAL_GPT.VOCAB_SIZE]
                    [--model.language-modeling.general-gpt.padding-index MODEL.LANGUAGE_MODELING.GENERAL_GPT.PADDING_INDEX]
                    [--model.language-modeling.kv-prediction.auxkv-num-layers-to-basekv-num-layers MODEL.LANGUAGE_MODELING.KV_PREDICTION.AUXKV_NUM_LAYERS_TO_BASEKV_NUM_LAYERS]
                    [--model.language-modeling.kv-prediction.base-model MODEL.LANGUAGE_MODELING.KV_PREDICTION.BASE_MODEL]
                    [--model.language-modeling.kv-prediction.auxiliary-model MODEL.LANGUAGE_MODELING.KV_PREDICTION.AUXILIARY_MODEL]
                    [--model.multi-modal-image-text.name MODEL.MULTI_MODAL_IMAGE_TEXT.NAME]
                    [--model.multi-modal-image-text.lr-multiplier-img-encoder MODEL.MULTI_MODAL_IMAGE_TEXT.LR_MULTIPLIER_IMG_ENCODER]
                    [--model.multi-modal-image-text.lr-multiplier-text-encoder MODEL.MULTI_MODAL_IMAGE_TEXT.LR_MULTIPLIER_TEXT_ENCODER]
                    [--model.multi-modal-image-text.pretrained MODEL.MULTI_MODAL_IMAGE_TEXT.PRETRAINED]
                    [--model.multi-modal-image-text.freeze-batch-norm]
                    [--model.multi-modal-image-text.clip.projection-dim MODEL.MULTI_MODAL_IMAGE_TEXT.CLIP.PROJECTION_DIM]
                    [--model.segmentation.name MODEL.SEGMENTATION.NAME]
                    [--model.segmentation.n-classes MODEL.SEGMENTATION.N_CLASSES]
                    [--model.segmentation.pretrained MODEL.SEGMENTATION.PRETRAINED]
                    [--model.segmentation.lr-multiplier MODEL.SEGMENTATION.LR_MULTIPLIER]
                    [--model.segmentation.classifier-dropout MODEL.SEGMENTATION.CLASSIFIER_DROPOUT]
                    [--model.segmentation.use-aux-head]
                    [--model.segmentation.aux-dropout MODEL.SEGMENTATION.AUX_DROPOUT]
                    [--model.segmentation.output-stride MODEL.SEGMENTATION.OUTPUT_STRIDE]
                    [--model.segmentation.replace-stride-with-dilation]
                    [--model.segmentation.activation.name MODEL.SEGMENTATION.ACTIVATION.NAME]
                    [--model.segmentation.activation.inplace]
                    [--model.segmentation.activation.neg-slope MODEL.SEGMENTATION.ACTIVATION.NEG_SLOPE]
                    [--model.segmentation.freeze-batch-norm]
                    [--model.segmentation.use-level5-exp]
                    [--model.segmentation.finetune-pretrained-model]
                    [--model.segmentation.n-pretrained-classes MODEL.SEGMENTATION.N_PRETRAINED_CLASSES]
                    [--model.segmentation.norm-layer MODEL.SEGMENTATION.NORM_LAYER]
                    [--model.segmentation.seg-head MODEL.SEGMENTATION.SEG_HEAD]
                    [--model.segmentation.deeplabv3.aspp-rates MODEL.SEGMENTATION.DEEPLABV3.ASPP_RATES]
                    [--model.segmentation.deeplabv3.aspp-out-channels MODEL.SEGMENTATION.DEEPLABV3.ASPP_OUT_CHANNELS]
                    [--model.segmentation.deeplabv3.aspp-sep-conv]
                    [--model.segmentation.deeplabv3.aspp-dropout MODEL.SEGMENTATION.DEEPLABV3.ASPP_DROPOUT]
                    [--model.segmentation.deeplabv3.aspp-in-channels MODEL.SEGMENTATION.DEEPLABV3.ASPP_IN_CHANNELS]
                    [--model.segmentation.deeplabv3.output-upsample-factor MODEL.SEGMENTATION.DEEPLABV3.OUTPUT_UPSAMPLE_FACTOR]
                    [--model.segmentation.pspnet.psp-pool-sizes MODEL.SEGMENTATION.PSPNET.PSP_POOL_SIZES [MODEL.SEGMENTATION.PSPNET.PSP_POOL_SIZES ...]]
                    [--model.segmentation.pspnet.psp-out-channels MODEL.SEGMENTATION.PSPNET.PSP_OUT_CHANNELS]
                    [--model.segmentation.pspnet.psp-dropout MODEL.SEGMENTATION.PSPNET.PSP_DROPOUT]
                    [--model.video-classification.classifier-dropout MODEL.VIDEO_CLASSIFICATION.CLASSIFIER_DROPOUT]
                    [--model.video-classification.name MODEL.VIDEO_CLASSIFICATION.NAME]
                    [--model.video-classification.n-classes MODEL.VIDEO_CLASSIFICATION.N_CLASSES]
                    [--model.video-classification.pretrained MODEL.VIDEO_CLASSIFICATION.PRETRAINED]
                    [--model.video-classification.freeze-batch-norm]
                    [--model.video-classification.activation.name MODEL.VIDEO_CLASSIFICATION.ACTIVATION.NAME]
                    [--model.video-classification.activation.inplace]
                    [--model.video-classification.activation.neg-slope MODEL.VIDEO_CLASSIFICATION.ACTIVATION.NEG_SLOPE]
                    [--model.video-classification.clip-out-voting-fn {sum,max}]
                    [--model.video-classification.inference-mode]
                    [--fsdp.sharding-strategy {full_shard,no_shard,grad_op_shard}]
                    [--fsdp.backward-prefetching {pre,post}]
                    [--fsdp.parameter-datatype {float16,float32,bfloat16}]
                    [--fsdp.gradient-reduction-datatype {float16,float32,bfloat16}]
                    [--fsdp.buffer-datatype {float16,float32,bfloat16}]
                    [--fsdp.limit-all-gathers] [--fsdp.cpu-offload]
                    [--model.layer.linear-init MODEL.LAYER.LINEAR_INIT]
                    [--model.layer.linear-init-std-dev MODEL.LAYER.LINEAR_INIT_STD_DEV]
                    [--model.layer.group-linear-init MODEL.LAYER.GROUP_LINEAR_INIT]
                    [--model.layer.group-linear-init-std-dev MODEL.LAYER.GROUP_LINEAR_INIT_STD_DEV]
                    [--model.layer.global-pool MODEL.LAYER.GLOBAL_POOL]
                    [--model.layer.conv-init MODEL.LAYER.CONV_INIT]
                    [--model.layer.conv-init-std-dev MODEL.LAYER.CONV_INIT_STD_DEV]
                    [--model.activation.name MODEL.ACTIVATION.NAME]
                    [--model.activation.inplace]
                    [--model.activation.neg-slope MODEL.ACTIVATION.NEG_SLOPE]
                    [--model.normalization.name MODEL.NORMALIZATION.NAME]
                    [--model.normalization.groups MODEL.NORMALIZATION.GROUPS]
                    [--model.normalization.momentum MODEL.NORMALIZATION.MOMENTUM]
                    [--ema.enable] [--ema.momentum EMA.MOMENTUM]
                    [--anchor-generator.name ANCHOR_GENERATOR.NAME]
                    [--anchor-generator.ssd.output-strides ANCHOR_GENERATOR.SSD.OUTPUT_STRIDES [ANCHOR_GENERATOR.SSD.OUTPUT_STRIDES ...]]
                    [--anchor-generator.ssd.aspect-ratios ANCHOR_GENERATOR.SSD.ASPECT_RATIOS [ANCHOR_GENERATOR.SSD.ASPECT_RATIOS ...]]
                    [--anchor-generator.ssd.min-scale-ratio ANCHOR_GENERATOR.SSD.MIN_SCALE_RATIO]
                    [--anchor-generator.ssd.max-scale-ratio ANCHOR_GENERATOR.SSD.MAX_SCALE_RATIO]
                    [--anchor-generator.ssd.no-clipping]
                    [--anchor-generator.ssd.step ANCHOR_GENERATOR.SSD.STEP [ANCHOR_GENERATOR.SSD.STEP ...]]
                    [--matcher.name MATCHER.NAME]
                    [--matcher.ssd.center-variance MATCHER.SSD.CENTER_VARIANCE]
                    [--matcher.ssd.size-variance MATCHER.SSD.SIZE_VARIANCE]
                    [--matcher.ssd.iou-threshold MATCHER.SSD.IOU_THRESHOLD]
                    [--model.learn-augmentation.mode {basic,distribution}]
                    [--model.learn-augmentation.brightness]
                    [--model.learn-augmentation.contrast]
                    [--model.learn-augmentation.noise]
                    [--model.learn-augmentation.lr-multiplier MODEL.LEARN_AUGMENTATION.LR_MULTIPLIER]
                    [--teacher.model.text.name TEACHER.MODEL.TEXT.NAME]
                    [--teacher.model.text.padding-index TEACHER.MODEL.TEXT.PADDING_INDEX]
                    [--teacher.model.text.context-length TEACHER.MODEL.TEXT.CONTEXT_LENGTH]
                    [--teacher.model.text.vocab-size TEACHER.MODEL.TEXT.VOCAB_SIZE]
                    [--teacher.model.text.transformer.model-dim TEACHER.MODEL.TEXT.TRANSFORMER.MODEL_DIM]
                    [--teacher.model.text.transformer.no-scale-embedding [TEACHER.MODEL.TEXT.TRANSFORMER.NO_SCALE_EMBEDDING]]
                    [--teacher.model.text.transformer.no-pos-embedding [TEACHER.MODEL.TEXT.TRANSFORMER.NO_POS_EMBEDDING]]
                    [--teacher.model.text.transformer.embed-dropout TEACHER.MODEL.TEXT.TRANSFORMER.EMBED_DROPOUT]
                    [--teacher.model.text.transformer.n-transformer-layers TEACHER.MODEL.TEXT.TRANSFORMER.N_TRANSFORMER_LAYERS]
                    [--teacher.model.text.transformer.n-heads-per-layer TEACHER.MODEL.TEXT.TRANSFORMER.N_HEADS_PER_LAYER [TEACHER.MODEL.TEXT.TRANSFORMER.N_HEADS_PER_LAYER ...]]
                    [--teacher.model.text.transformer.ffn-multiplier-per-layer TEACHER.MODEL.TEXT.TRANSFORMER.FFN_MULTIPLIER_PER_LAYER [TEACHER.MODEL.TEXT.TRANSFORMER.FFN_MULTIPLIER_PER_LAYER ...]]
                    [--teacher.model.text.transformer.attn-dropout TEACHER.MODEL.TEXT.TRANSFORMER.ATTN_DROPOUT]
                    [--teacher.model.text.transformer.ffn-dropout TEACHER.MODEL.TEXT.TRANSFORMER.FFN_DROPOUT]
                    [--teacher.model.text.transformer.dropout TEACHER.MODEL.TEXT.TRANSFORMER.DROPOUT]
                    [--teacher.model.text.transformer.norm-layer TEACHER.MODEL.TEXT.TRANSFORMER.NORM_LAYER]
                    [--teacher.model.text.transformer.sinusoidal-pos-emb [TEACHER.MODEL.TEXT.TRANSFORMER.SINUSOIDAL_POS_EMB]]
                    [--teacher.model.text.transformer.causal-masking [TEACHER.MODEL.TEXT.TRANSFORMER.CAUSAL_MASKING]]
                    [--teacher.model.text.transformer.classes-per-split-zero-shot TEACHER.MODEL.TEXT.TRANSFORMER.CLASSES_PER_SPLIT_ZERO_SHOT]
                    [--teacher.model.image-projection-head.name TEACHER.MODEL.IMAGE_PROJECTION_HEAD.NAME]
                    [--teacher.model.image-projection-head.lr-multiplier TEACHER.MODEL.IMAGE_PROJECTION_HEAD.LR_MULTIPLIER]
                    [--teacher.model.image-projection-head.attention-pool-nchw2nc.num-pos-embeddings TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NUM_POS_EMBEDDINGS]
                    [--teacher.model.image-projection-head.attention-pool-nchw2nc.use-sinusoidal-pos-embeddings [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.USE_SINUSOIDAL_POS_EMBEDDINGS]]
                    [--teacher.model.image-projection-head.attention-pool-nchw2nc.num-attn-heads TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NUM_ATTN_HEADS]
                    [--teacher.model.image-projection-head.attention-pool-nchw2nc.no-feature-normalization [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NO_FEATURE_NORMALIZATION]]
                    [--teacher.model.image-projection-head.attention-pool-nchw2nc.use-pytorch-mha [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.USE_PYTORCH_MHA]]
                    [--teacher.model.image-projection-head.global-pool-nchw2nc.no-feature-normalization [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.GLOBAL_POOL_NCHW2NC.NO_FEATURE_NORMALIZATION]]
                    [--teacher.model.image-projection-head.global-pool-nchw2nc.identity-if-same-size [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.GLOBAL_POOL_NCHW2NC.IDENTITY_IF_SAME_SIZE]]
                    [--teacher.model.image-projection-head.simple-projection-nc2nc.no-feature-normalization [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.SIMPLE_PROJECTION_NC2NC.NO_FEATURE_NORMALIZATION]]
                    [--teacher.model.image-projection-head.simple-projection-nc2nc.identity-if-same-size [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.SIMPLE_PROJECTION_NC2NC.IDENTITY_IF_SAME_SIZE]]
                    [--teacher.model.resume-exclude-scopes TEACHER.MODEL.RESUME_EXCLUDE_SCOPES]
                    [--teacher.model.ignore-missing-scopes TEACHER.MODEL.IGNORE_MISSING_SCOPES]
                    [--teacher.model.rename-scopes-map TEACHER.MODEL.RENAME_SCOPES_MAP]
                    [--teacher.model.freeze-modules TEACHER.MODEL.FREEZE_MODULES]
                    [--teacher.model.activation-checkpointing [TEACHER.MODEL.ACTIVATION_CHECKPOINTING]]
                    [--teacher.model.lora.config TEACHER.MODEL.LORA.CONFIG]
                    [--teacher.model.lora.use-lora [TEACHER.MODEL.LORA.USE_LORA]]
                    [--teacher.model.audio-classification.name TEACHER.MODEL.AUDIO_CLASSIFICATION.NAME]
                    [--teacher.model.audio-classification.pretrained TEACHER.MODEL.AUDIO_CLASSIFICATION.PRETRAINED]
                    [--teacher.model.classification.byteformer.dropout TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.DROPOUT]
                    [--teacher.model.classification.byteformer.stochastic-dropout TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.STOCHASTIC_DROPOUT]
                    [--teacher.model.classification.byteformer.norm-layer {group_norm,layer_norm,layer_norm_nchw,layer_norm_2d,layer_norm_fp32,instance_norm_2d,instance_norm,instance_norm_1d,batch_norm_2d,batch_norm,batch_norm_fp32,batch_norm_1d,batch_norm_3d,sync_batch_norm,sync_batch_norm_fp32,rms_norm}]
                    [--teacher.model.classification.byteformer.sinusoidal-pos-emb [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.SINUSOIDAL_POS_EMB]]
                    [--teacher.model.classification.byteformer.use-pytorch-mha [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.USE_PYTORCH_MHA]]
                    [--teacher.model.classification.byteformer.mode {tiny,small,base,huge}]
                    [--teacher.model.classification.byteformer.vocab-size TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.VOCAB_SIZE]
                    [--teacher.model.classification.byteformer.max-num-tokens TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.MAX_NUM_TOKENS]
                    [--teacher.model.classification.byteformer.conv-kernel-size TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.CONV_KERNEL_SIZE]
                    [--teacher.model.classification.byteformer.window-sizes [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.WINDOW_SIZES ...]]
                    [--teacher.model.classification.byteformer.window-shifts [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.WINDOW_SHIFTS ...]]
                    [--teacher.model.classification.byteformer.downsample [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.DOWNSAMPLE ...]]
                    [--teacher.model.classification.byteformer.padding-index TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.PADDING_INDEX]
                    [--teacher.model.classification.byteformer.dummy-input-token-length TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.DUMMY_INPUT_TOKEN_LENGTH]
                    [--teacher.model.classification.classifier-dropout TEACHER.MODEL.CLASSIFICATION.CLASSIFIER_DROPOUT]
                    [--teacher.model.classification.name TEACHER.MODEL.CLASSIFICATION.NAME]
                    [--teacher.model.classification.n-classes TEACHER.MODEL.CLASSIFICATION.N_CLASSES]
                    [--teacher.model.classification.pretrained TEACHER.MODEL.CLASSIFICATION.PRETRAINED]
                    [--teacher.model.classification.freeze-batch-norm [TEACHER.MODEL.CLASSIFICATION.FREEZE_BATCH_NORM]]
                    [--teacher.model.classification.activation.name TEACHER.MODEL.CLASSIFICATION.ACTIVATION.NAME]
                    [--teacher.model.classification.activation.inplace [TEACHER.MODEL.CLASSIFICATION.ACTIVATION.INPLACE]]
                    [--teacher.model.classification.activation.neg-slope TEACHER.MODEL.CLASSIFICATION.ACTIVATION.NEG_SLOPE]
                    [--teacher.model.classification.finetune-pretrained-model [TEACHER.MODEL.CLASSIFICATION.FINETUNE_PRETRAINED_MODEL]]
                    [--teacher.model.classification.n-pretrained-classes TEACHER.MODEL.CLASSIFICATION.N_PRETRAINED_CLASSES]
                    [--teacher.model.classification.gradient-checkpointing [TEACHER.MODEL.CLASSIFICATION.GRADIENT_CHECKPOINTING]]
                    [--teacher.model.classification.enable-layer-wise-lr-decay [TEACHER.MODEL.CLASSIFICATION.ENABLE_LAYER_WISE_LR_DECAY]]
                    [--teacher.model.classification.layer-wise-lr-decay-rate TEACHER.MODEL.CLASSIFICATION.LAYER_WISE_LR_DECAY_RATE]
                    [--teacher.model.classification.efficientnet.mode {b0,b1,b2,b3,b4,b5,b6,b7}]
                    [--teacher.model.classification.efficientnet.stochastic-depth-prob TEACHER.MODEL.CLASSIFICATION.EFFICIENTNET.STOCHASTIC_DEPTH_PROB]
                    [--teacher.model.classification.fastvit.variant TEACHER.MODEL.CLASSIFICATION.FASTVIT.VARIANT]
                    [--teacher.model.classification.fastvit.inference-mode TEACHER.MODEL.CLASSIFICATION.FASTVIT.INFERENCE_MODE]
                    [--teacher.model.classification.fastvit.dropout TEACHER.MODEL.CLASSIFICATION.FASTVIT.DROPOUT]
                    [--teacher.model.classification.fastvit.drop-path TEACHER.MODEL.CLASSIFICATION.FASTVIT.DROP_PATH]
                    [--teacher.model.classification.fastvit.use-layer-scale TEACHER.MODEL.CLASSIFICATION.FASTVIT.USE_LAYER_SCALE]
                    [--teacher.model.classification.fastvit.layer-scale-init-value TEACHER.MODEL.CLASSIFICATION.FASTVIT.LAYER_SCALE_INIT_VALUE]
                    [--teacher.model.classification.mobilenetv1.width-multiplier TEACHER.MODEL.CLASSIFICATION.MOBILENETV1.WIDTH_MULTIPLIER]
                    [--teacher.model.classification.mobilenetv2.width-multiplier TEACHER.MODEL.CLASSIFICATION.MOBILENETV2.WIDTH_MULTIPLIER]
                    [--teacher.model.classification.mobilenetv3.mode {small,large}]
                    [--teacher.model.classification.mobilenetv3.width-multiplier TEACHER.MODEL.CLASSIFICATION.MOBILENETV3.WIDTH_MULTIPLIER]
                    [--teacher.model.classification.mobileone.variant TEACHER.MODEL.CLASSIFICATION.MOBILEONE.VARIANT]
                    [--teacher.model.classification.mobileone.inference-mode TEACHER.MODEL.CLASSIFICATION.MOBILEONE.INFERENCE_MODE]
                    [--teacher.model.classification.mit.mode {xx_small,x_small,small}]
                    [--teacher.model.classification.mit.attn-dropout TEACHER.MODEL.CLASSIFICATION.MIT.ATTN_DROPOUT]
                    [--teacher.model.classification.mit.ffn-dropout TEACHER.MODEL.CLASSIFICATION.MIT.FFN_DROPOUT]
                    [--teacher.model.classification.mit.dropout TEACHER.MODEL.CLASSIFICATION.MIT.DROPOUT]
                    [--teacher.model.classification.mit.transformer-norm-layer TEACHER.MODEL.CLASSIFICATION.MIT.TRANSFORMER_NORM_LAYER]
                    [--teacher.model.classification.mit.no-fuse-local-global-features [TEACHER.MODEL.CLASSIFICATION.MIT.NO_FUSE_LOCAL_GLOBAL_FEATURES]]
                    [--teacher.model.classification.mit.conv-kernel-size TEACHER.MODEL.CLASSIFICATION.MIT.CONV_KERNEL_SIZE]
                    [--teacher.model.classification.mit.head-dim TEACHER.MODEL.CLASSIFICATION.MIT.HEAD_DIM]
                    [--teacher.model.classification.mit.number-heads TEACHER.MODEL.CLASSIFICATION.MIT.NUMBER_HEADS]
                    [--teacher.model.classification.mitv2.attn-dropout TEACHER.MODEL.CLASSIFICATION.MITV2.ATTN_DROPOUT]
                    [--teacher.model.classification.mitv2.ffn-dropout TEACHER.MODEL.CLASSIFICATION.MITV2.FFN_DROPOUT]
                    [--teacher.model.classification.mitv2.dropout TEACHER.MODEL.CLASSIFICATION.MITV2.DROPOUT]
                    [--teacher.model.classification.mitv2.width-multiplier TEACHER.MODEL.CLASSIFICATION.MITV2.WIDTH_MULTIPLIER]
                    [--teacher.model.classification.mitv2.attn-norm-layer TEACHER.MODEL.CLASSIFICATION.MITV2.ATTN_NORM_LAYER]
                    [--teacher.model.classification.regnet.mode TEACHER.MODEL.CLASSIFICATION.REGNET.MODE]
                    [--teacher.model.classification.regnet.stochastic-depth-prob TEACHER.MODEL.CLASSIFICATION.REGNET.STOCHASTIC_DEPTH_PROB]
                    [--teacher.model.classification.regnet.stem-width TEACHER.MODEL.CLASSIFICATION.REGNET.STEM_WIDTH]
                    [--teacher.model.classification.resnet.depth TEACHER.MODEL.CLASSIFICATION.RESNET.DEPTH]
                    [--teacher.model.classification.resnet.dropout TEACHER.MODEL.CLASSIFICATION.RESNET.DROPOUT]
                    [--teacher.model.classification.resnet.stochastic-depth-prob TEACHER.MODEL.CLASSIFICATION.RESNET.STOCHASTIC_DEPTH_PROB]
                    [--teacher.model.classification.resnet.se-resnet [TEACHER.MODEL.CLASSIFICATION.RESNET.SE_RESNET]]
                    [--teacher.model.classification.swin.mode TEACHER.MODEL.CLASSIFICATION.SWIN.MODE]
                    [--teacher.model.classification.swin.stochastic-depth-prob TEACHER.MODEL.CLASSIFICATION.SWIN.STOCHASTIC_DEPTH_PROB]
                    [--teacher.model.classification.swin.extract-end-point-format {nchw,nhwc}]
                    [--teacher.model.classification.vit.mode {tiny,small,base,large,huge}]
                    [--teacher.model.classification.vit.dropout TEACHER.MODEL.CLASSIFICATION.VIT.DROPOUT]
                    [--teacher.model.classification.vit.stochastic-dropout TEACHER.MODEL.CLASSIFICATION.VIT.STOCHASTIC_DROPOUT]
                    [--teacher.model.classification.vit.norm-layer TEACHER.MODEL.CLASSIFICATION.VIT.NORM_LAYER]
                    [--teacher.model.classification.vit.sinusoidal-pos-emb [TEACHER.MODEL.CLASSIFICATION.VIT.SINUSOIDAL_POS_EMB]]
                    [--teacher.model.classification.vit.no-cls-token [TEACHER.MODEL.CLASSIFICATION.VIT.NO_CLS_TOKEN]]
                    [--teacher.model.classification.vit.use-simple-fpn [TEACHER.MODEL.CLASSIFICATION.VIT.USE_SIMPLE_FPN]]
                    [--teacher.model.classification.vit.use-flash-attention [TEACHER.MODEL.CLASSIFICATION.VIT.USE_FLASH_ATTENTION]]
                    [--teacher.model.detection.name TEACHER.MODEL.DETECTION.NAME]
                    [--teacher.model.detection.n-classes TEACHER.MODEL.DETECTION.N_CLASSES]
                    [--teacher.model.detection.pretrained TEACHER.MODEL.DETECTION.PRETRAINED]
                    [--teacher.model.detection.output-stride TEACHER.MODEL.DETECTION.OUTPUT_STRIDE]
                    [--teacher.model.detection.replace-stride-with-dilation [TEACHER.MODEL.DETECTION.REPLACE_STRIDE_WITH_DILATION]]
                    [--teacher.model.detection.freeze-batch-norm [TEACHER.MODEL.DETECTION.FREEZE_BATCH_NORM]]
                    [--teacher.model.detection.mask-rcnn.backbone-projection-channels TEACHER.MODEL.DETECTION.MASK_RCNN.BACKBONE_PROJECTION_CHANNELS]
                    [--teacher.model.detection.mask-rcnn.backbone-lr-multiplier TEACHER.MODEL.DETECTION.MASK_RCNN.BACKBONE_LR_MULTIPLIER]
                    [--teacher.model.detection.mask-rcnn.output-strides TEACHER.MODEL.DETECTION.MASK_RCNN.OUTPUT_STRIDES [TEACHER.MODEL.DETECTION.MASK_RCNN.OUTPUT_STRIDES ...]]
                    [--teacher.model.detection.mask-rcnn.anchor-sizes TEACHER.MODEL.DETECTION.MASK_RCNN.ANCHOR_SIZES [TEACHER.MODEL.DETECTION.MASK_RCNN.ANCHOR_SIZES ...]]
                    [--teacher.model.detection.mask-rcnn.aspect-ratio TEACHER.MODEL.DETECTION.MASK_RCNN.ASPECT_RATIO [TEACHER.MODEL.DETECTION.MASK_RCNN.ASPECT_RATIO ...]]
                    [--teacher.model.detection.mask-rcnn.bbox-head-fm-size TEACHER.MODEL.DETECTION.MASK_RCNN.BBOX_HEAD_FM_SIZE]
                    [--teacher.model.detection.mask-rcnn.mask-head-fm-size TEACHER.MODEL.DETECTION.MASK_RCNN.MASK_HEAD_FM_SIZE]
                    [--teacher.model.detection.mask-rcnn.representation-size TEACHER.MODEL.DETECTION.MASK_RCNN.REPRESENTATION_SIZE]
                    [--teacher.model.detection.mask-rcnn.box-fm-size-conv-layer TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_FM_SIZE_CONV_LAYER [TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_FM_SIZE_CONV_LAYER ...]]
                    [--teacher.model.detection.mask-rcnn.mask-fm-size-conv-layer TEACHER.MODEL.DETECTION.MASK_RCNN.MASK_FM_SIZE_CONV_LAYER [TEACHER.MODEL.DETECTION.MASK_RCNN.MASK_FM_SIZE_CONV_LAYER ...]]
                    [--teacher.model.detection.mask-rcnn.mask-dilation TEACHER.MODEL.DETECTION.MASK_RCNN.MASK_DILATION]
                    [--teacher.model.detection.mask-rcnn.rpn-pre-nms-top-n-train TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_PRE_NMS_TOP_N_TRAIN]
                    [--teacher.model.detection.mask-rcnn.rpn-pre-nms-top-n-test TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_PRE_NMS_TOP_N_TEST]
                    [--teacher.model.detection.mask-rcnn.rpn-post-nms-top-n-train TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_POST_NMS_TOP_N_TRAIN]
                    [--teacher.model.detection.mask-rcnn.rpn-post-nms-top-n-test TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_POST_NMS_TOP_N_TEST]
                    [--teacher.model.detection.mask-rcnn.rpn-nms-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_NMS_THRESH]
                    [--teacher.model.detection.mask-rcnn.rpn-fg-iou-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_FG_IOU_THRESH]
                    [--teacher.model.detection.mask-rcnn.rpn-bg-iou-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_BG_IOU_THRESH]
                    [--teacher.model.detection.mask-rcnn.rpn-batch-size-per-image TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_BATCH_SIZE_PER_IMAGE]
                    [--teacher.model.detection.mask-rcnn.rpn-positive-fraction TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_POSITIVE_FRACTION]
                    [--teacher.model.detection.mask-rcnn.rpn-score-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_SCORE_THRESH]
                    [--teacher.model.detection.mask-rcnn.box-score-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_SCORE_THRESH]
                    [--teacher.model.detection.mask-rcnn.box-nms-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_NMS_THRESH]
                    [--teacher.model.detection.mask-rcnn.box-detections-per-img TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_DETECTIONS_PER_IMG]
                    [--teacher.model.detection.mask-rcnn.box-fg-iou-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_FG_IOU_THRESH]
                    [--teacher.model.detection.mask-rcnn.box-bg-iou-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_BG_IOU_THRESH]
                    [--teacher.model.detection.mask-rcnn.box-batch-size-per-image TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_BATCH_SIZE_PER_IMAGE]
                    [--teacher.model.detection.mask-rcnn.box-positive-fraction TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_POSITIVE_FRACTION]
                    [--teacher.model.detection.mask-rcnn.norm-layer TEACHER.MODEL.DETECTION.MASK_RCNN.NORM_LAYER]
                    [--teacher.model.detection.mask-rcnn.disable-fpn [TEACHER.MODEL.DETECTION.MASK_RCNN.DISABLE_FPN]]
                    [--teacher.model.detection.ssd.anchors-aspect-ratio TEACHER.MODEL.DETECTION.SSD.ANCHORS_ASPECT_RATIO [TEACHER.MODEL.DETECTION.SSD.ANCHORS_ASPECT_RATIO ...]]
                    [--teacher.model.detection.ssd.output-strides TEACHER.MODEL.DETECTION.SSD.OUTPUT_STRIDES [TEACHER.MODEL.DETECTION.SSD.OUTPUT_STRIDES ...]]
                    [--teacher.model.detection.ssd.proj-channels TEACHER.MODEL.DETECTION.SSD.PROJ_CHANNELS [TEACHER.MODEL.DETECTION.SSD.PROJ_CHANNELS ...]]
                    [--teacher.model.detection.ssd.min-box-size TEACHER.MODEL.DETECTION.SSD.MIN_BOX_SIZE]
                    [--teacher.model.detection.ssd.max-box-size TEACHER.MODEL.DETECTION.SSD.MAX_BOX_SIZE]
                    [--teacher.model.detection.ssd.center-variance TEACHER.MODEL.DETECTION.SSD.CENTER_VARIANCE]
                    [--teacher.model.detection.ssd.size-variance TEACHER.MODEL.DETECTION.SSD.SIZE_VARIANCE]
                    [--teacher.model.detection.ssd.iou-threshold TEACHER.MODEL.DETECTION.SSD.IOU_THRESHOLD]
                    [--teacher.model.detection.ssd.conf-threshold TEACHER.MODEL.DETECTION.SSD.CONF_THRESHOLD]
                    [--teacher.model.detection.ssd.top-k TEACHER.MODEL.DETECTION.SSD.TOP_K]
                    [--teacher.model.detection.ssd.objects-per-image TEACHER.MODEL.DETECTION.SSD.OBJECTS_PER_IMAGE]
                    [--teacher.model.detection.ssd.nms-iou-threshold TEACHER.MODEL.DETECTION.SSD.NMS_IOU_THRESHOLD]
                    [--teacher.model.detection.ssd.fpn-out-channels TEACHER.MODEL.DETECTION.SSD.FPN_OUT_CHANNELS]
                    [--teacher.model.detection.ssd.use-fpn [TEACHER.MODEL.DETECTION.SSD.USE_FPN]]
                    [--teacher.model.language-modeling.name TEACHER.MODEL.LANGUAGE_MODELING.NAME]
                    [--teacher.model.language-modeling.pretrained TEACHER.MODEL.LANGUAGE_MODELING.PRETRAINED]
                    [--teacher.model.language-modeling.general-gpt.model-name {gpt-test,gpt-1_3B,OpenELM-270M,OpenELM-450M,OpenELM-1_1B,OpenELM-3B}]
                    [--teacher.model.language-modeling.general-gpt.max-context-length TEACHER.MODEL.LANGUAGE_MODELING.GENERAL_GPT.MAX_CONTEXT_LENGTH]
                    [--teacher.model.language-modeling.general-gpt.vocab-size TEACHER.MODEL.LANGUAGE_MODELING.GENERAL_GPT.VOCAB_SIZE]
                    [--teacher.model.language-modeling.general-gpt.padding-index TEACHER.MODEL.LANGUAGE_MODELING.GENERAL_GPT.PADDING_INDEX]
                    [--teacher.model.language-modeling.kv-prediction.auxkv-num-layers-to-basekv-num-layers TEACHER.MODEL.LANGUAGE_MODELING.KV_PREDICTION.AUXKV_NUM_LAYERS_TO_BASEKV_NUM_LAYERS]
                    [--teacher.model.language-modeling.kv-prediction.base-model TEACHER.MODEL.LANGUAGE_MODELING.KV_PREDICTION.BASE_MODEL]
                    [--teacher.model.language-modeling.kv-prediction.auxiliary-model TEACHER.MODEL.LANGUAGE_MODELING.KV_PREDICTION.AUXILIARY_MODEL]
                    [--teacher.model.multi-modal-image-text.name TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.NAME]
                    [--teacher.model.multi-modal-image-text.lr-multiplier-img-encoder TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.LR_MULTIPLIER_IMG_ENCODER]
                    [--teacher.model.multi-modal-image-text.lr-multiplier-text-encoder TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.LR_MULTIPLIER_TEXT_ENCODER]
                    [--teacher.model.multi-modal-image-text.pretrained TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.PRETRAINED]
                    [--teacher.model.multi-modal-image-text.freeze-batch-norm [TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.FREEZE_BATCH_NORM]]
                    [--teacher.model.multi-modal-image-text.clip.projection-dim TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.CLIP.PROJECTION_DIM]
                    [--teacher.model.segmentation.name TEACHER.MODEL.SEGMENTATION.NAME]
                    [--teacher.model.segmentation.n-classes TEACHER.MODEL.SEGMENTATION.N_CLASSES]
                    [--teacher.model.segmentation.pretrained TEACHER.MODEL.SEGMENTATION.PRETRAINED]
                    [--teacher.model.segmentation.lr-multiplier TEACHER.MODEL.SEGMENTATION.LR_MULTIPLIER]
                    [--teacher.model.segmentation.classifier-dropout TEACHER.MODEL.SEGMENTATION.CLASSIFIER_DROPOUT]
                    [--teacher.model.segmentation.use-aux-head [TEACHER.MODEL.SEGMENTATION.USE_AUX_HEAD]]
                    [--teacher.model.segmentation.aux-dropout TEACHER.MODEL.SEGMENTATION.AUX_DROPOUT]
                    [--teacher.model.segmentation.output-stride TEACHER.MODEL.SEGMENTATION.OUTPUT_STRIDE]
                    [--teacher.model.segmentation.replace-stride-with-dilation [TEACHER.MODEL.SEGMENTATION.REPLACE_STRIDE_WITH_DILATION]]
                    [--teacher.model.segmentation.activation.name TEACHER.MODEL.SEGMENTATION.ACTIVATION.NAME]
                    [--teacher.model.segmentation.activation.inplace [TEACHER.MODEL.SEGMENTATION.ACTIVATION.INPLACE]]
                    [--teacher.model.segmentation.activation.neg-slope TEACHER.MODEL.SEGMENTATION.ACTIVATION.NEG_SLOPE]
                    [--teacher.model.segmentation.freeze-batch-norm [TEACHER.MODEL.SEGMENTATION.FREEZE_BATCH_NORM]]
                    [--teacher.model.segmentation.use-level5-exp [TEACHER.MODEL.SEGMENTATION.USE_LEVEL5_EXP]]
                    [--teacher.model.segmentation.finetune-pretrained-model [TEACHER.MODEL.SEGMENTATION.FINETUNE_PRETRAINED_MODEL]]
                    [--teacher.model.segmentation.n-pretrained-classes TEACHER.MODEL.SEGMENTATION.N_PRETRAINED_CLASSES]
                    [--teacher.model.segmentation.norm-layer TEACHER.MODEL.SEGMENTATION.NORM_LAYER]
                    [--teacher.model.segmentation.seg-head TEACHER.MODEL.SEGMENTATION.SEG_HEAD]
                    [--teacher.model.segmentation.deeplabv3.aspp-rates TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_RATES]
                    [--teacher.model.segmentation.deeplabv3.aspp-out-channels TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_OUT_CHANNELS]
                    [--teacher.model.segmentation.deeplabv3.aspp-sep-conv [TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_SEP_CONV]]
                    [--teacher.model.segmentation.deeplabv3.aspp-dropout TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_DROPOUT]
                    [--teacher.model.segmentation.deeplabv3.aspp-in-channels TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_IN_CHANNELS]
                    [--teacher.model.segmentation.deeplabv3.output-upsample-factor TEACHER.MODEL.SEGMENTATION.DEEPLABV3.OUTPUT_UPSAMPLE_FACTOR]
                    [--teacher.model.segmentation.pspnet.psp-pool-sizes TEACHER.MODEL.SEGMENTATION.PSPNET.PSP_POOL_SIZES [TEACHER.MODEL.SEGMENTATION.PSPNET.PSP_POOL_SIZES ...]]
                    [--teacher.model.segmentation.pspnet.psp-out-channels TEACHER.MODEL.SEGMENTATION.PSPNET.PSP_OUT_CHANNELS]
                    [--teacher.model.segmentation.pspnet.psp-dropout TEACHER.MODEL.SEGMENTATION.PSPNET.PSP_DROPOUT]
                    [--teacher.model.video-classification.classifier-dropout TEACHER.MODEL.VIDEO_CLASSIFICATION.CLASSIFIER_DROPOUT]
                    [--teacher.model.video-classification.name TEACHER.MODEL.VIDEO_CLASSIFICATION.NAME]
                    [--teacher.model.video-classification.n-classes TEACHER.MODEL.VIDEO_CLASSIFICATION.N_CLASSES]
                    [--teacher.model.video-classification.pretrained TEACHER.MODEL.VIDEO_CLASSIFICATION.PRETRAINED]
                    [--teacher.model.video-classification.freeze-batch-norm [TEACHER.MODEL.VIDEO_CLASSIFICATION.FREEZE_BATCH_NORM]]
                    [--teacher.model.video-classification.activation.name TEACHER.MODEL.VIDEO_CLASSIFICATION.ACTIVATION.NAME]
                    [--teacher.model.video-classification.activation.inplace [TEACHER.MODEL.VIDEO_CLASSIFICATION.ACTIVATION.INPLACE]]
                    [--teacher.model.video-classification.activation.neg-slope TEACHER.MODEL.VIDEO_CLASSIFICATION.ACTIVATION.NEG_SLOPE]
                    [--teacher.model.video-classification.clip-out-voting-fn {sum,max}]
                    [--teacher.model.video-classification.inference-mode [TEACHER.MODEL.VIDEO_CLASSIFICATION.INFERENCE_MODE]]
                    [--teacher.model.layer.linear-init TEACHER.MODEL.LAYER.LINEAR_INIT]
                    [--teacher.model.layer.linear-init-std-dev TEACHER.MODEL.LAYER.LINEAR_INIT_STD_DEV]
                    [--teacher.model.layer.group-linear-init TEACHER.MODEL.LAYER.GROUP_LINEAR_INIT]
                    [--teacher.model.layer.group-linear-init-std-dev TEACHER.MODEL.LAYER.GROUP_LINEAR_INIT_STD_DEV]
                    [--teacher.model.layer.global-pool TEACHER.MODEL.LAYER.GLOBAL_POOL]
                    [--teacher.model.layer.conv-init TEACHER.MODEL.LAYER.CONV_INIT]
                    [--teacher.model.layer.conv-init-std-dev TEACHER.MODEL.LAYER.CONV_INIT_STD_DEV]
                    [--teacher.model.activation.name TEACHER.MODEL.ACTIVATION.NAME]
                    [--teacher.model.activation.inplace [TEACHER.MODEL.ACTIVATION.INPLACE]]
                    [--teacher.model.activation.neg-slope TEACHER.MODEL.ACTIVATION.NEG_SLOPE]
                    [--teacher.model.normalization.name TEACHER.MODEL.NORMALIZATION.NAME]
                    [--teacher.model.normalization.groups TEACHER.MODEL.NORMALIZATION.GROUPS]
                    [--teacher.model.normalization.momentum TEACHER.MODEL.NORMALIZATION.MOMENTUM]
                    [--teacher.model.learn-augmentation.mode {basic,distribution}]
                    [--teacher.model.learn-augmentation.brightness [TEACHER.MODEL.LEARN_AUGMENTATION.BRIGHTNESS]]
                    [--teacher.model.learn-augmentation.contrast [TEACHER.MODEL.LEARN_AUGMENTATION.CONTRAST]]
                    [--teacher.model.learn-augmentation.noise [TEACHER.MODEL.LEARN_AUGMENTATION.NOISE]]
                    [--teacher.model.learn-augmentation.lr-multiplier TEACHER.MODEL.LEARN_AUGMENTATION.LR_MULTIPLIER]
                    [--sampler.bs.crop-size-width SAMPLER.BS.CROP_SIZE_WIDTH]
                    [--sampler.bs.crop-size-height SAMPLER.BS.CROP_SIZE_HEIGHT]
                    [--sampler.chain-sampler SAMPLER.CHAIN_SAMPLER]
                    [--sampler.chain-sampler-mode {sequential,interleave}]
                    [--sampler.msc.crop-size-width SAMPLER.MSC.CROP_SIZE_WIDTH]
                    [--sampler.msc.crop-size-height SAMPLER.MSC.CROP_SIZE_HEIGHT]
                    [--sampler.msc.min-crop-size-width SAMPLER.MSC.MIN_CROP_SIZE_WIDTH]
                    [--sampler.msc.max-crop-size-width SAMPLER.MSC.MAX_CROP_SIZE_WIDTH]
                    [--sampler.msc.min-crop-size-height SAMPLER.MSC.MIN_CROP_SIZE_HEIGHT]
                    [--sampler.msc.max-crop-size-height SAMPLER.MSC.MAX_CROP_SIZE_HEIGHT]
                    [--sampler.msc.max-n-scales SAMPLER.MSC.MAX_N_SCALES]
                    [--sampler.msc.check-scale SAMPLER.MSC.CHECK_SCALE]
                    [--sampler.msc.scale-inc]
                    [--sampler.vbs.crop-size-width SAMPLER.VBS.CROP_SIZE_WIDTH]
                    [--sampler.vbs.crop-size-height SAMPLER.VBS.CROP_SIZE_HEIGHT]
                    [--sampler.vbs.min-crop-size-width SAMPLER.VBS.MIN_CROP_SIZE_WIDTH]
                    [--sampler.vbs.max-crop-size-width SAMPLER.VBS.MAX_CROP_SIZE_WIDTH]
                    [--sampler.vbs.min-crop-size-height SAMPLER.VBS.MIN_CROP_SIZE_HEIGHT]
                    [--sampler.vbs.max-crop-size-height SAMPLER.VBS.MAX_CROP_SIZE_HEIGHT]
                    [--sampler.vbs.max-n-scales SAMPLER.VBS.MAX_N_SCALES]
                    [--sampler.vbs.check-scale SAMPLER.VBS.CHECK_SCALE]
                    [--sampler.vbs.ep-intervals SAMPLER.VBS.EP_INTERVALS]
                    [--sampler.vbs.min-scale-inc-factor SAMPLER.VBS.MIN_SCALE_INC_FACTOR]
                    [--sampler.vbs.max-scale-inc-factor SAMPLER.VBS.MAX_SCALE_INC_FACTOR]
                    [--sampler.vbs.scale-inc]
                    [--sampler.bs.num-frames-per-clip SAMPLER.BS.NUM_FRAMES_PER_CLIP]
                    [--sampler.bs.clips-per-video SAMPLER.BS.CLIPS_PER_VIDEO]
                    [--sampler.vcbs.num-frames-per-clip SAMPLER.VCBS.NUM_FRAMES_PER_CLIP]
                    [--sampler.vcbs.video-fps SAMPLER.VCBS.VIDEO_FPS]
                    [--sampler.vcbs.audio-fps SAMPLER.VCBS.AUDIO_FPS]
                    [--sampler.vcbs.min-clip-fps-scale SAMPLER.VCBS.MIN_CLIP_FPS_SCALE]
                    [--sampler.vcbs.max-clip-fps-scale SAMPLER.VCBS.MAX_CLIP_FPS_SCALE]
                    [--sampler.vcbs.video-fps-num-scales SAMPLER.VCBS.VIDEO_FPS_NUM_SCALES]
                    [--sampler.vcbs.num-clips-per-second-train SAMPLER.VCBS.NUM_CLIPS_PER_SECOND_TRAIN]
                    [--sampler.vcbs.num-clips-per-second-val SAMPLER.VCBS.NUM_CLIPS_PER_SECOND_VAL]
                    [--sampler.vcbs.max-num-clips-per-batch SAMPLER.VCBS.MAX_NUM_CLIPS_PER_BATCH]
                    [--sampler.vcbs.num-samples-per-clip SAMPLER.VCBS.NUM_SAMPLES_PER_CLIP]
                    [--sampler.vbs.num-frames-per-clip SAMPLER.VBS.NUM_FRAMES_PER_CLIP]
                    [--sampler.vbs.random-video-clips]
                    [--sampler.vbs.min-clips-per-video SAMPLER.VBS.MIN_CLIPS_PER_VIDEO]
                    [--sampler.vbs.max-clips-per-video SAMPLER.VBS.MAX_CLIPS_PER_VIDEO]
                    [--sampler.vbs.clips-per-video SAMPLER.VBS.CLIPS_PER_VIDEO]
                    [--sampler.vbs.min-frames-per-clip SAMPLER.VBS.MIN_FRAMES_PER_CLIP]
                    [--sampler.name SAMPLER.NAME]
                    [--sampler.num-repeats SAMPLER.NUM_REPEATS]
                    [--sampler.truncated-repeat-aug-sampler]
                    [--sampler.start-shuffling-from-epoch SAMPLER.START_SHUFFLING_FROM_EPOCH]
                    [--sampler.use-shards]
                    [--sampler.disable-shuffle-sharding]
                    [--dataset.collate-fn-name-train DATASET.COLLATE_FN_NAME_TRAIN]
                    [--dataset.collate-fn-name-val DATASET.COLLATE_FN_NAME_VAL]
                    [--dataset.collate-fn-name-test DATASET.COLLATE_FN_NAME_TEST]
                    [--image-augmentation.fixed-size-crop.enable]
                    [--image-augmentation.fixed-size-crop.size IMAGE_AUGMENTATION.FIXED_SIZE_CROP.SIZE [IMAGE_AUGMENTATION.FIXED_SIZE_CROP.SIZE ...]]
                    [--image-augmentation.fixed-size-crop.fill IMAGE_AUGMENTATION.FIXED_SIZE_CROP.FILL]
                    [--image-augmentation.fixed-size-crop.padding-mode IMAGE_AUGMENTATION.FIXED_SIZE_CROP.PADDING_MODE]
                    [--image-augmentation.scale-jitter.enable]
                    [--image-augmentation.scale-jitter.interpolation IMAGE_AUGMENTATION.SCALE_JITTER.INTERPOLATION]
                    [--image-augmentation.scale-jitter.target-size IMAGE_AUGMENTATION.SCALE_JITTER.TARGET_SIZE [IMAGE_AUGMENTATION.SCALE_JITTER.TARGET_SIZE ...]]
                    [--image-augmentation.scale-jitter.scale-range IMAGE_AUGMENTATION.SCALE_JITTER.SCALE_RANGE [IMAGE_AUGMENTATION.SCALE_JITTER.SCALE_RANGE ...]]
                    [--image-augmentation.random-resized-crop.enable]
                    [--image-augmentation.random-resized-crop.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--image-augmentation.random-resized-crop.scale IMAGE_AUGMENTATION.RANDOM_RESIZED_CROP.SCALE]
                    [--image-augmentation.random-resized-crop.aspect-ratio IMAGE_AUGMENTATION.RANDOM_RESIZED_CROP.ASPECT_RATIO]
                    [--image-augmentation.auto-augment.enable]
                    [--image-augmentation.auto-augment.policy IMAGE_AUGMENTATION.AUTO_AUGMENT.POLICY]
                    [--image-augmentation.auto-augment.interpolation IMAGE_AUGMENTATION.AUTO_AUGMENT.INTERPOLATION]
                    [--image-augmentation.rand-augment.enable]
                    [--image-augmentation.rand-augment.num-ops IMAGE_AUGMENTATION.RAND_AUGMENT.NUM_OPS]
                    [--image-augmentation.rand-augment.magnitude IMAGE_AUGMENTATION.RAND_AUGMENT.MAGNITUDE]
                    [--image-augmentation.rand-augment.num-magnitude-bins IMAGE_AUGMENTATION.RAND_AUGMENT.NUM_MAGNITUDE_BINS]
                    [--image-augmentation.rand-augment.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--image-augmentation.trivial-augment-wide.enable]
                    [--image-augmentation.trivial-augment-wide.num-magnitude-bins IMAGE_AUGMENTATION.TRIVIAL_AUGMENT_WIDE.NUM_MAGNITUDE_BINS]
                    [--image-augmentation.trivial-augment-wide.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--image-augmentation.random-horizontal-flip.enable]
                    [--image-augmentation.random-horizontal-flip.p IMAGE_AUGMENTATION.RANDOM_HORIZONTAL_FLIP.P]
                    [--image-augmentation.random-rotate.enable]
                    [--image-augmentation.random-rotate.angle IMAGE_AUGMENTATION.RANDOM_ROTATE.ANGLE]
                    [--image-augmentation.random-rotate.mask-fill IMAGE_AUGMENTATION.RANDOM_ROTATE.MASK_FILL]
                    [--image-augmentation.resize.enable]
                    [--image-augmentation.resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--image-augmentation.resize.size IMAGE_AUGMENTATION.RESIZE.SIZE [IMAGE_AUGMENTATION.RESIZE.SIZE ...]]
                    [--image-augmentation.center-crop.enable]
                    [--image-augmentation.center-crop.size IMAGE_AUGMENTATION.CENTER_CROP.SIZE [IMAGE_AUGMENTATION.CENTER_CROP.SIZE ...]]
                    [--image-augmentation.ssd-crop.enable]
                    [--image-augmentation.ssd-crop.iou-thresholds IMAGE_AUGMENTATION.SSD_CROP.IOU_THRESHOLDS [IMAGE_AUGMENTATION.SSD_CROP.IOU_THRESHOLDS ...]]
                    [--image-augmentation.ssd-crop.n-trials IMAGE_AUGMENTATION.SSD_CROP.N_TRIALS]
                    [--image-augmentation.ssd-crop.min-aspect-ratio IMAGE_AUGMENTATION.SSD_CROP.MIN_ASPECT_RATIO]
                    [--image-augmentation.ssd-crop.max-aspect-ratio IMAGE_AUGMENTATION.SSD_CROP.MAX_ASPECT_RATIO]
                    [--image-augmentation.photo-metric-distort.enable]
                    [--image-augmentation.photo-metric-distort.alpha-min IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.ALPHA_MIN]
                    [--image-augmentation.photo-metric-distort.alpha-max IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.ALPHA_MAX]
                    [--image-augmentation.photo-metric-distort.beta-min IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.BETA_MIN]
                    [--image-augmentation.photo-metric-distort.beta-max IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.BETA_MAX]
                    [--image-augmentation.photo-metric-distort.gamma-min IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.GAMMA_MIN]
                    [--image-augmentation.photo-metric-distort.gamma-max IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.GAMMA_MAX]
                    [--image-augmentation.photo-metric-distort.delta-min IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.DELTA_MIN]
                    [--image-augmentation.photo-metric-distort.delta-max IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.DELTA_MAX]
                    [--image-augmentation.photo-metric-distort.p IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.P]
                    [--image-augmentation.random-resize.enable]
                    [--image-augmentation.random-resize.max-scale-long-edge IMAGE_AUGMENTATION.RANDOM_RESIZE.MAX_SCALE_LONG_EDGE]
                    [--image-augmentation.random-resize.max-scale-short-edge IMAGE_AUGMENTATION.RANDOM_RESIZE.MAX_SCALE_SHORT_EDGE]
                    [--image-augmentation.random-resize.min-ratio IMAGE_AUGMENTATION.RANDOM_RESIZE.MIN_RATIO]
                    [--image-augmentation.random-resize.max-ratio IMAGE_AUGMENTATION.RANDOM_RESIZE.MAX_RATIO]
                    [--image-augmentation.random-resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--image-augmentation.random-short-size-resize.enable]
                    [--image-augmentation.random-short-size-resize.short-side-min IMAGE_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.SHORT_SIDE_MIN]
                    [--image-augmentation.random-short-size-resize.short-side-max IMAGE_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.SHORT_SIDE_MAX]
                    [--image-augmentation.random-short-size-resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--image-augmentation.random-short-size-resize.max-img-dim IMAGE_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.MAX_IMG_DIM]
                    [--image-augmentation.random-erase.enable]
                    [--image-augmentation.random-erase.p IMAGE_AUGMENTATION.RANDOM_ERASE.P]
                    [--image-augmentation.random-gaussian-noise.enable]
                    [--image-augmentation.random-gaussian-noise.p IMAGE_AUGMENTATION.RANDOM_GAUSSIAN_NOISE.P]
                    [--image-augmentation.random-crop.enable]
                    [--image-augmentation.random-crop.seg-class-max-ratio IMAGE_AUGMENTATION.RANDOM_CROP.SEG_CLASS_MAX_RATIO]
                    [--image-augmentation.random-crop.pad-if-needed]
                    [--image-augmentation.random-crop.mask-fill IMAGE_AUGMENTATION.RANDOM_CROP.MASK_FILL]
                    [--image-augmentation.to-tensor.dtype IMAGE_AUGMENTATION.TO_TENSOR.DTYPE]
                    [--image-augmentation.to-tensor.mean-std-normalization.enable]
                    [--image-augmentation.to-tensor.mean-std-normalization.mean IMAGE_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.MEAN [IMAGE_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.MEAN ...]]
                    [--image-augmentation.to-tensor.mean-std-normalization.std IMAGE_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.STD [IMAGE_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.STD ...]]
                    [--image-augmentation.random-order.enable]
                    [--image-augmentation.random-order.apply-k IMAGE_AUGMENTATION.RANDOM_ORDER.APPLY_K]
                    [--image-augmentation.rand-augment.use-timm-library]
                    [--image-augmentation.rand-augment.timm-config-str IMAGE_AUGMENTATION.RAND_AUGMENT.TIMM_CONFIG_STR]
                    [--image-augmentation.mixup.enable]
                    [--image-augmentation.mixup.alpha IMAGE_AUGMENTATION.MIXUP.ALPHA]
                    [--image-augmentation.mixup.p IMAGE_AUGMENTATION.MIXUP.P]
                    [--image-augmentation.mixup.inplace]
                    [--image-augmentation.mixup.sample-key IMAGE_AUGMENTATION.MIXUP.SAMPLE_KEY]
                    [--image-augmentation.mixup.target-key IMAGE_AUGMENTATION.MIXUP.TARGET_KEY]
                    [--image-augmentation.cutmix.enable]
                    [--image-augmentation.cutmix.alpha IMAGE_AUGMENTATION.CUTMIX.ALPHA]
                    [--image-augmentation.cutmix.p IMAGE_AUGMENTATION.CUTMIX.P]
                    [--image-augmentation.cutmix.inplace]
                    [--image-augmentation.cutmix.sample-key IMAGE_AUGMENTATION.CUTMIX.SAMPLE_KEY]
                    [--image-augmentation.cutmix.target-key IMAGE_AUGMENTATION.CUTMIX.TARGET_KEY]
                    [--audio-augmentation.gain.enable]
                    [--audio-augmentation.gain.levels AUDIO_AUGMENTATION.GAIN.LEVELS [AUDIO_AUGMENTATION.GAIN.LEVELS ...]]
                    [--audio-augmentation.gain.share-clip-params]
                    [--audio-augmentation.noise.enable]
                    [--audio-augmentation.noise.levels AUDIO_AUGMENTATION.NOISE.LEVELS [AUDIO_AUGMENTATION.NOISE.LEVELS ...]]
                    [--audio-augmentation.noise.cache-size AUDIO_AUGMENTATION.NOISE.CACHE_SIZE]
                    [--audio-augmentation.noise.files-dir AUDIO_AUGMENTATION.NOISE.FILES_DIR]
                    [--audio-augmentation.noise.refresh-freq AUDIO_AUGMENTATION.NOISE.REFRESH_FREQ]
                    [--audio-augmentation.set-fixed-length.enable]
                    [--audio-augmentation.set-fixed-length.length AUDIO_AUGMENTATION.SET_FIXED_LENGTH.LENGTH]
                    [--audio-augmentation.roll.enable]
                    [--audio-augmentation.roll.window AUDIO_AUGMENTATION.ROLL.WINDOW]
                    [--audio-augmentation.mfccs.num-mfccs AUDIO_AUGMENTATION.MFCCS.NUM_MFCCS]
                    [--audio-augmentation.mfccs.window-length AUDIO_AUGMENTATION.MFCCS.WINDOW_LENGTH]
                    [--audio-augmentation.mfccs.num-frames AUDIO_AUGMENTATION.MFCCS.NUM_FRAMES]
                    [--audio-augmentation.audio-resample.enable]
                    [--audio-augmentation.audio-resample.audio-fps AUDIO_AUGMENTATION.AUDIO_RESAMPLE.AUDIO_FPS]
                    [--audio-augmentation.standardize-channels.num-channels AUDIO_AUGMENTATION.STANDARDIZE_CHANNELS.NUM_CHANNELS]
                    [--audio-augmentation.standardize-channels.enable]
                    [--audio-augmentation.gaussian-noise.enable]
                    [--audio-augmentation.gaussian-noise.audio-noise-scale-range AUDIO_AUGMENTATION.GAUSSIAN_NOISE.AUDIO_NOISE_SCALE_RANGE AUDIO_AUGMENTATION.GAUSSIAN_NOISE.AUDIO_NOISE_SCALE_RANGE]
                    [--audio-augmentation.torchaudio-save.enable]
                    [--audio-augmentation.torchaudio-save.encoding-dtype {float32,int32,int16,uint8}]
                    [--audio-augmentation.torchaudio-save.format {wav,mp3}]
                    [--audio-augmentation.torchaudio-save.backend {ffmpeg,sox,soundfile}]
                    [--image-augmentation.pil-save.enable]
                    [--image-augmentation.pil-save.file-encoding {fCHW,fHWC,TIFF,PNG,JPEG}]
                    [--image-augmentation.pil-save.quality IMAGE_AUGMENTATION.PIL_SAVE.QUALITY]
                    [--image-augmentation.shuffle-bytes.enable]
                    [--image-augmentation.shuffle-bytes.mode {reverse,random_shuffle,cyclic_half_length,stride,window_shuffle}]
                    [--image-augmentation.shuffle-bytes.stride IMAGE_AUGMENTATION.SHUFFLE_BYTES.STRIDE]
                    [--image-augmentation.shuffle-bytes.window-size IMAGE_AUGMENTATION.SHUFFLE_BYTES.WINDOW_SIZE]
                    [--image-augmentation.mask-positions.enable]
                    [--image-augmentation.mask-positions.keep-frac IMAGE_AUGMENTATION.MASK_POSITIONS.KEEP_FRAC]
                    [--image-augmentation.byte-permutation.enable]
                    [--image-augmentation.random-uniform.enable]
                    [--image-augmentation.random-uniform.width-range IMAGE_AUGMENTATION.RANDOM_UNIFORM.WIDTH_RANGE IMAGE_AUGMENTATION.RANDOM_UNIFORM.WIDTH_RANGE]
                    [--video-augmentation.save-inputs.save-dir VIDEO_AUGMENTATION.SAVE_INPUTS.SAVE_DIR]
                    [--video-augmentation.save-inputs.add-labels]
                    [--video-augmentation.save-inputs.enable]
                    [--video-augmentation.save-inputs.symlink-to-original]
                    [--video-augmentation.random-resized-crop.enable]
                    [--video-augmentation.random-resized-crop.interpolation {nearest,bilinear,bicubic}]
                    [--video-augmentation.random-resized-crop.scale VIDEO_AUGMENTATION.RANDOM_RESIZED_CROP.SCALE]
                    [--video-augmentation.random-resized-crop.aspect-ratio VIDEO_AUGMENTATION.RANDOM_RESIZED_CROP.ASPECT_RATIO]
                    [--video-augmentation.random-short-side-resize-crop.enable]
                    [--video-augmentation.random-short-side-resize-crop.interpolation {nearest,bilinear,bicubic}]
                    [--video-augmentation.random-short-side-resize-crop.short-side-min VIDEO_AUGMENTATION.RANDOM_SHORT_SIDE_RESIZE_CROP.SHORT_SIDE_MIN]
                    [--video-augmentation.random-short-side-resize-crop.short-side-max VIDEO_AUGMENTATION.RANDOM_SHORT_SIDE_RESIZE_CROP.SHORT_SIDE_MAX]
                    [--video-augmentation.random-crop.enable]
                    [--video-augmentation.random-horizontal-flip.enable]
                    [--video-augmentation.random-horizontal-flip.p VIDEO_AUGMENTATION.RANDOM_HORIZONTAL_FLIP.P]
                    [--video-augmentation.center-crop.enable]
                    [--video-augmentation.resize.enable]
                    [--video-augmentation.resize.interpolation {nearest,bilinear,bicubic}]
                    [--video-augmentation.resize.size VIDEO_AUGMENTATION.RESIZE.SIZE [VIDEO_AUGMENTATION.RESIZE.SIZE ...]]
                    [--video-augmentation.crop-by-bounding-box.enable]
                    [--video-augmentation.crop-by-bounding-box.image-size VIDEO_AUGMENTATION.CROP_BY_BOUNDING_BOX.IMAGE_SIZE]
                    [--video-augmentation.crop-by-bounding-box.channel-first]
                    [--video-augmentation.crop-by-bounding-box.multiplier-range VIDEO_AUGMENTATION.CROP_BY_BOUNDING_BOX.MULTIPLIER_RANGE VIDEO_AUGMENTATION.CROP_BY_BOUNDING_BOX.MULTIPLIER_RANGE]
                    [--video-augmentation.crop-by-bounding-box.multiplier VIDEO_AUGMENTATION.CROP_BY_BOUNDING_BOX.MULTIPLIER]
                    [--video-augmentation.crop-by-bounding-box.interpolation {nearest,bilinear,bicubic}]
                    [--video-augmentation.shuffle-audios.shuffle-ratio-train VIDEO_AUGMENTATION.SHUFFLE_AUDIOS.SHUFFLE_RATIO_TRAIN]
                    [--video-augmentation.shuffle-audios.shuffle-ratio-val VIDEO_AUGMENTATION.SHUFFLE_AUDIOS.SHUFFLE_RATIO_VAL]
                    [--video-augmentation.shuffle-audios.shuffle-ratio-test VIDEO_AUGMENTATION.SHUFFLE_AUDIOS.SHUFFLE_RATIO_TEST]
                    [--video-augmentation.shuffle-audios.generate-frame-level-targets]
                    [--video-augmentation.shuffle-audios.target-key VIDEO_AUGMENTATION.SHUFFLE_AUDIOS.TARGET_KEY]
                    [--video-augmentation.shuffle-audios.debug-mode]
                    [--video-reader.name VIDEO_READER.NAME]
                    [--video-reader.fast-video-decoding]
                    [--video-reader.frame-stack-format {sequence_first,channel_first}]
                    [--frame-augmentation.fixed-size-crop.enable [FRAME_AUGMENTATION.FIXED_SIZE_CROP.ENABLE]]
                    [--frame-augmentation.fixed-size-crop.size FRAME_AUGMENTATION.FIXED_SIZE_CROP.SIZE [FRAME_AUGMENTATION.FIXED_SIZE_CROP.SIZE ...]]
                    [--frame-augmentation.fixed-size-crop.fill FRAME_AUGMENTATION.FIXED_SIZE_CROP.FILL]
                    [--frame-augmentation.fixed-size-crop.padding-mode FRAME_AUGMENTATION.FIXED_SIZE_CROP.PADDING_MODE]
                    [--frame-augmentation.scale-jitter.enable [FRAME_AUGMENTATION.SCALE_JITTER.ENABLE]]
                    [--frame-augmentation.scale-jitter.interpolation FRAME_AUGMENTATION.SCALE_JITTER.INTERPOLATION]
                    [--frame-augmentation.scale-jitter.target-size FRAME_AUGMENTATION.SCALE_JITTER.TARGET_SIZE [FRAME_AUGMENTATION.SCALE_JITTER.TARGET_SIZE ...]]
                    [--frame-augmentation.scale-jitter.scale-range FRAME_AUGMENTATION.SCALE_JITTER.SCALE_RANGE [FRAME_AUGMENTATION.SCALE_JITTER.SCALE_RANGE ...]]
                    [--frame-augmentation.random-resized-crop.enable [FRAME_AUGMENTATION.RANDOM_RESIZED_CROP.ENABLE]]
                    [--frame-augmentation.random-resized-crop.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--frame-augmentation.random-resized-crop.scale FRAME_AUGMENTATION.RANDOM_RESIZED_CROP.SCALE]
                    [--frame-augmentation.random-resized-crop.aspect-ratio FRAME_AUGMENTATION.RANDOM_RESIZED_CROP.ASPECT_RATIO]
                    [--frame-augmentation.auto-augment.enable [FRAME_AUGMENTATION.AUTO_AUGMENT.ENABLE]]
                    [--frame-augmentation.auto-augment.policy FRAME_AUGMENTATION.AUTO_AUGMENT.POLICY]
                    [--frame-augmentation.auto-augment.interpolation FRAME_AUGMENTATION.AUTO_AUGMENT.INTERPOLATION]
                    [--frame-augmentation.rand-augment.enable [FRAME_AUGMENTATION.RAND_AUGMENT.ENABLE]]
                    [--frame-augmentation.rand-augment.num-ops FRAME_AUGMENTATION.RAND_AUGMENT.NUM_OPS]
                    [--frame-augmentation.rand-augment.magnitude FRAME_AUGMENTATION.RAND_AUGMENT.MAGNITUDE]
                    [--frame-augmentation.rand-augment.num-magnitude-bins FRAME_AUGMENTATION.RAND_AUGMENT.NUM_MAGNITUDE_BINS]
                    [--frame-augmentation.rand-augment.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--frame-augmentation.trivial-augment-wide.enable [FRAME_AUGMENTATION.TRIVIAL_AUGMENT_WIDE.ENABLE]]
                    [--frame-augmentation.trivial-augment-wide.num-magnitude-bins FRAME_AUGMENTATION.TRIVIAL_AUGMENT_WIDE.NUM_MAGNITUDE_BINS]
                    [--frame-augmentation.trivial-augment-wide.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--frame-augmentation.random-horizontal-flip.enable [FRAME_AUGMENTATION.RANDOM_HORIZONTAL_FLIP.ENABLE]]
                    [--frame-augmentation.random-horizontal-flip.p FRAME_AUGMENTATION.RANDOM_HORIZONTAL_FLIP.P]
                    [--frame-augmentation.random-rotate.enable [FRAME_AUGMENTATION.RANDOM_ROTATE.ENABLE]]
                    [--frame-augmentation.random-rotate.angle FRAME_AUGMENTATION.RANDOM_ROTATE.ANGLE]
                    [--frame-augmentation.random-rotate.mask-fill FRAME_AUGMENTATION.RANDOM_ROTATE.MASK_FILL]
                    [--frame-augmentation.resize.enable [FRAME_AUGMENTATION.RESIZE.ENABLE]]
                    [--frame-augmentation.resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--frame-augmentation.resize.size FRAME_AUGMENTATION.RESIZE.SIZE [FRAME_AUGMENTATION.RESIZE.SIZE ...]]
                    [--frame-augmentation.center-crop.enable [FRAME_AUGMENTATION.CENTER_CROP.ENABLE]]
                    [--frame-augmentation.center-crop.size FRAME_AUGMENTATION.CENTER_CROP.SIZE [FRAME_AUGMENTATION.CENTER_CROP.SIZE ...]]
                    [--frame-augmentation.ssd-crop.enable [FRAME_AUGMENTATION.SSD_CROP.ENABLE]]
                    [--frame-augmentation.ssd-crop.iou-thresholds FRAME_AUGMENTATION.SSD_CROP.IOU_THRESHOLDS [FRAME_AUGMENTATION.SSD_CROP.IOU_THRESHOLDS ...]]
                    [--frame-augmentation.ssd-crop.n-trials FRAME_AUGMENTATION.SSD_CROP.N_TRIALS]
                    [--frame-augmentation.ssd-crop.min-aspect-ratio FRAME_AUGMENTATION.SSD_CROP.MIN_ASPECT_RATIO]
                    [--frame-augmentation.ssd-crop.max-aspect-ratio FRAME_AUGMENTATION.SSD_CROP.MAX_ASPECT_RATIO]
                    [--frame-augmentation.photo-metric-distort.enable [FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.ENABLE]]
                    [--frame-augmentation.photo-metric-distort.alpha-min FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.ALPHA_MIN]
                    [--frame-augmentation.photo-metric-distort.alpha-max FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.ALPHA_MAX]
                    [--frame-augmentation.photo-metric-distort.beta-min FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.BETA_MIN]
                    [--frame-augmentation.photo-metric-distort.beta-max FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.BETA_MAX]
                    [--frame-augmentation.photo-metric-distort.gamma-min FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.GAMMA_MIN]
                    [--frame-augmentation.photo-metric-distort.gamma-max FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.GAMMA_MAX]
                    [--frame-augmentation.photo-metric-distort.delta-min FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.DELTA_MIN]
                    [--frame-augmentation.photo-metric-distort.delta-max FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.DELTA_MAX]
                    [--frame-augmentation.photo-metric-distort.p FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.P]
                    [--frame-augmentation.random-resize.enable [FRAME_AUGMENTATION.RANDOM_RESIZE.ENABLE]]
                    [--frame-augmentation.random-resize.max-scale-long-edge FRAME_AUGMENTATION.RANDOM_RESIZE.MAX_SCALE_LONG_EDGE]
                    [--frame-augmentation.random-resize.max-scale-short-edge FRAME_AUGMENTATION.RANDOM_RESIZE.MAX_SCALE_SHORT_EDGE]
                    [--frame-augmentation.random-resize.min-ratio FRAME_AUGMENTATION.RANDOM_RESIZE.MIN_RATIO]
                    [--frame-augmentation.random-resize.max-ratio FRAME_AUGMENTATION.RANDOM_RESIZE.MAX_RATIO]
                    [--frame-augmentation.random-resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--frame-augmentation.random-short-size-resize.enable [FRAME_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.ENABLE]]
                    [--frame-augmentation.random-short-size-resize.short-side-min FRAME_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.SHORT_SIDE_MIN]
                    [--frame-augmentation.random-short-size-resize.short-side-max FRAME_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.SHORT_SIDE_MAX]
                    [--frame-augmentation.random-short-size-resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}]
                    [--frame-augmentation.random-short-size-resize.max-img-dim FRAME_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.MAX_IMG_DIM]
                    [--frame-augmentation.random-erase.enable [FRAME_AUGMENTATION.RANDOM_ERASE.ENABLE]]
                    [--frame-augmentation.random-erase.p FRAME_AUGMENTATION.RANDOM_ERASE.P]
                    [--frame-augmentation.random-gaussian-noise.enable [FRAME_AUGMENTATION.RANDOM_GAUSSIAN_NOISE.ENABLE]]
                    [--frame-augmentation.random-gaussian-noise.p FRAME_AUGMENTATION.RANDOM_GAUSSIAN_NOISE.P]
                    [--frame-augmentation.random-crop.enable [FRAME_AUGMENTATION.RANDOM_CROP.ENABLE]]
                    [--frame-augmentation.random-crop.seg-class-max-ratio FRAME_AUGMENTATION.RANDOM_CROP.SEG_CLASS_MAX_RATIO]
                    [--frame-augmentation.random-crop.pad-if-needed [FRAME_AUGMENTATION.RANDOM_CROP.PAD_IF_NEEDED]]
                    [--frame-augmentation.random-crop.mask-fill FRAME_AUGMENTATION.RANDOM_CROP.MASK_FILL]
                    [--frame-augmentation.to-tensor.dtype FRAME_AUGMENTATION.TO_TENSOR.DTYPE]
                    [--frame-augmentation.to-tensor.mean-std-normalization.enable [FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.ENABLE]]
                    [--frame-augmentation.to-tensor.mean-std-normalization.mean FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.MEAN [FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.MEAN ...]]
                    [--frame-augmentation.to-tensor.mean-std-normalization.std FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.STD [FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.STD ...]]
                    [--frame-augmentation.random-order.enable [FRAME_AUGMENTATION.RANDOM_ORDER.ENABLE]]
                    [--frame-augmentation.random-order.apply-k FRAME_AUGMENTATION.RANDOM_ORDER.APPLY_K]
                    [--frame-augmentation.rand-augment.use-timm-library [FRAME_AUGMENTATION.RAND_AUGMENT.USE_TIMM_LIBRARY]]
                    [--frame-augmentation.rand-augment.timm-config-str FRAME_AUGMENTATION.RAND_AUGMENT.TIMM_CONFIG_STR]
                    [--frame-augmentation.mixup.enable [FRAME_AUGMENTATION.MIXUP.ENABLE]]
                    [--frame-augmentation.mixup.alpha FRAME_AUGMENTATION.MIXUP.ALPHA]
                    [--frame-augmentation.mixup.p FRAME_AUGMENTATION.MIXUP.P]
                    [--frame-augmentation.mixup.inplace [FRAME_AUGMENTATION.MIXUP.INPLACE]]
                    [--frame-augmentation.mixup.sample-key FRAME_AUGMENTATION.MIXUP.SAMPLE_KEY]
                    [--frame-augmentation.mixup.target-key FRAME_AUGMENTATION.MIXUP.TARGET_KEY]
                    [--frame-augmentation.cutmix.enable [FRAME_AUGMENTATION.CUTMIX.ENABLE]]
                    [--frame-augmentation.cutmix.alpha FRAME_AUGMENTATION.CUTMIX.ALPHA]
                    [--frame-augmentation.cutmix.p FRAME_AUGMENTATION.CUTMIX.P]
                    [--frame-augmentation.cutmix.inplace [FRAME_AUGMENTATION.CUTMIX.INPLACE]]
                    [--frame-augmentation.cutmix.sample-key FRAME_AUGMENTATION.CUTMIX.SAMPLE_KEY]
                    [--frame-augmentation.cutmix.target-key FRAME_AUGMENTATION.CUTMIX.TARGET_KEY]
                    [--frame-augmentation.pil-save.enable [FRAME_AUGMENTATION.PIL_SAVE.ENABLE]]
                    [--frame-augmentation.pil-save.file-encoding {fCHW,fHWC,TIFF,PNG,JPEG}]
                    [--frame-augmentation.pil-save.quality FRAME_AUGMENTATION.PIL_SAVE.QUALITY]
                    [--frame-augmentation.shuffle-bytes.enable [FRAME_AUGMENTATION.SHUFFLE_BYTES.ENABLE]]
                    [--frame-augmentation.shuffle-bytes.mode {reverse,random_shuffle,cyclic_half_length,stride,window_shuffle}]
                    [--frame-augmentation.shuffle-bytes.stride FRAME_AUGMENTATION.SHUFFLE_BYTES.STRIDE]
                    [--frame-augmentation.shuffle-bytes.window-size FRAME_AUGMENTATION.SHUFFLE_BYTES.WINDOW_SIZE]
                    [--frame-augmentation.mask-positions.enable [FRAME_AUGMENTATION.MASK_POSITIONS.ENABLE]]
                    [--frame-augmentation.mask-positions.keep-frac FRAME_AUGMENTATION.MASK_POSITIONS.KEEP_FRAC]
                    [--frame-augmentation.byte-permutation.enable [FRAME_AUGMENTATION.BYTE_PERMUTATION.ENABLE]]
                    [--frame-augmentation.random-uniform.enable [FRAME_AUGMENTATION.RANDOM_UNIFORM.ENABLE]]
                    [--frame-augmentation.random-uniform.width-range FRAME_AUGMENTATION.RANDOM_UNIFORM.WIDTH_RANGE FRAME_AUGMENTATION.RANDOM_UNIFORM.WIDTH_RANGE]
                    [--loss.category LOSS.CATEGORY]
                    [--loss.classification.name LOSS.CLASSIFICATION.NAME]
                    [--loss.classification.binary-cross-entropy.reduction {sum,mean,none,batch_mean}]
                    [--loss.classification.cross-entropy.class-weights]
                    [--loss.classification.cross-entropy.ignore-index LOSS.CLASSIFICATION.CROSS_ENTROPY.IGNORE_INDEX]
                    [--loss.classification.cross-entropy.label-smoothing LOSS.CLASSIFICATION.CROSS_ENTROPY.LABEL_SMOOTHING]
                    [--loss.classification.focal-loss.gamma LOSS.CLASSIFICATION.FOCAL_LOSS.GAMMA]
                    [--loss.classification.focal-loss.weights [LOSS.CLASSIFICATION.FOCAL_LOSS.WEIGHTS ...]]
                    [--loss.composite-loss LOSS.COMPOSITE_LOSS]
                    [--loss.detection.name LOSS.DETECTION.NAME]
                    [--loss.detection.mask-rcnn-loss.classifier-weight LOSS.DETECTION.MASK_RCNN_LOSS.CLASSIFIER_WEIGHT]
                    [--loss.detection.mask-rcnn-loss.box-reg-weight LOSS.DETECTION.MASK_RCNN_LOSS.BOX_REG_WEIGHT]
                    [--loss.detection.mask-rcnn-loss.mask-weight LOSS.DETECTION.MASK_RCNN_LOSS.MASK_WEIGHT]
                    [--loss.detection.mask-rcnn-loss.objectness-weight LOSS.DETECTION.MASK_RCNN_LOSS.OBJECTNESS_WEIGHT]
                    [--loss.detection.mask-rcnn-loss.rpn-box-reg LOSS.DETECTION.MASK_RCNN_LOSS.RPN_BOX_REG]
                    [--loss.detection.ssd-multibox-loss.neg-pos-ratio LOSS.DETECTION.SSD_MULTIBOX_LOSS.NEG_POS_RATIO]
                    [--loss.detection.ssd-multibox-loss.max-monitor-iter LOSS.DETECTION.SSD_MULTIBOX_LOSS.MAX_MONITOR_ITER]
                    [--loss.detection.ssd-multibox-loss.update-wt-freq LOSS.DETECTION.SSD_MULTIBOX_LOSS.UPDATE_WT_FREQ]
                    [--loss.detection.ssd-multibox-loss.label-smoothing LOSS.DETECTION.SSD_MULTIBOX_LOSS.LABEL_SMOOTHING]
                    [--loss.distillation.name LOSS.DISTILLATION.NAME]
                    [--loss.distillation.hard-distillation.topk LOSS.DISTILLATION.HARD_DISTILLATION.TOPK]
                    [--loss.distillation.hard-distillation.label-smoothing LOSS.DISTILLATION.HARD_DISTILLATION.LABEL_SMOOTHING]
                    [--loss.distillation.soft-kl-loss.temperature LOSS.DISTILLATION.SOFT_KL_LOSS.TEMPERATURE]
                    [--loss.language-modeling.name LOSS.LANGUAGE_MODELING.NAME]
                    [--loss.language-modeling.cross-entropy.ignore-index LOSS.LANGUAGE_MODELING.CROSS_ENTROPY.IGNORE_INDEX]
                    [--loss.language-modeling.cross-entropy.label-smoothing LOSS.LANGUAGE_MODELING.CROSS_ENTROPY.LABEL_SMOOTHING]
                    [--loss.language-modeling.cross-entropy.use-z-loss]
                    [--loss.language-modeling.cross-entropy.z-loss-eps LOSS.LANGUAGE_MODELING.CROSS_ENTROPY.Z_LOSS_EPS]
                    [--loss.language-modeling.cross-entropy-for-kv-prediction.ignore-index LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.IGNORE_INDEX]
                    [--loss.language-modeling.cross-entropy-for-kv-prediction.label-smoothing LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.LABEL_SMOOTHING]
                    [--loss.language-modeling.cross-entropy-for-kv-prediction.use-z-loss]
                    [--loss.language-modeling.cross-entropy-for-kv-prediction.z-loss-eps LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.Z_LOSS_EPS]
                    [--loss.language-modeling.cross-entropy-for-kv-prediction.auxiliary-loss LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.AUXILIARY_LOSS]
                    [--loss.language-modeling.cross-entropy-for-kv-prediction.base-loss LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.BASE_LOSS]
                    [--loss.language-modeling.cross-entropy-for-kv-prediction.kv-loss LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.KV_LOSS]
                    [--loss.multi-modal-image-text.name LOSS.MULTI_MODAL_IMAGE_TEXT.NAME]
                    [--loss.neural-augmentation.perceptual-metric {psnr}]
                    [--loss.neural-augmentation.target-value LOSS.NEURAL_AUGMENTATION.TARGET_VALUE [LOSS.NEURAL_AUGMENTATION.TARGET_VALUE ...]]
                    [--loss.neural-augmentation.curriculum-method {linear,cosine}]
                    [--loss.neural-augmentation.alpha LOSS.NEURAL_AUGMENTATION.ALPHA]
                    [--loss.segmentation.name LOSS.SEGMENTATION.NAME]
                    [--loss.segmentation.cross-entropy.class-weights]
                    [--loss.segmentation.cross-entropy.ignore-index LOSS.SEGMENTATION.CROSS_ENTROPY.IGNORE_INDEX]
                    [--loss.segmentation.cross-entropy.aux-weight LOSS.SEGMENTATION.CROSS_ENTROPY.AUX_WEIGHT]
                    [--loss.segmentation.cross-entropy.label-smoothing LOSS.SEGMENTATION.CROSS_ENTROPY.LABEL_SMOOTHING]
                    [--optim.name OPTIM.NAME] [--optim.eps OPTIM.EPS]
                    [--optim.weight-decay OPTIM.WEIGHT_DECAY]
                    [--optim.no-decay-bn-filter-bias]
                    [--optim.bypass-parameters-check]
                    [--optim.adam.beta1 OPTIM.ADAM.BETA1]
                    [--optim.adam.beta2 OPTIM.ADAM.BETA2]
                    [--optim.adam.amsgrad] [--optim.adam.eps OPTIM.ADAM.EPS]
                    [--optim.adamw.beta1 OPTIM.ADAMW.BETA1]
                    [--optim.adamw.beta2 OPTIM.ADAMW.BETA2]
                    [--optim.adamw.amsgrad]
                    [--optim.adamw.eps OPTIM.ADAMW.EPS]
                    [--optim.sgd.momentum OPTIM.SGD.MOMENTUM]
                    [--optim.sgd.nesterov] [--scheduler.name SCHEDULER.NAME]
                    [--scheduler.lr SCHEDULER.LR]
                    [--scheduler.max-epochs SCHEDULER.MAX_EPOCHS]
                    [--scheduler.max-iterations SCHEDULER.MAX_ITERATIONS]
                    [--scheduler.warmup-iterations SCHEDULER.WARMUP_ITERATIONS]
                    [--scheduler.warmup-init-lr SCHEDULER.WARMUP_INIT_LR]
                    [--scheduler.is-iteration-based]
                    [--scheduler.adjust-period-for-epochs]
                    [--scheduler.cosine.min-lr SCHEDULER.COSINE.MIN_LR]
                    [--scheduler.cosine.max-lr SCHEDULER.COSINE.MAX_LR]
                    [--scheduler.cyclic.min-lr SCHEDULER.CYCLIC.MIN_LR]
                    [--scheduler.cyclic.last-cycle-end-lr SCHEDULER.CYCLIC.LAST_CYCLE_END_LR]
                    [--scheduler.cyclic.total-cycles SCHEDULER.CYCLIC.TOTAL_CYCLES]
                    [--scheduler.cyclic.epochs-per-cycle SCHEDULER.CYCLIC.EPOCHS_PER_CYCLE]
                    [--scheduler.cyclic.steps SCHEDULER.CYCLIC.STEPS [SCHEDULER.CYCLIC.STEPS ...]]
                    [--scheduler.cyclic.gamma SCHEDULER.CYCLIC.GAMMA]
                    [--scheduler.cyclic.last-cycle-type {cosine,linear}]
                    [--scheduler.fixed.lr SCHEDULER.FIXED.LR]
                    [--scheduler.multi-step.lr SCHEDULER.MULTI_STEP.LR]
                    [--scheduler.multi-step.gamma SCHEDULER.MULTI_STEP.GAMMA]
                    [--scheduler.multi-step.milestones SCHEDULER.MULTI_STEP.MILESTONES [SCHEDULER.MULTI_STEP.MILESTONES ...]]
                    [--scheduler.polynomial.power SCHEDULER.POLYNOMIAL.POWER]
                    [--scheduler.polynomial.start-lr SCHEDULER.POLYNOMIAL.START_LR]
                    [--scheduler.polynomial.end-lr SCHEDULER.POLYNOMIAL.END_LR]
                    [--ddp.rank DDP.RANK] [--ddp.world-size DDP.WORLD_SIZE]
                    [--ddp.dist-url DDP.DIST_URL]
                    [--ddp.dist-port DDP.DIST_PORT]
                    [--ddp.device-id DDP.DEVICE_ID]
                    [--ddp.backend DDP.BACKEND] [--ddp.find-unused-params]
                    [--ddp.use-deprecated-data-parallel]
                    [--stats.val STATS.VAL [STATS.VAL ...]]
                    [--stats.train STATS.TRAIN [STATS.TRAIN ...]]
                    [--stats.checkpoint-metric STATS.CHECKPOINT_METRIC]
                    [--stats.checkpoint-metric-max]
                    [--stats.coco-map.iou-types {bbox,segm} [{bbox,segm} ...]]
                    [--taskname TASKNAME] [--common.seed COMMON.SEED]
                    [--common.config-file COMMON.CONFIG_FILE]
                    [--common.results-loc COMMON.RESULTS_LOC]
                    [--common.logs-loc COMMON.LOGS_LOC]
                    [--common.run-label COMMON.RUN_LABEL]
                    [--common.eval-stage-name COMMON.EVAL_STAGE_NAME]
                    [--common.resume COMMON.RESUME]
                    [--common.finetune COMMON.FINETUNE]
                    [--common.finetune-ema COMMON.FINETUNE_EMA]
                    [--common.mixed-precision]
                    [--common.mixed-precision-dtype COMMON.MIXED_PRECISION_DTYPE]
                    [--common.accum-freq COMMON.ACCUM_FREQ]
                    [--common.accum-after-epoch COMMON.ACCUM_AFTER_EPOCH]
                    [--common.log-freq COMMON.LOG_FREQ] [--common.auto-resume]
                    [--common.grad-clip COMMON.GRAD_CLIP]
                    [--common.k-best-checkpoints COMMON.K_BEST_CHECKPOINTS]
                    [--common.save-all-checkpoints] [--common.channels-last]
                    [--common.tensorboard-logging] [--common.file-logging]
                    [--common.override-kwargs [COMMON.OVERRIDE_KWARGS ...]]
                    [--common.enable-coreml-compatible-module]
                    [--common.debug-mode]
                    [--common.save-interval-freq COMMON.SAVE_INTERVAL_FREQ]
                    [--common.eval-every-k-iterations COMMON.EVAL_EVERY_K_ITERATIONS]
                    [--common.set-grad-to-none]
                    [--text-tokenizer.name TEXT_TOKENIZER.NAME]
                    [--text-tokenizer.sot-token TEXT_TOKENIZER.SOT_TOKEN]
                    [--text-tokenizer.eot-token TEXT_TOKENIZER.EOT_TOKEN]
                    [--text-tokenizer.pad-token TEXT_TOKENIZER.PAD_TOKEN]
                    [--text-tokenizer.clip.merges-path TEXT_TOKENIZER.CLIP.MERGES_PATH]
                    [--text-tokenizer.clip.encoder-json-path TEXT_TOKENIZER.CLIP.ENCODER_JSON_PATH]
                    [--text-tokenizer.sentence-piece.model-path TEXT_TOKENIZER.SENTENCE_PIECE.MODEL_PATH]
                    [--text-tokenizer.sentence-piece.enable-nfc-normalization]
                    [--text-tokenizer.sentence-piece.append-sot-token]
                    [--text-tokenizer.sentence-piece.append-eot-token]
                    [--text-tokenizer.openai-clip.bpe-path TEXT_TOKENIZER.OPENAI_CLIP.BPE_PATH]
                    [--stats.metrics.retrieval-cmc.subset-fraction STATS.METRICS.RETRIEVAL_CMC.SUBSET_FRACTION]
                    [--stats.metrics.retrieval-cmc.k STATS.METRICS.RETRIEVAL_CMC.K]
                    [--stats.metrics.retrieval-cmc.distance-metric {l2,cosine}]
                    [--stats.metrics.img-text-retrieval.distance-metric {cosine,l2}]
                    [--stats.metrics.multiclass-classification-pr.include-curve]
                    [--stats.metrics.multiclass-classification-pr.include-classwise-ap]
                    [--stats.metrics.multiclass-classification-pr.suppress-warnings]
                    [--stats.metrics.prob-hist.num-bins STATS.METRICS.PROB_HIST.NUM_BINS]
                    [--train-eval-pipeline.name TRAIN_EVAL_PIPELINE.NAME]

Training arguments

options:
  -h, --help            show this help message and exit
  --model.layer.linear-init MODEL.LAYER.LINEAR_INIT
                        Init type for linear layers
  --model.layer.linear-init-std-dev MODEL.LAYER.LINEAR_INIT_STD_DEV
                        Std deviation for Linear layers
  --model.layer.group-linear-init MODEL.LAYER.GROUP_LINEAR_INIT
                        Init type for group linear layers
  --model.layer.group-linear-init-std-dev MODEL.LAYER.GROUP_LINEAR_INIT_STD_DEV
                        Std deviation for group linear layers
  --model.layer.conv-init-std-dev MODEL.LAYER.CONV_INIT_STD_DEV
                        Std deviation for conv layers
  --teacher.model.text.name TEACHER.MODEL.TEXT.NAME
                        Name of the text encoder
  --teacher.model.text.padding-index TEACHER.MODEL.TEXT.PADDING_INDEX
                        Padding index. Defaults to None.
  --teacher.model.text.context-length TEACHER.MODEL.TEXT.CONTEXT_LENGTH
                        Context length. Defaults to None.
  --teacher.model.text.vocab-size TEACHER.MODEL.TEXT.VOCAB_SIZE
                        Vocabulary size. Defaults to None.
  --teacher.model.text.transformer.model-dim TEACHER.MODEL.TEXT.TRANSFORMER.MODEL_DIM
                        Model dimension of the transformer model
  --teacher.model.text.transformer.no-scale-embedding [TEACHER.MODEL.TEXT.TRANSFORMER.NO_SCALE_EMBEDDING]
                        Do not scale the output of embedding layer in
                        TextTransformer
  --teacher.model.text.transformer.no-pos-embedding [TEACHER.MODEL.TEXT.TRANSFORMER.NO_POS_EMBEDDING]
                        Do not add positional embeddings to the output of
                        embedding layer in TextTransformer
  --teacher.model.text.transformer.embed-dropout TEACHER.MODEL.TEXT.TRANSFORMER.EMBED_DROPOUT
                        Dropout in embedding layer
  --teacher.model.text.transformer.n-transformer-layers TEACHER.MODEL.TEXT.TRANSFORMER.N_TRANSFORMER_LAYERS
                        Number of transformer layers in TextTransformer
  --teacher.model.text.transformer.n-heads-per-layer TEACHER.MODEL.TEXT.TRANSFORMER.N_HEADS_PER_LAYER [TEACHER.MODEL.TEXT.TRANSFORMER.N_HEADS_PER_LAYER ...]
                        Number of transformer heads per transformer layer
  --teacher.model.text.transformer.ffn-multiplier-per-layer TEACHER.MODEL.TEXT.TRANSFORMER.FFN_MULTIPLIER_PER_LAYER [TEACHER.MODEL.TEXT.TRANSFORMER.FFN_MULTIPLIER_PER_LAYER ...]
                        FFN multiplier for each transformer layer
  --teacher.model.text.transformer.attn-dropout TEACHER.MODEL.TEXT.TRANSFORMER.ATTN_DROPOUT
                        Dropout in multi-head attention
  --teacher.model.text.transformer.ffn-dropout TEACHER.MODEL.TEXT.TRANSFORMER.FFN_DROPOUT
                        Dropout between linear layers in FFN
  --teacher.model.text.transformer.dropout TEACHER.MODEL.TEXT.TRANSFORMER.DROPOUT
                        Dropout in transformer
  --teacher.model.text.transformer.norm-layer TEACHER.MODEL.TEXT.TRANSFORMER.NORM_LAYER
                        Normalization layer
  --teacher.model.text.transformer.sinusoidal-pos-emb [TEACHER.MODEL.TEXT.TRANSFORMER.SINUSOIDAL_POS_EMB]
                        Use sinusoidal positional embedding
  --teacher.model.text.transformer.causal-masking [TEACHER.MODEL.TEXT.TRANSFORMER.CAUSAL_MASKING]
                        Use causal masking
  --teacher.model.text.transformer.classes-per-split-zero-shot TEACHER.MODEL.TEXT.TRANSFORMER.CLASSES_PER_SPLIT_ZERO_SHOT
                        Divide zero-shot classes into these many chunks, for
                        faster processing
  --teacher.model.image-projection-head.name TEACHER.MODEL.IMAGE_PROJECTION_HEAD.NAME
                        Name of the image projection head
  --teacher.model.image-projection-head.lr-multiplier TEACHER.MODEL.IMAGE_PROJECTION_HEAD.LR_MULTIPLIER
                        LR multiplier for image projection head
  --teacher.model.image-projection-head.attention-pool-nchw2nc.num-pos-embeddings TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NUM_POS_EMBEDDINGS
                        Number of positional embeddings
  --teacher.model.image-projection-head.attention-pool-nchw2nc.use-sinusoidal-pos-embeddings [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.USE_SINUSOIDAL_POS_EMBEDDINGS]
                        Use sinusoidal positional embeddings instead of
                        learnable
  --teacher.model.image-projection-head.attention-pool-nchw2nc.num-attn-heads TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NUM_ATTN_HEADS
                        Number of attention heads in AttentionPool2dHead
  --teacher.model.image-projection-head.attention-pool-nchw2nc.no-feature-normalization [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NO_FEATURE_NORMALIZATION]
                        Don't normalize image features
  --teacher.model.image-projection-head.attention-pool-nchw2nc.use-pytorch-mha [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.USE_PYTORCH_MHA]
                        Use Pytorch Multi-head attention
  --teacher.model.image-projection-head.global-pool-nchw2nc.no-feature-normalization [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.GLOBAL_POOL_NCHW2NC.NO_FEATURE_NORMALIZATION]
                        Don't normalize image features. Defaults to False.
  --teacher.model.image-projection-head.global-pool-nchw2nc.identity-if-same-size [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.GLOBAL_POOL_NCHW2NC.IDENTITY_IF_SAME_SIZE]
                        Use identity projection when projection input/output
                        dims are the same. Defaults to False.
  --teacher.model.image-projection-head.simple-projection-nc2nc.no-feature-normalization [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.SIMPLE_PROJECTION_NC2NC.NO_FEATURE_NORMALIZATION]
                        Don't normalize image features. Defaults to False.
  --teacher.model.image-projection-head.simple-projection-nc2nc.identity-if-same-size [TEACHER.MODEL.IMAGE_PROJECTION_HEAD.SIMPLE_PROJECTION_NC2NC.IDENTITY_IF_SAME_SIZE]
                        Use identity projection when projection input/output
                        dims are the same. Defaults to False
  --teacher.model.resume-exclude-scopes TEACHER.MODEL.RESUME_EXCLUDE_SCOPES
                        Comma-separated list of parameter scopes (regex
                        strings) to exclude when loading a pre-trained model
  --teacher.model.ignore-missing-scopes TEACHER.MODEL.IGNORE_MISSING_SCOPES
                        Comma-separated list of parameter scopes (regex
                        strings) to ignore if they are missing from the pre-
                        training model
  --teacher.model.rename-scopes-map TEACHER.MODEL.RENAME_SCOPES_MAP
                        A mapping from checkpoint variable names to match the
                        existing model names. The mapping is represented as a
                        List[List[str]], e.g. [['before', 'after'], ['this',
                        'that']]. Note: only loading from Yaml file is
                        supported for this argument.
  --teacher.model.freeze-modules TEACHER.MODEL.FREEZE_MODULES
                        Comma-separated list of parameter scopes (regex
                        strings) to freeze.
  --teacher.model.activation-checkpointing [TEACHER.MODEL.ACTIVATION_CHECKPOINTING]
                        If enabled, layer specified in model using
                        'get_activation_checkpoint_submodule_class' would be
                        used for activation checkpointing (a.k.a. gradient
                        checkpointing).
  --teacher.model.lora.config TEACHER.MODEL.LORA.CONFIG
                        A json-formatted configuration for the LoRA model. See
                        corenet/modeling/models/language_modeling/lora.py for
                        details.
  --teacher.model.lora.use-lora [TEACHER.MODEL.LORA.USE_LORA]
                        If set, use LoRA for the model. Note that parameters
                        are not automatically frozen. They must be frozen with
                        --model.freeze-modules.
  --teacher.model.audio-classification.name TEACHER.MODEL.AUDIO_CLASSIFICATION.NAME
                        Name of the audio classification model. Defaults to
                        None.
  --teacher.model.audio-classification.pretrained TEACHER.MODEL.AUDIO_CLASSIFICATION.PRETRAINED
                        Path of the pretrained backbone. Defaults to None.
  --teacher.model.classification.byteformer.dropout TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.DROPOUT
                        Dropout in Byteformer layers. Defaults to 0.0.
  --teacher.model.classification.byteformer.stochastic-dropout TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.STOCHASTIC_DROPOUT
                        Probability of applying stochastic dropout to
                        TransformerEncoder submodules. Defaults to 0.0.
  --teacher.model.classification.byteformer.norm-layer {group_norm,layer_norm,layer_norm_nchw,layer_norm_2d,layer_norm_fp32,instance_norm_2d,instance_norm,instance_norm_1d,batch_norm_2d,batch_norm,batch_norm_fp32,batch_norm_1d,batch_norm_3d,sync_batch_norm,sync_batch_norm_fp32,rms_norm}
                        Normalization layer in Byteformer. Defaults to
                        LayerNorm.
  --teacher.model.classification.byteformer.sinusoidal-pos-emb [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.SINUSOIDAL_POS_EMB]
                        Use sinusoidal instead of learnable positional
                        encoding. Defaults to False.
  --teacher.model.classification.byteformer.use-pytorch-mha [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.USE_PYTORCH_MHA]
                        Use PyTorch's native multi-head attention. Defaults to
                        False.
  --teacher.model.classification.byteformer.mode {tiny,small,base,huge}
                        Byteformer mode, which determines the model size.
                        Defaults to tiny.
  --teacher.model.classification.byteformer.vocab-size TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.VOCAB_SIZE
                        The vocab size of the token embedding. Defaults to
                        257,corresponding to the number of unique bytes (256)
                        plus 1 more for the mask token.
  --teacher.model.classification.byteformer.max-num-tokens TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.MAX_NUM_TOKENS
                        The maximum number of tokens that can be input to the
                        network. Defaults to 10000.
  --teacher.model.classification.byteformer.conv-kernel-size TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.CONV_KERNEL_SIZE
                        The size of the kernel of the initial downsampling
                        conv1d. Defaults to 16.
  --teacher.model.classification.byteformer.window-sizes [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.WINDOW_SIZES ...]
                        A list of window sizes used in shifted window
                        attention. If the list is length 1, the same window
                        size is used for all windows. Defaults to 128 for all
                        windows.
  --teacher.model.classification.byteformer.window-shifts [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.WINDOW_SHIFTS ...]
                        A list of shifts used in shifted window attention.
                        Defaults to values that alternate between 0 and 64.
  --teacher.model.classification.byteformer.downsample [TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.DOWNSAMPLE ...]
                        A list of boolean values, where the i'th element
                        specifies whether to downsample after the transformer
                        block with index i. Defaults to [True, True, False,
                        True, False, True, False, True, False, True, False,
                        False].
  --teacher.model.classification.byteformer.padding-index TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.PADDING_INDEX
                        The index used for padding tokens. Defaults to -1.
  --teacher.model.classification.byteformer.dummy-input-token-length TEACHER.MODEL.CLASSIFICATION.BYTEFORMER.DUMMY_INPUT_TOKEN_LENGTH
                        The token length to use for dummy inputs. Defaults to
                        48564, corresponding to the average length of 224x224
                        JPEG images from ImageNet.
  --teacher.model.classification.classifier-dropout TEACHER.MODEL.CLASSIFICATION.CLASSIFIER_DROPOUT
                        Dropout rate in classifier
  --teacher.model.classification.name TEACHER.MODEL.CLASSIFICATION.NAME
                        Model name
  --teacher.model.classification.n-classes TEACHER.MODEL.CLASSIFICATION.N_CLASSES
                        Number of classes in the dataset
  --teacher.model.classification.pretrained TEACHER.MODEL.CLASSIFICATION.PRETRAINED
                        Path of the pretrained backbone
  --teacher.model.classification.freeze-batch-norm [TEACHER.MODEL.CLASSIFICATION.FREEZE_BATCH_NORM]
                        Freeze batch norm layers
  --teacher.model.classification.activation.name TEACHER.MODEL.CLASSIFICATION.ACTIVATION.NAME
                        Non-linear function name (e.g., relu)
  --teacher.model.classification.activation.inplace [TEACHER.MODEL.CLASSIFICATION.ACTIVATION.INPLACE]
                        Inplace non-linear functions
  --teacher.model.classification.activation.neg-slope TEACHER.MODEL.CLASSIFICATION.ACTIVATION.NEG_SLOPE
                        Negative slope in leaky relu
  --teacher.model.classification.finetune-pretrained-model [TEACHER.MODEL.CLASSIFICATION.FINETUNE_PRETRAINED_MODEL]
                        Finetune a pretrained model
  --teacher.model.classification.n-pretrained-classes TEACHER.MODEL.CLASSIFICATION.N_PRETRAINED_CLASSES
                        Number of pre-trained classes
  --teacher.model.classification.gradient-checkpointing [TEACHER.MODEL.CLASSIFICATION.GRADIENT_CHECKPOINTING]
                        Checkpoint output of each spatial level in the
                        classification backbone. Note thatwe only take care of
                        checkpointing in {}. If custom forward functions are
                        used, pleaseimplement checkpointing accordingly. This
                        option is deprecated in favor or --model.activation-
                        checkpointing.
  --teacher.model.classification.enable-layer-wise-lr-decay [TEACHER.MODEL.CLASSIFICATION.ENABLE_LAYER_WISE_LR_DECAY]
                        Enable layer-wise LR.
  --teacher.model.classification.layer-wise-lr-decay-rate TEACHER.MODEL.CLASSIFICATION.LAYER_WISE_LR_DECAY_RATE
                        Layer-wise LR decay range. Each model needs to define
                        how layer-wise LR should be decayed.For ViT, we decay
                        layer_wise_lr_decay_rate ** (n_layers - i), where i is
                        the layer index.
  --teacher.model.classification.efficientnet.mode {b0,b1,b2,b3,b4,b5,b6,b7}
  --teacher.model.classification.efficientnet.stochastic-depth-prob TEACHER.MODEL.CLASSIFICATION.EFFICIENTNET.STOCHASTIC_DEPTH_PROB
  --teacher.model.classification.fastvit.variant TEACHER.MODEL.CLASSIFICATION.FASTVIT.VARIANT
                        Variant string for FastViT. Default: T8
  --teacher.model.classification.fastvit.inference-mode TEACHER.MODEL.CLASSIFICATION.FASTVIT.INFERENCE_MODE
                        Flag to instantiate inference mode architecture.
                        Default: False
  --teacher.model.classification.fastvit.dropout TEACHER.MODEL.CLASSIFICATION.FASTVIT.DROPOUT
                        Dropout rate for regularization. Default: 0.0
  --teacher.model.classification.fastvit.drop-path TEACHER.MODEL.CLASSIFICATION.FASTVIT.DROP_PATH
                        Drop path rate. Default: 0.0
  --teacher.model.classification.fastvit.use-layer-scale TEACHER.MODEL.CLASSIFICATION.FASTVIT.USE_LAYER_SCALE
                        Flag to turn on layer scale regularization. Default:
                        True
  --teacher.model.classification.fastvit.layer-scale-init-value TEACHER.MODEL.CLASSIFICATION.FASTVIT.LAYER_SCALE_INIT_VALUE
                        Drop path rate. Default: 1e-5
  --teacher.model.classification.mobilenetv1.width-multiplier TEACHER.MODEL.CLASSIFICATION.MOBILENETV1.WIDTH_MULTIPLIER
                        Width multiplier for MobileNetv1. Default: 1.0
  --teacher.model.classification.mobilenetv2.width-multiplier TEACHER.MODEL.CLASSIFICATION.MOBILENETV2.WIDTH_MULTIPLIER
                        Width multiplier for MobileNetv2. Default: 1.0
  --teacher.model.classification.mobilenetv3.mode {small,large}
                        Configuration for mobilenetv3. Default: large
  --teacher.model.classification.mobilenetv3.width-multiplier TEACHER.MODEL.CLASSIFICATION.MOBILENETV3.WIDTH_MULTIPLIER
                        Width multiplier for mobilenetv3. Default: 1.0
  --teacher.model.classification.mobileone.variant TEACHER.MODEL.CLASSIFICATION.MOBILEONE.VARIANT
                        Variant string for MobileOne. Default: s1
  --teacher.model.classification.mobileone.inference-mode TEACHER.MODEL.CLASSIFICATION.MOBILEONE.INFERENCE_MODE
                        Flag to instantiate inference mode architecture.
                        Default: False
  --teacher.model.classification.mit.mode {xx_small,x_small,small}
                        MobileViT mode. Defaults to small
  --teacher.model.classification.mit.attn-dropout TEACHER.MODEL.CLASSIFICATION.MIT.ATTN_DROPOUT
                        Dropout in attention layer. Defaults to 0.0
  --teacher.model.classification.mit.ffn-dropout TEACHER.MODEL.CLASSIFICATION.MIT.FFN_DROPOUT
                        Dropout between FFN layers. Defaults to 0.0
  --teacher.model.classification.mit.dropout TEACHER.MODEL.CLASSIFICATION.MIT.DROPOUT
                        Dropout in Transformer layer. Defaults to 0.0
  --teacher.model.classification.mit.transformer-norm-layer TEACHER.MODEL.CLASSIFICATION.MIT.TRANSFORMER_NORM_LAYER
                        Normalization layer in transformer. Defaults to
                        LayerNorm
  --teacher.model.classification.mit.no-fuse-local-global-features [TEACHER.MODEL.CLASSIFICATION.MIT.NO_FUSE_LOCAL_GLOBAL_FEATURES]
                        Do not combine local and global features in MobileViT
                        block
  --teacher.model.classification.mit.conv-kernel-size TEACHER.MODEL.CLASSIFICATION.MIT.CONV_KERNEL_SIZE
                        Kernel size of Conv layers in MobileViT block
  --teacher.model.classification.mit.head-dim TEACHER.MODEL.CLASSIFICATION.MIT.HEAD_DIM
                        Head dimension in transformer
  --teacher.model.classification.mit.number-heads TEACHER.MODEL.CLASSIFICATION.MIT.NUMBER_HEADS
                        Number of heads in transformer
  --teacher.model.classification.mitv2.attn-dropout TEACHER.MODEL.CLASSIFICATION.MITV2.ATTN_DROPOUT
                        Dropout in attention layer. Defaults to 0.0
  --teacher.model.classification.mitv2.ffn-dropout TEACHER.MODEL.CLASSIFICATION.MITV2.FFN_DROPOUT
                        Dropout between FFN layers. Defaults to 0.0
  --teacher.model.classification.mitv2.dropout TEACHER.MODEL.CLASSIFICATION.MITV2.DROPOUT
                        Dropout in attention layer. Defaults to 0.0
  --teacher.model.classification.mitv2.width-multiplier TEACHER.MODEL.CLASSIFICATION.MITV2.WIDTH_MULTIPLIER
                        Width multiplier. Defaults to 1.0
  --teacher.model.classification.mitv2.attn-norm-layer TEACHER.MODEL.CLASSIFICATION.MITV2.ATTN_NORM_LAYER
                        Norm layer in attention block. Defaults to LayerNorm
  --teacher.model.classification.regnet.mode TEACHER.MODEL.CLASSIFICATION.REGNET.MODE
                        The RegNet<mode> to use. Must be one of x_200mf,
                        x_400mf, x_600mf, x_800mf, x_1.6gf, x_3.2gf, x_4.0gf,
                        x_6.4gf, x_8.0gf, x_12gf, x_16gf, x_32gf, y_200mf,
                        y_400mf, y_800mf, y_600mf, y_1.6gf, y_3.2gf, y_4.0gf,
                        y_6.4gf, y_8.0gf, y_12gf, y_16gf, y_32gf. Defaults to
                        y_4.0gf.
  --teacher.model.classification.regnet.stochastic-depth-prob TEACHER.MODEL.CLASSIFICATION.REGNET.STOCHASTIC_DEPTH_PROB
                        Stochastic depth drop probability in RegNet blocks.
                        Defaults to 0.
  --teacher.model.classification.regnet.stem-width TEACHER.MODEL.CLASSIFICATION.REGNET.STEM_WIDTH
                        The number of output channels of the first conv layer.
                        Defaults to 32
  --teacher.model.classification.resnet.depth TEACHER.MODEL.CLASSIFICATION.RESNET.DEPTH
  --teacher.model.classification.resnet.dropout TEACHER.MODEL.CLASSIFICATION.RESNET.DROPOUT
                        Dropout in Resnet blocks. Defaults to 0.
  --teacher.model.classification.resnet.stochastic-depth-prob TEACHER.MODEL.CLASSIFICATION.RESNET.STOCHASTIC_DEPTH_PROB
                        Stochastic depth drop probability in Resnet blocks.
                        Defaults to 0.
  --teacher.model.classification.resnet.se-resnet [TEACHER.MODEL.CLASSIFICATION.RESNET.SE_RESNET]
                        Whether to use SE block to construct SE-ResNet model.
                        Defaults to False.
  --teacher.model.classification.swin.mode TEACHER.MODEL.CLASSIFICATION.SWIN.MODE
                        SwinTransformer mode. Default is swin_t
  --teacher.model.classification.swin.stochastic-depth-prob TEACHER.MODEL.CLASSIFICATION.SWIN.STOCHASTIC_DEPTH_PROB
  --teacher.model.classification.swin.extract-end-point-format {nchw,nhwc}
                        End point extraction format in Swin Transformer. This
                        is useful for down-stream tasks where task-specific
                        heads are either in nhwc format or nchw format.
                        Defaults to nchw.
  --teacher.model.classification.vit.mode {tiny,small,base,large,huge}
                        ViT mode. Default is base.
  --teacher.model.classification.vit.dropout TEACHER.MODEL.CLASSIFICATION.VIT.DROPOUT
                        Dropout in Transformer layers. Defaults to 0.0.
  --teacher.model.classification.vit.stochastic-dropout TEACHER.MODEL.CLASSIFICATION.VIT.STOCHASTIC_DROPOUT
                        Stochastic Dropout in Transformer layers. Defaults to
                        0.0.
  --teacher.model.classification.vit.norm-layer TEACHER.MODEL.CLASSIFICATION.VIT.NORM_LAYER
                        Normalization layer to be used in Transformer layer.
                        Defaults to LayerNorm.
  --teacher.model.classification.vit.sinusoidal-pos-emb [TEACHER.MODEL.CLASSIFICATION.VIT.SINUSOIDAL_POS_EMB]
                        Use sinusoidal instead of learnable positional
                        embedding. Defaults to False.
  --teacher.model.classification.vit.no-cls-token [TEACHER.MODEL.CLASSIFICATION.VIT.NO_CLS_TOKEN]
                        Do not use classification token. Defaults to False.
  --teacher.model.classification.vit.use-simple-fpn [TEACHER.MODEL.CLASSIFICATION.VIT.USE_SIMPLE_FPN]
                        Add simple FPN for down-stream tasks (e.g.,
                        detection). Defaults to False.
  --teacher.model.classification.vit.use-flash-attention [TEACHER.MODEL.CLASSIFICATION.VIT.USE_FLASH_ATTENTION]
                        Use Transformer layers with flash attention for
                        efficiently computing scaled dot-product attention.
                        Defaults to False.
  --teacher.model.detection.name TEACHER.MODEL.DETECTION.NAME
                        Detection model name
  --teacher.model.detection.n-classes TEACHER.MODEL.DETECTION.N_CLASSES
                        Number of classes in the dataset. Defaults to 80.
  --teacher.model.detection.pretrained TEACHER.MODEL.DETECTION.PRETRAINED
                        Path of the pretrained detection model. Defaults to
                        None.
  --teacher.model.detection.output-stride TEACHER.MODEL.DETECTION.OUTPUT_STRIDE
                        Output stride of the classification network. Defaults
                        to None.
  --teacher.model.detection.replace-stride-with-dilation [TEACHER.MODEL.DETECTION.REPLACE_STRIDE_WITH_DILATION]
                        Replace stride with dilation
  --teacher.model.detection.freeze-batch-norm [TEACHER.MODEL.DETECTION.FREEZE_BATCH_NORM]
                        Freeze batch norm layers in detection model. Defaults
                        to False.
  --teacher.model.detection.mask-rcnn.backbone-projection-channels TEACHER.MODEL.DETECTION.MASK_RCNN.BACKBONE_PROJECTION_CHANNELS
                        Projection channels for the encoder in Mask-RCNN
  --teacher.model.detection.mask-rcnn.backbone-lr-multiplier TEACHER.MODEL.DETECTION.MASK_RCNN.BACKBONE_LR_MULTIPLIER
                        LR multiplier for MASK RCNN head
  --teacher.model.detection.mask-rcnn.output-strides TEACHER.MODEL.DETECTION.MASK_RCNN.OUTPUT_STRIDES [TEACHER.MODEL.DETECTION.MASK_RCNN.OUTPUT_STRIDES ...]
                        Extract backbone feature maps from these output
                        strides. If output stride is greater than 32, extra
                        layers are added.
  --teacher.model.detection.mask-rcnn.anchor-sizes TEACHER.MODEL.DETECTION.MASK_RCNN.ANCHOR_SIZES [TEACHER.MODEL.DETECTION.MASK_RCNN.ANCHOR_SIZES ...]
                        Anchor sizes at each output stride
  --teacher.model.detection.mask-rcnn.aspect-ratio TEACHER.MODEL.DETECTION.MASK_RCNN.ASPECT_RATIO [TEACHER.MODEL.DETECTION.MASK_RCNN.ASPECT_RATIO ...]
                        Aspect ratios. These are the same for all feature maps
  --teacher.model.detection.mask-rcnn.bbox-head-fm-size TEACHER.MODEL.DETECTION.MASK_RCNN.BBOX_HEAD_FM_SIZE
                        Feature map size for the box head
  --teacher.model.detection.mask-rcnn.mask-head-fm-size TEACHER.MODEL.DETECTION.MASK_RCNN.MASK_HEAD_FM_SIZE
                        Feature map size for the max head
  --teacher.model.detection.mask-rcnn.representation-size TEACHER.MODEL.DETECTION.MASK_RCNN.REPRESENTATION_SIZE
                        Size of the intermediate representation in Mask RCNN
  --teacher.model.detection.mask-rcnn.box-fm-size-conv-layer TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_FM_SIZE_CONV_LAYER [TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_FM_SIZE_CONV_LAYER ...]
                        Feature dim of each Convolution layer in the Faster
                        RCNN head. Defaults to [256, 256, 256, 256]
  --teacher.model.detection.mask-rcnn.mask-fm-size-conv-layer TEACHER.MODEL.DETECTION.MASK_RCNN.MASK_FM_SIZE_CONV_LAYER [TEACHER.MODEL.DETECTION.MASK_RCNN.MASK_FM_SIZE_CONV_LAYER ...]
                        Feature dim of each Convolution layer in the Mask RCNN
                        head. Defaults to [256, 256, 256, 256]
  --teacher.model.detection.mask-rcnn.mask-dilation TEACHER.MODEL.DETECTION.MASK_RCNN.MASK_DILATION
                        Dilation rate in Mask RCNN head. Defaults to 1
  --teacher.model.detection.mask-rcnn.rpn-pre-nms-top-n-train TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_PRE_NMS_TOP_N_TRAIN
                        Number of proposals to keep before applying NMS during
                        training
  --teacher.model.detection.mask-rcnn.rpn-pre-nms-top-n-test TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_PRE_NMS_TOP_N_TEST
                        Number of proposals to keep before applying NMS during
                        test
  --teacher.model.detection.mask-rcnn.rpn-post-nms-top-n-train TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_POST_NMS_TOP_N_TRAIN
                        Number of proposals to keep after applying NMS during
                        training
  --teacher.model.detection.mask-rcnn.rpn-post-nms-top-n-test TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_POST_NMS_TOP_N_TEST
                        Number of proposals to keep after applying NMS during
                        test
  --teacher.model.detection.mask-rcnn.rpn-nms-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_NMS_THRESH
                        NMS threshold used for postprocessing the RPN
                        proposals
  --teacher.model.detection.mask-rcnn.rpn-fg-iou-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_FG_IOU_THRESH
                        minimum IoU between the anchor and the GT box so that
                        they can be considered as positive during training of
                        the RPN.
  --teacher.model.detection.mask-rcnn.rpn-bg-iou-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_BG_IOU_THRESH
                        minimum IoU between the anchor and the GT box so that
                        they can be considered as negative during training of
                        the RPN.
  --teacher.model.detection.mask-rcnn.rpn-batch-size-per-image TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_BATCH_SIZE_PER_IMAGE
                        Number of anchors that are sampled during training of
                        the RPN for computing the loss
  --teacher.model.detection.mask-rcnn.rpn-positive-fraction TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_POSITIVE_FRACTION
                        Proportion of positive anchors in a mini-batch during
                        training of the RPN
  --teacher.model.detection.mask-rcnn.rpn-score-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.RPN_SCORE_THRESH
                        During inference, only return proposals with a
                        classification score greater than rpn_score_thresh
  --teacher.model.detection.mask-rcnn.box-score-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_SCORE_THRESH
                        During inference, only return proposals with a
                        classification score greater than box_score_thresh
  --teacher.model.detection.mask-rcnn.box-nms-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_NMS_THRESH
                        During inference, NMS threshold for the prediction
                        head.
  --teacher.model.detection.mask-rcnn.box-detections-per-img TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_DETECTIONS_PER_IMG
                        Maximum number of detections per image, for all
                        classes
  --teacher.model.detection.mask-rcnn.box-fg-iou-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_FG_IOU_THRESH
                        Minimum IoU between the proposals and the GT box so
                        that they can be considered as positive during
                        training of the classification head
  --teacher.model.detection.mask-rcnn.box-bg-iou-thresh TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_BG_IOU_THRESH
                        Minimum IoU between the proposals and the GT box so
                        that they can be considered as negative during
                        training of the classification head
  --teacher.model.detection.mask-rcnn.box-batch-size-per-image TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_BATCH_SIZE_PER_IMAGE
                        Number of proposals that are sampled during training
                        of the classification head
  --teacher.model.detection.mask-rcnn.box-positive-fraction TEACHER.MODEL.DETECTION.MASK_RCNN.BOX_POSITIVE_FRACTION
                        Proportion of positive proposals in a mini-batch
                        during training of the classification head
  --teacher.model.detection.mask-rcnn.norm-layer TEACHER.MODEL.DETECTION.MASK_RCNN.NORM_LAYER
                        Mask RCNN Norm layer
  --teacher.model.detection.mask-rcnn.disable-fpn [TEACHER.MODEL.DETECTION.MASK_RCNN.DISABLE_FPN]
                        Do not use FPN
  --teacher.model.detection.ssd.anchors-aspect-ratio TEACHER.MODEL.DETECTION.SSD.ANCHORS_ASPECT_RATIO [TEACHER.MODEL.DETECTION.SSD.ANCHORS_ASPECT_RATIO ...]
                        Anchors aspect ratio in each feature map obtained at
                        different output strides.
  --teacher.model.detection.ssd.output-strides TEACHER.MODEL.DETECTION.SSD.OUTPUT_STRIDES [TEACHER.MODEL.DETECTION.SSD.OUTPUT_STRIDES ...]
                        Extract feature maps from these output strides.
  --teacher.model.detection.ssd.proj-channels TEACHER.MODEL.DETECTION.SSD.PROJ_CHANNELS [TEACHER.MODEL.DETECTION.SSD.PROJ_CHANNELS ...]
                        Projection channels for feature map obtained at each
                        output stride
  --teacher.model.detection.ssd.min-box-size TEACHER.MODEL.DETECTION.SSD.MIN_BOX_SIZE
                        Min. box size. Value between 0 and 1. Good default
                        value is 0.1
  --teacher.model.detection.ssd.max-box-size TEACHER.MODEL.DETECTION.SSD.MAX_BOX_SIZE
                        Max. box size. Value between 0 and 1. Good default
                        value is 1.05
  --teacher.model.detection.ssd.center-variance TEACHER.MODEL.DETECTION.SSD.CENTER_VARIANCE
                        Center variance.
  --teacher.model.detection.ssd.size-variance TEACHER.MODEL.DETECTION.SSD.SIZE_VARIANCE
                        Size variance.
  --teacher.model.detection.ssd.iou-threshold TEACHER.MODEL.DETECTION.SSD.IOU_THRESHOLD
                        IOU Threshold.
  --teacher.model.detection.ssd.conf-threshold TEACHER.MODEL.DETECTION.SSD.CONF_THRESHOLD
                        Confidence threshold. For evaluation on COCO, set to
                        0.01, so that we can compute mAP
  --teacher.model.detection.ssd.top-k TEACHER.MODEL.DETECTION.SSD.TOP_K
                        Keep only top-k objects before NMS
  --teacher.model.detection.ssd.objects-per-image TEACHER.MODEL.DETECTION.SSD.OBJECTS_PER_IMAGE
                        Keep only these many objects after NMS
  --teacher.model.detection.ssd.nms-iou-threshold TEACHER.MODEL.DETECTION.SSD.NMS_IOU_THRESHOLD
                        NMS IoU threshold
  --teacher.model.detection.ssd.fpn-out-channels TEACHER.MODEL.DETECTION.SSD.FPN_OUT_CHANNELS
                        Number of output channels in FPN
  --teacher.model.detection.ssd.use-fpn [TEACHER.MODEL.DETECTION.SSD.USE_FPN]
                        Use SSD with FPN
  --teacher.model.language-modeling.name TEACHER.MODEL.LANGUAGE_MODELING.NAME
                        Name of the language model. Defaults to None (i.e.,
                        user need to specify the model name).
  --teacher.model.language-modeling.pretrained TEACHER.MODEL.LANGUAGE_MODELING.PRETRAINED
                        Path of the pre-trained model. Defaults to None (i.e.,
                        user needs to specify the path of pre-trained model).
  --teacher.model.language-modeling.general-gpt.model-name {gpt-test,gpt-1_3B,OpenELM-270M,OpenELM-450M,OpenELM-1_1B,OpenELM-3B}
                        Name of the generative transformer-based LM model.
                        Defaults to None (i.e., user need to specify the model
                        name.).
  --teacher.model.language-modeling.general-gpt.max-context-length TEACHER.MODEL.LANGUAGE_MODELING.GENERAL_GPT.MAX_CONTEXT_LENGTH
                        Maximum context length. Defaults to None (i.e., user
                        needs to specify the maximum contenxt length value.).
  --teacher.model.language-modeling.general-gpt.vocab-size TEACHER.MODEL.LANGUAGE_MODELING.GENERAL_GPT.VOCAB_SIZE
                        Vocabulary size. Defaults to None (i.e., user needs to
                        specify the vocabulary size.).
  --teacher.model.language-modeling.general-gpt.padding-index TEACHER.MODEL.LANGUAGE_MODELING.GENERAL_GPT.PADDING_INDEX
                        Padding index. Defaults to None (i.e., no padding).
  --teacher.model.language-modeling.kv-prediction.auxkv-num-layers-to-basekv-num-layers TEACHER.MODEL.LANGUAGE_MODELING.KV_PREDICTION.AUXKV_NUM_LAYERS_TO_BASEKV_NUM_LAYERS
                        The mapping from auxiliary layers to base model
                        layers. The element at index i is used to tell which
                        Auxiliary layer is used to predict the KV cache at
                        Base layer i.
  --teacher.model.language-modeling.kv-prediction.base-model TEACHER.MODEL.LANGUAGE_MODELING.KV_PREDICTION.BASE_MODEL
                        A config for the base model.
  --teacher.model.language-modeling.kv-prediction.auxiliary-model TEACHER.MODEL.LANGUAGE_MODELING.KV_PREDICTION.AUXILIARY_MODEL
                        A config for the auxiliary model.
  --teacher.model.multi-modal-image-text.name TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.NAME
                        Name of the multi-modal image-text model
  --teacher.model.multi-modal-image-text.lr-multiplier-img-encoder TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.LR_MULTIPLIER_IMG_ENCODER
                        LR multiplier for the image encoder in
                        BaseMultiModalImageText
  --teacher.model.multi-modal-image-text.lr-multiplier-text-encoder TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.LR_MULTIPLIER_TEXT_ENCODER
                        LR multiplier for the text encoder in
                        BaseMultiModalImageText
  --teacher.model.multi-modal-image-text.pretrained TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.PRETRAINED
                        Path of the pretrained backbone
  --teacher.model.multi-modal-image-text.freeze-batch-norm [TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.FREEZE_BATCH_NORM]
                        Freeze batch norm layers
  --teacher.model.multi-modal-image-text.clip.projection-dim TEACHER.MODEL.MULTI_MODAL_IMAGE_TEXT.CLIP.PROJECTION_DIM
                        Project image and text features to this dimensionality
  --teacher.model.segmentation.name TEACHER.MODEL.SEGMENTATION.NAME
                        Segmentation model name. Defaults to None.
  --teacher.model.segmentation.n-classes TEACHER.MODEL.SEGMENTATION.N_CLASSES
                        Number of classes in the dataset. Defaults to 21.
  --teacher.model.segmentation.pretrained TEACHER.MODEL.SEGMENTATION.PRETRAINED
                        Path of the pretrained segmentation model. Useful for
                        evaluation
  --teacher.model.segmentation.lr-multiplier TEACHER.MODEL.SEGMENTATION.LR_MULTIPLIER
                        Multiply the learning rate in segmentation network
                        (e.g., decoder) by this factor.Defaults to 1.0.
  --teacher.model.segmentation.classifier-dropout TEACHER.MODEL.SEGMENTATION.CLASSIFIER_DROPOUT
                        Dropout rate in classifier
  --teacher.model.segmentation.use-aux-head [TEACHER.MODEL.SEGMENTATION.USE_AUX_HEAD]
                        Use auxiliary output
  --teacher.model.segmentation.aux-dropout TEACHER.MODEL.SEGMENTATION.AUX_DROPOUT
                        Dropout in auxiliary branch
  --teacher.model.segmentation.output-stride TEACHER.MODEL.SEGMENTATION.OUTPUT_STRIDE
                        Output stride in classification network
  --teacher.model.segmentation.replace-stride-with-dilation [TEACHER.MODEL.SEGMENTATION.REPLACE_STRIDE_WITH_DILATION]
                        Replace stride with dilation
  --teacher.model.segmentation.activation.name TEACHER.MODEL.SEGMENTATION.ACTIVATION.NAME
                        Non-linear function type
  --teacher.model.segmentation.activation.inplace [TEACHER.MODEL.SEGMENTATION.ACTIVATION.INPLACE]
                        Inplace non-linear functions
  --teacher.model.segmentation.activation.neg-slope TEACHER.MODEL.SEGMENTATION.ACTIVATION.NEG_SLOPE
                        Negative slope in leaky relu
  --teacher.model.segmentation.freeze-batch-norm [TEACHER.MODEL.SEGMENTATION.FREEZE_BATCH_NORM]
                        Freeze batch norm layers
  --teacher.model.segmentation.use-level5-exp [TEACHER.MODEL.SEGMENTATION.USE_LEVEL5_EXP]
                        Use output of Level 5 expansion layer in base feature
                        extractor
  --teacher.model.segmentation.finetune-pretrained-model [TEACHER.MODEL.SEGMENTATION.FINETUNE_PRETRAINED_MODEL]
                        Finetune a pretrained segmentation model. Defaults to
                        False.
  --teacher.model.segmentation.n-pretrained-classes TEACHER.MODEL.SEGMENTATION.N_PRETRAINED_CLASSES
                        Number of classes in the pre-trained segmentation
                        model. Defaults to None.
  --teacher.model.segmentation.norm-layer TEACHER.MODEL.SEGMENTATION.NORM_LAYER
                        Normalization layer for segmentation. Defaults to
                        batch_norm.
  --teacher.model.segmentation.seg-head TEACHER.MODEL.SEGMENTATION.SEG_HEAD
                        Segmentation head
  --teacher.model.segmentation.deeplabv3.aspp-rates TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_RATES
                        Atrous rates to be used in the ASPP module in
                        DeeplabV3 segmentation head. Defaults to (6, 12, 18).
  --teacher.model.segmentation.deeplabv3.aspp-out-channels TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_OUT_CHANNELS
                        Output channels of ASPP module in DeeplabV3
                        segmentation head. Defaults to 256.
  --teacher.model.segmentation.deeplabv3.aspp-sep-conv [TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_SEP_CONV]
                        Use separable convolution in the ASPP module in
                        DeeplabV3 segmentation head. Defaults to False.
  --teacher.model.segmentation.deeplabv3.aspp-dropout TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_DROPOUT
                        Dropout value in the ASPP module in DeeplabV3
                        segmentation head. Defaults to 0.1.
  --teacher.model.segmentation.deeplabv3.aspp-in-channels TEACHER.MODEL.SEGMENTATION.DEEPLABV3.ASPP_IN_CHANNELS
                        Input channels of the ASPP module. This is only used
                        in MultiScaleDeeplabV3. Defaults to 512.
  --teacher.model.segmentation.deeplabv3.output-upsample-factor TEACHER.MODEL.SEGMENTATION.DEEPLABV3.OUTPUT_UPSAMPLE_FACTOR
                        Output stide of the image encoder. This argument is
                        used on MultiScaleDeeplabV3. Default value is None.
  --teacher.model.segmentation.pspnet.psp-pool-sizes TEACHER.MODEL.SEGMENTATION.PSPNET.PSP_POOL_SIZES [TEACHER.MODEL.SEGMENTATION.PSPNET.PSP_POOL_SIZES ...]
                        Pool sizes in the PSPNet module
  --teacher.model.segmentation.pspnet.psp-out-channels TEACHER.MODEL.SEGMENTATION.PSPNET.PSP_OUT_CHANNELS
                        Output channels of PSPNet module
  --teacher.model.segmentation.pspnet.psp-dropout TEACHER.MODEL.SEGMENTATION.PSPNET.PSP_DROPOUT
                        Dropout in the PSPNet module
  --teacher.model.video-classification.classifier-dropout TEACHER.MODEL.VIDEO_CLASSIFICATION.CLASSIFIER_DROPOUT
                        Dropout rate in classifier
  --teacher.model.video-classification.name TEACHER.MODEL.VIDEO_CLASSIFICATION.NAME
                        Model name
  --teacher.model.video-classification.n-classes TEACHER.MODEL.VIDEO_CLASSIFICATION.N_CLASSES
                        Number of classes in the dataset
  --teacher.model.video-classification.pretrained TEACHER.MODEL.VIDEO_CLASSIFICATION.PRETRAINED
                        Path of the pretrained backbone
  --teacher.model.video-classification.freeze-batch-norm [TEACHER.MODEL.VIDEO_CLASSIFICATION.FREEZE_BATCH_NORM]
                        Freeze batch norm layers
  --teacher.model.video-classification.activation.name TEACHER.MODEL.VIDEO_CLASSIFICATION.ACTIVATION.NAME
                        Non-linear function type
  --teacher.model.video-classification.activation.inplace [TEACHER.MODEL.VIDEO_CLASSIFICATION.ACTIVATION.INPLACE]
                        Inplace non-linear functions
  --teacher.model.video-classification.activation.neg-slope TEACHER.MODEL.VIDEO_CLASSIFICATION.ACTIVATION.NEG_SLOPE
                        Negative slope in leaky relu
  --teacher.model.video-classification.clip-out-voting-fn {sum,max}
                        How to fuse the outputs of different clips in a video
  --teacher.model.video-classification.inference-mode [TEACHER.MODEL.VIDEO_CLASSIFICATION.INFERENCE_MODE]
                        Inference mode
  --teacher.model.layer.linear-init TEACHER.MODEL.LAYER.LINEAR_INIT
                        Init type for linear layers
  --teacher.model.layer.linear-init-std-dev TEACHER.MODEL.LAYER.LINEAR_INIT_STD_DEV
                        Std deviation for Linear layers
  --teacher.model.layer.group-linear-init TEACHER.MODEL.LAYER.GROUP_LINEAR_INIT
                        Init type for group linear layers
  --teacher.model.layer.group-linear-init-std-dev TEACHER.MODEL.LAYER.GROUP_LINEAR_INIT_STD_DEV
                        Std deviation for group linear layers
  --teacher.model.layer.global-pool TEACHER.MODEL.LAYER.GLOBAL_POOL
                        Which global pooling?
  --teacher.model.layer.conv-init TEACHER.MODEL.LAYER.CONV_INIT
                        Init type for conv layers
  --teacher.model.layer.conv-init-std-dev TEACHER.MODEL.LAYER.CONV_INIT_STD_DEV
                        Std deviation for conv layers
  --teacher.model.activation.name TEACHER.MODEL.ACTIVATION.NAME
                        Non-linear function name
  --teacher.model.activation.inplace [TEACHER.MODEL.ACTIVATION.INPLACE]
                        Use non-linear functions inplace
  --teacher.model.activation.neg-slope TEACHER.MODEL.ACTIVATION.NEG_SLOPE
                        Negative slope in leaky relu function
  --teacher.model.normalization.name TEACHER.MODEL.NORMALIZATION.NAME
                        Normalization layer. Defaults to 'batch_norm'.
  --teacher.model.normalization.groups TEACHER.MODEL.NORMALIZATION.GROUPS
                        Number of groups in group normalization layer.
                        Defaults to 1.
  --teacher.model.normalization.momentum TEACHER.MODEL.NORMALIZATION.MOMENTUM
                        Momentum in normalization layers. Defaults to 0.1
  --teacher.model.learn-augmentation.mode {basic,distribution}
                        Neural augmentation mode
  --teacher.model.learn-augmentation.brightness [TEACHER.MODEL.LEARN_AUGMENTATION.BRIGHTNESS]
                        Learn parameters for brightness
  --teacher.model.learn-augmentation.contrast [TEACHER.MODEL.LEARN_AUGMENTATION.CONTRAST]
                        Learn parameters for contrast
  --teacher.model.learn-augmentation.noise [TEACHER.MODEL.LEARN_AUGMENTATION.NOISE]
                        Learn parameters for noise
  --teacher.model.learn-augmentation.lr-multiplier TEACHER.MODEL.LEARN_AUGMENTATION.LR_MULTIPLIER
                        LR multiplier for neural aug parameters
  --image-augmentation.to-tensor.dtype IMAGE_AUGMENTATION.TO_TENSOR.DTYPE
                        Tensor data type. Default is float
  --image-augmentation.to-tensor.mean-std-normalization.enable
                        This flag is used to normalize a tensor by a dataset's
                        mean and std. Defaults to False.
  --image-augmentation.to-tensor.mean-std-normalization.mean IMAGE_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.MEAN [IMAGE_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.MEAN ...]
                        The mean used to normalize the input. Defaults to
                        None.
  --image-augmentation.to-tensor.mean-std-normalization.std IMAGE_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.STD [IMAGE_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.STD ...]
                        The standard deviation used to normalize the input.
                        Defaults to None.
  --frame-augmentation.fixed-size-crop.enable [FRAME_AUGMENTATION.FIXED_SIZE_CROP.ENABLE]
                        use FixedSizeCrop. This flag is useful when you want
                        to study the effect of different transforms.
  --frame-augmentation.fixed-size-crop.size FRAME_AUGMENTATION.FIXED_SIZE_CROP.SIZE [FRAME_AUGMENTATION.FIXED_SIZE_CROP.SIZE ...]
                        Image size either as an int or (int, int).
  --frame-augmentation.fixed-size-crop.fill FRAME_AUGMENTATION.FIXED_SIZE_CROP.FILL
                        Fill value to be used during padding operation.
                        Defaults to 0.
  --frame-augmentation.fixed-size-crop.padding-mode FRAME_AUGMENTATION.FIXED_SIZE_CROP.PADDING_MODE
                        Padding modes. Defaults to constant
  --frame-augmentation.scale-jitter.enable [FRAME_AUGMENTATION.SCALE_JITTER.ENABLE]
                        use ScaleJitter. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.scale-jitter.interpolation FRAME_AUGMENTATION.SCALE_JITTER.INTERPOLATION
                        Interpolation method. Defaults to bilinear
                        interpolation
  --frame-augmentation.scale-jitter.target-size FRAME_AUGMENTATION.SCALE_JITTER.TARGET_SIZE [FRAME_AUGMENTATION.SCALE_JITTER.TARGET_SIZE ...]
                        Target image size either as an int or (int, int).
  --frame-augmentation.scale-jitter.scale-range FRAME_AUGMENTATION.SCALE_JITTER.SCALE_RANGE [FRAME_AUGMENTATION.SCALE_JITTER.SCALE_RANGE ...]
                        Scale range as (float, float).
  --frame-augmentation.random-resized-crop.enable [FRAME_AUGMENTATION.RANDOM_RESIZED_CROP.ENABLE]
                        use RandomResizedCrop. This flag is useful when you
                        want to study the effect of different transforms.
  --frame-augmentation.random-resized-crop.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Interpolation method for resizing. Defaults to
                        bilinear.
  --frame-augmentation.random-resized-crop.scale FRAME_AUGMENTATION.RANDOM_RESIZED_CROP.SCALE
                        Specifies the lower and upper bounds for the random
                        area of the crop, before resizing. The scale is
                        defined with respect to the area of the original
                        image. Defaults to (0.08, 1.0)
  --frame-augmentation.random-resized-crop.aspect-ratio FRAME_AUGMENTATION.RANDOM_RESIZED_CROP.ASPECT_RATIO
                        lower and upper bounds for the random aspect ratio of
                        the crop, before resizing. Defaults to (3./4., 4./3.)
  --frame-augmentation.auto-augment.enable [FRAME_AUGMENTATION.AUTO_AUGMENT.ENABLE]
                        use AutoAugment. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.auto-augment.policy FRAME_AUGMENTATION.AUTO_AUGMENT.POLICY
                        Auto-augment policy name. Defaults to imagenet.
  --frame-augmentation.auto-augment.interpolation FRAME_AUGMENTATION.AUTO_AUGMENT.INTERPOLATION
                        Auto-augment interpolation method. Defaults to
                        bilinear interpolation
  --frame-augmentation.rand-augment.enable [FRAME_AUGMENTATION.RAND_AUGMENT.ENABLE]
                        Use RandAugment. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.rand-augment.num-ops FRAME_AUGMENTATION.RAND_AUGMENT.NUM_OPS
                        Number of augmentation transformations to apply
                        sequentially. Defaults to 2.
  --frame-augmentation.rand-augment.magnitude FRAME_AUGMENTATION.RAND_AUGMENT.MAGNITUDE
                        Magnitude for all the transformations. Defaults to 9
  --frame-augmentation.rand-augment.num-magnitude-bins FRAME_AUGMENTATION.RAND_AUGMENT.NUM_MAGNITUDE_BINS
                        The number of different magnitude values. Defaults to
                        31.
  --frame-augmentation.rand-augment.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method. Defaults to bilinear
  --frame-augmentation.trivial-augment-wide.enable [FRAME_AUGMENTATION.TRIVIAL_AUGMENT_WIDE.ENABLE]
                        Use TrivialAugmentWide. This flag is useful when you
                        want to study the effect of different transforms.
  --frame-augmentation.trivial-augment-wide.num-magnitude-bins FRAME_AUGMENTATION.TRIVIAL_AUGMENT_WIDE.NUM_MAGNITUDE_BINS
                        The number of different magnitude values. Defaults to
                        31.
  --frame-augmentation.trivial-augment-wide.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method. Defaults to bilinear
  --frame-augmentation.random-horizontal-flip.enable [FRAME_AUGMENTATION.RANDOM_HORIZONTAL_FLIP.ENABLE]
                        use RandomHorizontalFlip. This flag is useful when you
                        want to study the effect of different transforms.
  --frame-augmentation.random-horizontal-flip.p FRAME_AUGMENTATION.RANDOM_HORIZONTAL_FLIP.P
                        Probability for applying random horizontal flip
  --frame-augmentation.random-rotate.enable [FRAME_AUGMENTATION.RANDOM_ROTATE.ENABLE]
                        use RandomRotate. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.random-rotate.angle FRAME_AUGMENTATION.RANDOM_ROTATE.ANGLE
                        Angle for rotation. Defaults to 10. The angle is
                        sampled uniformly from [-angle, angle]
  --frame-augmentation.random-rotate.mask-fill FRAME_AUGMENTATION.RANDOM_ROTATE.MASK_FILL
                        Fill value for the segmentation mask. Defaults to 0.
  --frame-augmentation.resize.enable [FRAME_AUGMENTATION.RESIZE.ENABLE]
                        use Resize. This flag is useful when you want to study
                        the effect of different transforms.
  --frame-augmentation.resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method for resizing. Defaults to
                        bilinear
  --frame-augmentation.resize.size FRAME_AUGMENTATION.RESIZE.SIZE [FRAME_AUGMENTATION.RESIZE.SIZE ...]
                        Resize image to the specified size. If int is passed,
                        then shorter side is resizedto the specified size and
                        longest side is resized while maintaining aspect
                        ratio.Defaults to None.
  --frame-augmentation.center-crop.enable [FRAME_AUGMENTATION.CENTER_CROP.ENABLE]
                        use CenterCrop. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.center-crop.size FRAME_AUGMENTATION.CENTER_CROP.SIZE [FRAME_AUGMENTATION.CENTER_CROP.SIZE ...]
                        Center crop size. Defaults to None.
  --frame-augmentation.ssd-crop.enable [FRAME_AUGMENTATION.SSD_CROP.ENABLE]
                        use SSDCroping. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.ssd-crop.iou-thresholds FRAME_AUGMENTATION.SSD_CROP.IOU_THRESHOLDS [FRAME_AUGMENTATION.SSD_CROP.IOU_THRESHOLDS ...]
                        IoU thresholds for SSD cropping. Defaults to [0.0,
                        0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
  --frame-augmentation.ssd-crop.n-trials FRAME_AUGMENTATION.SSD_CROP.N_TRIALS
                        Number of trials for SSD cropping. Defaults to 40
  --frame-augmentation.ssd-crop.min-aspect-ratio FRAME_AUGMENTATION.SSD_CROP.MIN_ASPECT_RATIO
                        Min. aspect ratio in SSD Cropping. Defaults to 0.5
  --frame-augmentation.ssd-crop.max-aspect-ratio FRAME_AUGMENTATION.SSD_CROP.MAX_ASPECT_RATIO
                        Max. aspect ratio in SSD Cropping. Defaults to 2.0
  --frame-augmentation.photo-metric-distort.enable [FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.ENABLE]
                        use PhotometricDistort. This flag is useful when you
                        want to study the effect of different transforms.
  --frame-augmentation.photo-metric-distort.alpha-min FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.ALPHA_MIN
                        Min. alpha value for contrast. Should be > 0. Defaults
                        to 0.5
  --frame-augmentation.photo-metric-distort.alpha-max FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.ALPHA_MAX
                        Max. alpha value for contrast. Should be > 0. Defaults
                        to 1.5
  --frame-augmentation.photo-metric-distort.beta-min FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.BETA_MIN
                        Min. beta value for brightness. Should be > 0.
                        Defaults to 0.8
  --frame-augmentation.photo-metric-distort.beta-max FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.BETA_MAX
                        Max. beta value for brightness. Should be > 0.
                        Defaults to 1.2
  --frame-augmentation.photo-metric-distort.gamma-min FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.GAMMA_MIN
                        Min. gamma value for saturation. Should be > 0.
                        Defaults to 0.5
  --frame-augmentation.photo-metric-distort.gamma-max FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.GAMMA_MAX
                        Max. gamma value for saturation. Should be > 0.
                        Defaults to 1.5
  --frame-augmentation.photo-metric-distort.delta-min FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.DELTA_MIN
                        Min. delta value for Hue. Should be between -1 and 1.
                        Defaults to -0.05
  --frame-augmentation.photo-metric-distort.delta-max FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.DELTA_MAX
                        Max. delta value for Hue. Should be between -1 and 1.
                        Defaults to 0.05
  --frame-augmentation.photo-metric-distort.p FRAME_AUGMENTATION.PHOTO_METRIC_DISTORT.P
                        Probability for applying a distortion. Defaults to 0.5
  --frame-augmentation.random-resize.enable [FRAME_AUGMENTATION.RANDOM_RESIZE.ENABLE]
                        use RandomResize. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.random-resize.max-scale-long-edge FRAME_AUGMENTATION.RANDOM_RESIZE.MAX_SCALE_LONG_EDGE
                        Max. value along the longest edge. Defaults to None
  --frame-augmentation.random-resize.max-scale-short-edge FRAME_AUGMENTATION.RANDOM_RESIZE.MAX_SCALE_SHORT_EDGE
                        Max. value along the shortest edge. Defaults to None.
  --frame-augmentation.random-resize.min-ratio FRAME_AUGMENTATION.RANDOM_RESIZE.MIN_RATIO
                        Min ratio for random resizing. Defaults to 0.5
  --frame-augmentation.random-resize.max-ratio FRAME_AUGMENTATION.RANDOM_RESIZE.MAX_RATIO
                        Max ratio for random resizing. Defaults to 2.0
  --frame-augmentation.random-resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method. Defaults to bilinear.
  --frame-augmentation.random-short-size-resize.enable [FRAME_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.ENABLE]
                        use RandomShortSizeResize. This flag is useful when
                        you want to study the effect of different transforms.
  --frame-augmentation.random-short-size-resize.short-side-min FRAME_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.SHORT_SIDE_MIN
                        Minimum value for image's shortest side. Defaults to
                        None.
  --frame-augmentation.random-short-size-resize.short-side-max FRAME_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.SHORT_SIDE_MAX
                        Maximum value for image's shortest side. Defaults to
                        None.
  --frame-augmentation.random-short-size-resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method. Defaults to bicubic
  --frame-augmentation.random-short-size-resize.max-img-dim FRAME_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.MAX_IMG_DIM
                        Max. image dimension. Defaults to None.
  --frame-augmentation.random-erase.enable [FRAME_AUGMENTATION.RANDOM_ERASE.ENABLE]
                        use RandomErasing. This flag is useful when you want
                        to study the effect of different transforms.
  --frame-augmentation.random-erase.p FRAME_AUGMENTATION.RANDOM_ERASE.P
                        Probability that random erasing operation will be
                        applied. Defaults to 0.5
  --frame-augmentation.random-gaussian-noise.enable [FRAME_AUGMENTATION.RANDOM_GAUSSIAN_NOISE.ENABLE]
                        use RandomGaussianBlur. This flag is useful when you
                        want to study the effect of different transforms.
  --frame-augmentation.random-gaussian-noise.p FRAME_AUGMENTATION.RANDOM_GAUSSIAN_NOISE.P
                        Probability for applying RandomGaussianBlur
  --frame-augmentation.random-crop.enable [FRAME_AUGMENTATION.RANDOM_CROP.ENABLE]
                        use RandomCrop. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.random-crop.seg-class-max-ratio FRAME_AUGMENTATION.RANDOM_CROP.SEG_CLASS_MAX_RATIO
                        Max. ratio that single segmentation class can occupy.
                        Defaults to None
  --frame-augmentation.random-crop.pad-if-needed [FRAME_AUGMENTATION.RANDOM_CROP.PAD_IF_NEEDED]
                        Pad images if needed. Defaults to False, i.e.,
                        resizing will be performed
  --frame-augmentation.random-crop.mask-fill FRAME_AUGMENTATION.RANDOM_CROP.MASK_FILL
                        Value to fill in segmentation mask in case of padding.
                        Defaults to 255. Generally, this value is the same as
                        background or undefined class id.
  --frame-augmentation.to-tensor.dtype FRAME_AUGMENTATION.TO_TENSOR.DTYPE
                        Tensor data type. Default is float
  --frame-augmentation.to-tensor.mean-std-normalization.enable [FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.ENABLE]
                        This flag is used to normalize a tensor by a dataset's
                        mean and std. Defaults to False.
  --frame-augmentation.to-tensor.mean-std-normalization.mean FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.MEAN [FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.MEAN ...]
                        The mean used to normalize the input. Defaults to
                        None.
  --frame-augmentation.to-tensor.mean-std-normalization.std FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.STD [FRAME_AUGMENTATION.TO_TENSOR.MEAN_STD_NORMALIZATION.STD ...]
                        The standard deviation used to normalize the input.
                        Defaults to None.
  --frame-augmentation.random-order.enable [FRAME_AUGMENTATION.RANDOM_ORDER.ENABLE]
                        use RandomOrder. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.random-order.apply-k FRAME_AUGMENTATION.RANDOM_ORDER.APPLY_K
                        Apply K percent of transforms randomly. Value between
                        0 and 1. Defaults to 1 (i.e., apply all transforms in
                        random order).
  --frame-augmentation.rand-augment.use-timm-library [FRAME_AUGMENTATION.RAND_AUGMENT.USE_TIMM_LIBRARY]
                        Use timm library for randaugment over PyTorch's
                        implementation
  --frame-augmentation.rand-augment.timm-config-str FRAME_AUGMENTATION.RAND_AUGMENT.TIMM_CONFIG_STR
                        Number of augmentation transformations to apply
                        sequentially. Defaults to 2.
  --frame-augmentation.mixup.enable [FRAME_AUGMENTATION.MIXUP.ENABLE]
                        use RandomMixup. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.mixup.alpha FRAME_AUGMENTATION.MIXUP.ALPHA
                        Alpha for MixUp augmentation. Defaults to 0.2
  --frame-augmentation.mixup.p FRAME_AUGMENTATION.MIXUP.P
                        Probability for applying MixUp augmentation. Defaults
                        to 1.0 . If both MixUp and CutMix are enabled, one is
                        used with probability 0.5 per batch.
  --frame-augmentation.mixup.inplace [FRAME_AUGMENTATION.MIXUP.INPLACE]
                        Apply MixUp augmentation inplace. Defaults to False.
  --frame-augmentation.mixup.sample-key FRAME_AUGMENTATION.MIXUP.SAMPLE_KEY
                        Name of the key if input is a dictionart. Defaults to
                        None.
  --frame-augmentation.mixup.target-key FRAME_AUGMENTATION.MIXUP.TARGET_KEY
                        Name of the key if target is a dictionary. Defaults to
                        None.
  --frame-augmentation.cutmix.enable [FRAME_AUGMENTATION.CUTMIX.ENABLE]
                        use RandomCutmix. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.cutmix.alpha FRAME_AUGMENTATION.CUTMIX.ALPHA
                        Alpha for cutmix augmentation. Defaults to 1.0
  --frame-augmentation.cutmix.p FRAME_AUGMENTATION.CUTMIX.P
                        Probability for applying cutmix augmentation. Defaults
                        to 1.0 If both MixUp and CutMix are enabled, one is
                        used with probability 0.5 per batch.
  --frame-augmentation.cutmix.inplace [FRAME_AUGMENTATION.CUTMIX.INPLACE]
                        Apply cutmix operation inplace. Defaults to False
  --frame-augmentation.cutmix.sample-key FRAME_AUGMENTATION.CUTMIX.SAMPLE_KEY
                        Name of the key if input is a dictionary. Defaults to
                        None.
  --frame-augmentation.cutmix.target-key FRAME_AUGMENTATION.CUTMIX.TARGET_KEY
                        Name of the key if target is a dictionary. Defaults to
                        None.
  --frame-augmentation.pil-save.enable [FRAME_AUGMENTATION.PIL_SAVE.ENABLE]
                        Use PILSave. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.pil-save.file-encoding {fCHW,fHWC,TIFF,PNG,JPEG}
                        The type of file encoding to use. Defaults to TIFF.
  --frame-augmentation.pil-save.quality FRAME_AUGMENTATION.PIL_SAVE.QUALITY
                        JPEG quality if using JPEG encoding. Defaults to 100.
  --frame-augmentation.shuffle-bytes.enable [FRAME_AUGMENTATION.SHUFFLE_BYTES.ENABLE]
                        Use ShuffleBytes. This flag is useful when you want to
                        study the effect of different transforms.
  --frame-augmentation.shuffle-bytes.mode {reverse,random_shuffle,cyclic_half_length,stride,window_shuffle}
                        The mode to use when shuffling bytes. Defaults to
                        'reverse'.
  --frame-augmentation.shuffle-bytes.stride FRAME_AUGMENTATION.SHUFFLE_BYTES.STRIDE
                        The stride of the window used in shuffling operations
                        that are windowed. Defaults to 1024.
  --frame-augmentation.shuffle-bytes.window-size FRAME_AUGMENTATION.SHUFFLE_BYTES.WINDOW_SIZE
                        The size of the window used in shuffling operations
                        that are windowed. Defaults to 1024.
  --frame-augmentation.mask-positions.enable [FRAME_AUGMENTATION.MASK_POSITIONS.ENABLE]
                        Use MaskPositions. This flag is useful when you want
                        to study the effect of different transforms.
  --frame-augmentation.mask-positions.keep-frac FRAME_AUGMENTATION.MASK_POSITIONS.KEEP_FRAC
                        The fraction of bytes to keep. Defaults to 0.5.
  --frame-augmentation.byte-permutation.enable [FRAME_AUGMENTATION.BYTE_PERMUTATION.ENABLE]
                        Use BytePermutation. This flag is useful when you want
                        to study the effect of different transforms.
  --frame-augmentation.random-uniform.enable [FRAME_AUGMENTATION.RANDOM_UNIFORM.ENABLE]
                        Use RandomUniformNoise. This flag is useful when you
                        want to study the effect of different transforms.
  --frame-augmentation.random-uniform.width-range FRAME_AUGMENTATION.RANDOM_UNIFORM.WIDTH_RANGE FRAME_AUGMENTATION.RANDOM_UNIFORM.WIDTH_RANGE
                        The range of values from which to add noise. It is
                        specified as [low, high] (inclusive). Defaults to [-5,
                        5].
  --stats.metrics.retrieval-cmc.subset-fraction STATS.METRICS.RETRIEVAL_CMC.SUBSET_FRACTION
                        Use fraction of gallery set for CMC calculation when
                        set. Defaults to 1.0
  --stats.metrics.retrieval-cmc.k STATS.METRICS.RETRIEVAL_CMC.K
                        CMC top-k: percentage of query images with at least
                        one same-class gallery image in their k-NN. Defaults
                        to 5.
  --stats.metrics.retrieval-cmc.distance-metric {l2,cosine}
                        Distance to use for nearest-neighbor calculation.
                        Defaults to l2
  --stats.metrics.img-text-retrieval.distance-metric {cosine,l2}
                        Distance to use for nearest-neighbor calculation.
  --stats.metrics.multiclass-classification-pr.include-curve
                        If set, PR curves will be stored.
  --stats.metrics.multiclass-classification-pr.include-classwise-ap
                        If set, AP will be plotted for each class.
  --stats.metrics.multiclass-classification-pr.suppress-warnings
                        If set, warnings will be suppressed. This is useful to
                        reduce the logs size during training.
  --stats.metrics.prob-hist.num-bins STATS.METRICS.PROB_HIST.NUM_BINS

S3Client:
  --common.s3.endpoint-url COMMON.S3.ENDPOINT_URL
                        Endpoint URL for S3 client. Defaults to None.
  --common.s3.aws-access-key-id COMMON.S3.AWS_ACCESS_KEY_ID
                        AWS Access key id. Defaults to None.
  --common.s3.aws-secret-access-key COMMON.S3.AWS_SECRET_ACCESS_KEY
                        AWS secret access key. Defaults to None.
  --common.s3.aws-session-token COMMON.S3.AWS_SESSION_TOKEN
                        AWS session token. Defaults to None.
  --common.s3.multipart-chunksize COMMON.S3.MULTIPART_CHUNKSIZE
                        The partition size of each part for a multipart
                        transfer. Defaults to 64 MB.

BaseDataset:
  --dataset.root-train DATASET.ROOT_TRAIN
                        Root location of train dataset
  --dataset.pcap-collate-mode DATASET.PCAP_COLLATE_MODE
                        Tensor per-stream vs per-packet
  --dataset.root-val DATASET.ROOT_VAL
                        Root location of valid dataset
  --dataset.root-test DATASET.ROOT_TEST
                        Root location of test dataset
  --dataset.disable-val
                        Disable validation during training
  --dataset.name DATASET.NAME
                        Dataset name (e.g., imagenet). Defaults to None.
  --dataset.category DATASET.CATEGORY
                        Dataset category (e.g., segmentation, classification).
                        Defaults to None.
  --dataset.percentage-of-samples DATASET.PERCENTAGE_OF_SAMPLES
                        Percentage of samples to use from the dataset.
  --dataset.sample-selection-random-seed DATASET.SAMPLE_SELECTION_RANDOM_SEED
                        Random seed for selecting a subset of samples to use
                        from the dataset.
  --dataset.train-batch-size0 DATASET.TRAIN_BATCH_SIZE0
                        Training batch size on GPU-0. Defaults to 128. Note
                        that we scale it depending on total GPUs available for
                        training. For example, if 2 GPUs are available and
                        value of `dataset.train_batch_size0` is 128, then
                        effective batch size will be 256.
  --dataset.val-batch-size0 DATASET.VAL_BATCH_SIZE0
                        Batch size on GPU-0 for validation. Defaults to 1.
                        Note that we scale it depending on total GPUs
                        available for training. For example, if 2 GPUs are
                        available and value of `dataset.val_batch_size0` is
                        128, then effective batch size will be 256.
  --dataset.eval-batch-size0 DATASET.EVAL_BATCH_SIZE0
                        Batch size on GPU-0 for testing or evaluation.
                        Defaults to 1.Note that we scale it automatically
                        depending on total number of GPUs available. We
                        recommend to run evaluation on a single GPU machine.
  --dataset.workers DATASET.WORKERS
                        Number of data workers. Defaults to -1.When number of
                        workers are specified as -1, then total number of
                        workers is equal to the number of available CPUs.
  --dataset.persistent-workers
                        Enabling this argument allows us to use same workers
                        for loading data throughout the training. Defaults to
                        False.
  --dataset.pin-memory  Enabling this allows us to use pin memory option in
                        data loader. Defaults to False.
  --dataset.prefetch-factor DATASET.PREFETCH_FACTOR
                        Number of samples loaded in advance by each data
                        worker. Defaults to 2.
  --dataset.padding-index DATASET.PADDING_INDEX
                        Padding index for text vocabulary. Defaults to None.
  --dataset.text-vocab-size DATASET.TEXT_VOCAB_SIZE
                        Text vocabulary size. Defaults to -1.
  --dataset.text-context-length DATASET.TEXT_CONTEXT_LENGTH
                        Context length for text encoder. Defaults to None.

BaseImageSegmentationDataset:
  --evaluation.segmentation.apply-color-map
                        Apply color map to different classes in segmentation
                        masks. Useful in visualization + some competitions
                        (e.g, PASCAL VOC) accept submissions with colored
                        segmentation masks.Defaults to False.
  --evaluation.segmentation.save-overlay-rgb-pred
                        Enable this flag to visualize predicted masks on top
                        of input image. Defaults to False.
  --evaluation.segmentation.save-masks
                        Save predicted masks without colormaps. Useful for
                        submitting to competitions like Cityscapes. Defaults
                        to False.
  --evaluation.segmentation.overlay-mask-weight EVALUATION.SEGMENTATION.OVERLAY_MASK_WEIGHT
                        Contribution of mask when overlaying on top of RGB
                        image. Defaults to 0.5.
  --evaluation.segmentation.mode {single_image,image_folder,validation_set}
                        Contribution of mask when overlaying on top of RGB
                        image. Defaults to validation_set.
  --evaluation.segmentation.path EVALUATION.SEGMENTATION.PATH
                        Path of the image or image folder (only required for
                        single_image and image_folder modes). Defaults to
                        None.
  --evaluation.segmentation.resize-input-images
                        Enable resizing input images while maintaining aspect
                        ratio during segmentation evaluation.Defaults to
                        False.
  --evaluation.segmentation.resize-input-images-fixed-size
                        Enable resizing input images to fixed size during
                        segmentation evaluation. Defaults to False.

BaseVideoDataset:
  --dataset.clips-per-video DATASET.CLIPS_PER_VIDEO
                        The number of clips that each video file gets split
                        into. Default value is 1, i.e., we don't split videos
                        into multiple clips.
  --dataset.n-frames-per-clip DATASET.N_FRAMES_PER_CLIP
                        The number of frames to read from the video file into
                        each clip. Defaults to 64.

BaseImageClassificationDataset:
  --dataset.num-samples-per-category DATASET.NUM_SAMPLES_PER_CATEGORY
                        Number of samples to use per category. If set to -1,
                        all samples will be used.

BaseDetectionDataset:
  --evaluation.detection.save-overlay-boxes
                        enable this flag to visualize predicted masks on top
                        of input image
  --evaluation.detection.mode {single_image,image_folder,validation_set}
                        Contribution of mask when overlaying on top of RGB
                        image.
  --evaluation.detection.path EVALUATION.DETECTION.PATH
                        Path of the image or image folder (only required for
                        single_image and image_folder modes).
  --evaluation.detection.num-classes EVALUATION.DETECTION.NUM_CLASSES
                        Number of segmentation classes used during training.
  --evaluation.detection.resize-input-images
                        Resize input images to fixed size during detection
                        evaluation.

BaseLMIterableDataset:
  --dataset.language-modeling.sequence-length DATASET.LANGUAGE_MODELING.SEQUENCE_LENGTH
                        Tokenized sequence length. Defaults to 2048.
  --dataset.language-modeling.min-tokens-per-text DATASET.LANGUAGE_MODELING.MIN_TOKENS_PER_TEXT
                        Minimum number of tokens per text after tokenization.
                        This flag allows us to skip short text sequences and
                        avoid excessive padding. Defaults to 0.
  --dataset.language-modeling.min-characters-per-text DATASET.LANGUAGE_MODELING.MIN_CHARACTERS_PER_TEXT
                        Minimum number of characters in a text sequence before
                        tokenization. This flag allows us to skip short text
                        sequences. Defaults to 0.
  --dataset.language-modeling.shuffle-data
                        The pre-training corpora consist of multiple text
                        files. This flag can be utilized to enable shuffling
                        of these data files. It defaults to False, with the
                        note that the user is responsible for implementing the
                        shuffling operation.
  --dataset.language-modeling.random-seed DATASET.LANGUAGE_MODELING.RANDOM_SEED
                        Random seed for shuffling data files. Defaults to 0.

BaseZeroShotImageClassificationDataset:
  --dataset.multi-modal-img-text.zero-shot-img-cls-dataset-name DATASET.MULTI_MODAL_IMG_TEXT.ZERO_SHOT_IMG_CLS_DATASET_NAME
                        Name of the dataset for zero-shot image classification
                        evaluation. Defaults to None.

BaseMultiModalImgText:
  --dataset.multi-modal-img-text.context-length DATASET.MULTI_MODAL_IMG_TEXT.CONTEXT_LENGTH
                        Context length for the text model. Defaults to 77, the
                        same as in the CLIP paper.
  --dataset.multi-modal-img-text.padding-index DATASET.MULTI_MODAL_IMG_TEXT.PADDING_INDEX
                        Padding index. Defaults to None.
  --dataset.multi-modal-img-text.trunc-seq-len
                        Many sequences in a batch do not have lengths equal to
                        specified context length. Enabling this flag allows us
                        to truncate the sequences such that the sequence
                        length of a batch is equal to sequence with max. non-
                        padded tokens. Defaults to False.

SpeechCommandsv2Dataset:
  --dataset.speech-commands-v2.mixup
                        If set, apply mixup inside the dataset.

Imagenetv2Dataset:
  --dataset.imagenet-v2.split {matched_frequency,threshold_0.7,top_images}
                        ImageNetv2 dataset. Possible choices are: ['1:
                        matched_frequency', '2: threshold_0.7', '3:
                        top_images']

WordnetTaggedClassificationDataset:
  --dataset.wordnet-tagged-classification.vocab-file DATASET.WORDNET_TAGGED_CLASSIFICATION.VOCAB_FILE
                        Location of vocab pickle file. Defaults to None.
  --dataset.wordnet-tagged-classification.metadata-file DATASET.WORDNET_TAGGED_CLASSIFICATION.METADATA_FILE
                        Metadata file containing information about img-text
                        pairs. Defaults to None.
  --dataset.wordnet-tagged-classification.vocab-size DATASET.WORDNET_TAGGED_CLASSIFICATION.VOCAB_SIZE
                        Vocabulary threshold. Synsets in the ordered
                        vocabulary dictionary beyond this threshold will not
                        be used. Defaults to None (i.e., user needs to specify
                        the value).

COCODetection:
  --dataset.detection.no-background-id
                        Do not include background id in detection class
                        labels. Defaults to False.

COCODetectionMaskRCNN:
  --dataset.detection.coco-mask-rcnn.use-lsj-aug
                        Use large scale jitter augmentation for training Mask
                        RCNN model

CommonSense170k:
  --dataset.language-modeling.commonsense-170k.path DATASET.LANGUAGE_MODELING.COMMONSENSE_170K.PATH
                        Path to the commonsense 170k json dataset file.
                        Default is None. Note, the dataset file is currently
                        available in the LLM-Adapters repository at
                        https://github.com/AGI-Edgerunners/LLM-
                        Adapters/blob/main/ft-
                        training_set/commonsense_170k.json.

GeneralLMDataset:
  --dataset.language-modeling.general-lm.train-data-info DATASET.LANGUAGE_MODELING.GENERAL_LM.TRAIN_DATA_INFO [DATASET.LANGUAGE_MODELING.GENERAL_LM.TRAIN_DATA_INFO ...]
                        Name of the parquet files for the train set. Defaults
                        to None (i.e., user needs to specify the value).
  --dataset.language-modeling.general-lm.val-data-info DATASET.LANGUAGE_MODELING.GENERAL_LM.VAL_DATA_INFO [DATASET.LANGUAGE_MODELING.GENERAL_LM.VAL_DATA_INFO ...]
                        Name of the parquet files for the val set. Defaults to
                        None (i.e., user needs to specify the value).
  --dataset.language-modeling.general-lm.test-data-info DATASET.LANGUAGE_MODELING.GENERAL_LM.TEST_DATA_INFO [DATASET.LANGUAGE_MODELING.GENERAL_LM.TEST_DATA_INFO ...]
                        Name of the parquet files for the test set. Defaults
                        to None (i.e., user needs to specify the value).
  --dataset.language-modeling.general-lm.data-state DATASET.LANGUAGE_MODELING.GENERAL_LM.DATA_STATE [DATASET.LANGUAGE_MODELING.GENERAL_LM.DATA_STATE ...]
                        A list containing the filenames that each process was
                        processing before crash. Defaults to None.
  --dataset.language-modeling.general-lm.reader-chunk-size DATASET.LANGUAGE_MODELING.GENERAL_LM.READER_CHUNK_SIZE
                        Number of documents to read from a dataset file at a
                        time. Defaults to 1024.
  --dataset.language-modeling.general-lm.document-split-size DATASET.LANGUAGE_MODELING.GENERAL_LM.DOCUMENT_SPLIT_SIZE
                        The length of each sequence when splitting a larger
                        document. Defaults to 2048 words.
  --dataset.language-modeling.general-lm.data-state-save-interval DATASET.LANGUAGE_MODELING.GENERAL_LM.DATA_STATE_SAVE_INTERVAL
                        Data state save interval in minutes. Defaults to 15
                        minutes.

ImgTextTarDataset:
  --dataset.multi-modal-img-text.img-text-tar.metadata-file DATASET.MULTI_MODAL_IMG_TEXT.IMG_TEXT_TAR.METADATA_FILE
                        Location of the metadata file storing information
                        about file indices and corresponding tar files.
                        Defaults to None.

PascalVOCDataset:
  --dataset.pascal.use-coco-data
                        Use MS-COCO data for training with PASCAL VOC dataset.
                        Defaults to False.
  --dataset.pascal.coco-root-dir DATASET.PASCAL.COCO_ROOT_DIR
                        Location of MS-COCO data. Defaults to None.

BaseTextEncoder:
  --model.text.name MODEL.TEXT.NAME
                        Name of the text encoder
  --model.text.padding-index MODEL.TEXT.PADDING_INDEX
                        Padding index. Defaults to None.
  --model.text.context-length MODEL.TEXT.CONTEXT_LENGTH
                        Context length. Defaults to None.
  --model.text.vocab-size MODEL.TEXT.VOCAB_SIZE
                        Vocabulary size. Defaults to None.

TextTransformer:
  --model.text.transformer.model-dim MODEL.TEXT.TRANSFORMER.MODEL_DIM
                        Model dimension of the transformer model
  --model.text.transformer.no-scale-embedding
                        Do not scale the output of embedding layer in
                        TextTransformer
  --model.text.transformer.no-pos-embedding
                        Do not add positional embeddings to the output of
                        embedding layer in TextTransformer
  --model.text.transformer.embed-dropout MODEL.TEXT.TRANSFORMER.EMBED_DROPOUT
                        Dropout in embedding layer
  --model.text.transformer.n-transformer-layers MODEL.TEXT.TRANSFORMER.N_TRANSFORMER_LAYERS
                        Number of transformer layers in TextTransformer
  --model.text.transformer.n-heads-per-layer MODEL.TEXT.TRANSFORMER.N_HEADS_PER_LAYER [MODEL.TEXT.TRANSFORMER.N_HEADS_PER_LAYER ...]
                        Number of transformer heads per transformer layer
  --model.text.transformer.ffn-multiplier-per-layer MODEL.TEXT.TRANSFORMER.FFN_MULTIPLIER_PER_LAYER [MODEL.TEXT.TRANSFORMER.FFN_MULTIPLIER_PER_LAYER ...]
                        FFN multiplier for each transformer layer
  --model.text.transformer.attn-dropout MODEL.TEXT.TRANSFORMER.ATTN_DROPOUT
                        Dropout in multi-head attention
  --model.text.transformer.ffn-dropout MODEL.TEXT.TRANSFORMER.FFN_DROPOUT
                        Dropout between linear layers in FFN
  --model.text.transformer.dropout MODEL.TEXT.TRANSFORMER.DROPOUT
                        Dropout in transformer
  --model.text.transformer.norm-layer MODEL.TEXT.TRANSFORMER.NORM_LAYER
                        Normalization layer
  --model.text.transformer.sinusoidal-pos-emb
                        Use sinusoidal positional embedding
  --model.text.transformer.causal-masking
                        Use causal masking
  --model.text.transformer.classes-per-split-zero-shot MODEL.TEXT.TRANSFORMER.CLASSES_PER_SPLIT_ZERO_SHOT
                        Divide zero-shot classes into these many chunks, for
                        faster processing

BaseImageProjectionHead:
  --model.image-projection-head.name MODEL.IMAGE_PROJECTION_HEAD.NAME
                        Name of the image projection head
  --model.image-projection-head.lr-multiplier MODEL.IMAGE_PROJECTION_HEAD.LR_MULTIPLIER
                        LR multiplier for image projection head

AttentionPool2dHead:
  --model.image-projection-head.attention-pool-nchw2nc.num-pos-embeddings MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NUM_POS_EMBEDDINGS
                        Number of positional embeddings
  --model.image-projection-head.attention-pool-nchw2nc.use-sinusoidal-pos-embeddings
                        Use sinusoidal positional embeddings instead of
                        learnable
  --model.image-projection-head.attention-pool-nchw2nc.num-attn-heads MODEL.IMAGE_PROJECTION_HEAD.ATTENTION_POOL_NCHW2NC.NUM_ATTN_HEADS
                        Number of attention heads in AttentionPool2dHead
  --model.image-projection-head.attention-pool-nchw2nc.no-feature-normalization
                        Don't normalize image features
  --model.image-projection-head.attention-pool-nchw2nc.use-pytorch-mha
                        Use Pytorch Multi-head attention

GlobalPool2D:
  --model.image-projection-head.global-pool-nchw2nc.no-feature-normalization
                        Don't normalize image features. Defaults to False.
  --model.image-projection-head.global-pool-nchw2nc.identity-if-same-size
                        Use identity projection when projection input/output
                        dims are the same. Defaults to False.

SimpleImageProjectionHead:
  --model.image-projection-head.simple-projection-nc2nc.no-feature-normalization
                        Don't normalize image features. Defaults to False.
  --model.image-projection-head.simple-projection-nc2nc.identity-if-same-size
                        Use identity projection when projection input/output
                        dims are the same. Defaults to False

BaseAnyNNModel:
  --model.resume-exclude-scopes MODEL.RESUME_EXCLUDE_SCOPES
                        Comma-separated list of parameter scopes (regex
                        strings) to exclude when loading a pre-trained model
  --model.ignore-missing-scopes MODEL.IGNORE_MISSING_SCOPES
                        Comma-separated list of parameter scopes (regex
                        strings) to ignore if they are missing from the pre-
                        training model
  --model.rename-scopes-map MODEL.RENAME_SCOPES_MAP
                        A mapping from checkpoint variable names to match the
                        existing model names. The mapping is represented as a
                        List[List[str]], e.g. [['before', 'after'], ['this',
                        'that']]. Note: only loading from Yaml file is
                        supported for this argument.
  --model.freeze-modules MODEL.FREEZE_MODULES
                        Comma-separated list of parameter scopes (regex
                        strings) to freeze.
  --model.activation-checkpointing
                        If enabled, layer specified in model using
                        'get_activation_checkpoint_submodule_class' would be
                        used for activation checkpointing (a.k.a. gradient
                        checkpointing).
  --model.lora.config MODEL.LORA.CONFIG
                        A json-formatted configuration for the LoRA model. See
                        corenet/modeling/models/language_modeling/lora.py for
                        details.
  --model.lora.use-lora
                        If set, use LoRA for the model. Note that parameters
                        are not automatically frozen. They must be frozen with
                        --model.freeze-modules.

BaseAudioClassification:
  --model.audio-classification.name MODEL.AUDIO_CLASSIFICATION.NAME
                        Name of the audio classification model. Defaults to
                        None.
  --model.audio-classification.pretrained MODEL.AUDIO_CLASSIFICATION.PRETRAINED
                        Path of the pretrained backbone. Defaults to None.

ByteFormer:
  --model.classification.byteformer.dropout MODEL.CLASSIFICATION.BYTEFORMER.DROPOUT
                        Dropout in Byteformer layers. Defaults to 0.0.
  --model.classification.byteformer.stochastic-dropout MODEL.CLASSIFICATION.BYTEFORMER.STOCHASTIC_DROPOUT
                        Probability of applying stochastic dropout to
                        TransformerEncoder submodules. Defaults to 0.0.
  --model.classification.byteformer.norm-layer {group_norm,layer_norm,layer_norm_nchw,layer_norm_2d,layer_norm_fp32,instance_norm_2d,instance_norm,instance_norm_1d,batch_norm_2d,batch_norm,batch_norm_fp32,batch_norm_1d,batch_norm_3d,sync_batch_norm,sync_batch_norm_fp32,rms_norm}
                        Normalization layer in Byteformer. Defaults to
                        LayerNorm.
  --model.classification.byteformer.sinusoidal-pos-emb
                        Use sinusoidal instead of learnable positional
                        encoding. Defaults to False.
  --model.classification.byteformer.use-pytorch-mha
                        Use PyTorch's native multi-head attention. Defaults to
                        False.
  --model.classification.byteformer.mode {tiny,small,base,huge}
                        Byteformer mode, which determines the model size.
                        Defaults to tiny.
  --model.classification.byteformer.vocab-size MODEL.CLASSIFICATION.BYTEFORMER.VOCAB_SIZE
                        The vocab size of the token embedding. Defaults to
                        257,corresponding to the number of unique bytes (256)
                        plus 1 more for the mask token.
  --model.classification.byteformer.max-num-tokens MODEL.CLASSIFICATION.BYTEFORMER.MAX_NUM_TOKENS
                        The maximum number of tokens that can be input to the
                        network. Defaults to 10000.
  --model.classification.byteformer.conv-kernel-size MODEL.CLASSIFICATION.BYTEFORMER.CONV_KERNEL_SIZE
                        The size of the kernel of the initial downsampling
                        conv1d. Defaults to 16.
  --model.classification.byteformer.window-sizes [MODEL.CLASSIFICATION.BYTEFORMER.WINDOW_SIZES ...]
                        A list of window sizes used in shifted window
                        attention. If the list is length 1, the same window
                        size is used for all windows. Defaults to 128 for all
                        windows.
  --model.classification.byteformer.window-shifts [MODEL.CLASSIFICATION.BYTEFORMER.WINDOW_SHIFTS ...]
                        A list of shifts used in shifted window attention.
                        Defaults to values that alternate between 0 and 64.
  --model.classification.byteformer.downsample [MODEL.CLASSIFICATION.BYTEFORMER.DOWNSAMPLE ...]
                        A list of boolean values, where the i'th element
                        specifies whether to downsample after the transformer
                        block with index i. Defaults to [True, True, False,
                        True, False, True, False, True, False, True, False,
                        False].
  --model.classification.byteformer.padding-index MODEL.CLASSIFICATION.BYTEFORMER.PADDING_INDEX
                        The index used for padding tokens. Defaults to -1.
  --model.classification.byteformer.dummy-input-token-length MODEL.CLASSIFICATION.BYTEFORMER.DUMMY_INPUT_TOKEN_LENGTH
                        The token length to use for dummy inputs. Defaults to
                        48564, corresponding to the average length of 224x224
                        JPEG images from ImageNet.

BaseImageEncoder:
  --model.classification.classifier-dropout MODEL.CLASSIFICATION.CLASSIFIER_DROPOUT
                        Dropout rate in classifier
  --model.classification.name MODEL.CLASSIFICATION.NAME
                        Model name
  --model.classification.n-classes MODEL.CLASSIFICATION.N_CLASSES
                        Number of classes in the dataset
  --model.classification.pretrained MODEL.CLASSIFICATION.PRETRAINED
                        Path of the pretrained backbone
  --model.classification.freeze-batch-norm
                        Freeze batch norm layers
  --model.classification.activation.name MODEL.CLASSIFICATION.ACTIVATION.NAME
                        Non-linear function name (e.g., relu)
  --model.classification.activation.inplace
                        Inplace non-linear functions
  --model.classification.activation.neg-slope MODEL.CLASSIFICATION.ACTIVATION.NEG_SLOPE
                        Negative slope in leaky relu
  --model.classification.finetune-pretrained-model
                        Finetune a pretrained model
  --model.classification.n-pretrained-classes MODEL.CLASSIFICATION.N_PRETRAINED_CLASSES
                        Number of pre-trained classes
  --model.classification.gradient-checkpointing
                        Checkpoint output of each spatial level in the
                        classification backbone. Note thatwe only take care of
                        checkpointing in {}. If custom forward functions are
                        used, pleaseimplement checkpointing accordingly. This
                        option is deprecated in favor or --model.activation-
                        checkpointing.
  --model.classification.enable-layer-wise-lr-decay
                        Enable layer-wise LR.
  --model.classification.layer-wise-lr-decay-rate MODEL.CLASSIFICATION.LAYER_WISE_LR_DECAY_RATE
                        Layer-wise LR decay range. Each model needs to define
                        how layer-wise LR should be decayed.For ViT, we decay
                        layer_wise_lr_decay_rate ** (n_layers - i), where i is
                        the layer index.

EfficientNet:
  --model.classification.efficientnet.mode {b0,b1,b2,b3,b4,b5,b6,b7}
  --model.classification.efficientnet.stochastic-depth-prob MODEL.CLASSIFICATION.EFFICIENTNET.STOCHASTIC_DEPTH_PROB

FastViT:
  --model.classification.fastvit.variant MODEL.CLASSIFICATION.FASTVIT.VARIANT
                        Variant string for FastViT. Default: T8
  --model.classification.fastvit.inference-mode MODEL.CLASSIFICATION.FASTVIT.INFERENCE_MODE
                        Flag to instantiate inference mode architecture.
                        Default: False
  --model.classification.fastvit.dropout MODEL.CLASSIFICATION.FASTVIT.DROPOUT
                        Dropout rate for regularization. Default: 0.0
  --model.classification.fastvit.drop-path MODEL.CLASSIFICATION.FASTVIT.DROP_PATH
                        Drop path rate. Default: 0.0
  --model.classification.fastvit.use-layer-scale MODEL.CLASSIFICATION.FASTVIT.USE_LAYER_SCALE
                        Flag to turn on layer scale regularization. Default:
                        True
  --model.classification.fastvit.layer-scale-init-value MODEL.CLASSIFICATION.FASTVIT.LAYER_SCALE_INIT_VALUE
                        Drop path rate. Default: 1e-5

MobileNetv1:
  --model.classification.mobilenetv1.width-multiplier MODEL.CLASSIFICATION.MOBILENETV1.WIDTH_MULTIPLIER
                        Width multiplier for MobileNetv1. Default: 1.0

MobileNetV2:
  --model.classification.mobilenetv2.width-multiplier MODEL.CLASSIFICATION.MOBILENETV2.WIDTH_MULTIPLIER
                        Width multiplier for MobileNetv2. Default: 1.0

MobileNetV3:
  --model.classification.mobilenetv3.mode {small,large}
                        Configuration for mobilenetv3. Default: large
  --model.classification.mobilenetv3.width-multiplier MODEL.CLASSIFICATION.MOBILENETV3.WIDTH_MULTIPLIER
                        Width multiplier for mobilenetv3. Default: 1.0

MobileOne:
  --model.classification.mobileone.variant MODEL.CLASSIFICATION.MOBILEONE.VARIANT
                        Variant string for MobileOne. Default: s1
  --model.classification.mobileone.inference-mode MODEL.CLASSIFICATION.MOBILEONE.INFERENCE_MODE
                        Flag to instantiate inference mode architecture.
                        Default: False

MobileViT:
  --model.classification.mit.mode {xx_small,x_small,small}
                        MobileViT mode. Defaults to small
  --model.classification.mit.attn-dropout MODEL.CLASSIFICATION.MIT.ATTN_DROPOUT
                        Dropout in attention layer. Defaults to 0.0
  --model.classification.mit.ffn-dropout MODEL.CLASSIFICATION.MIT.FFN_DROPOUT
                        Dropout between FFN layers. Defaults to 0.0
  --model.classification.mit.dropout MODEL.CLASSIFICATION.MIT.DROPOUT
                        Dropout in Transformer layer. Defaults to 0.0
  --model.classification.mit.transformer-norm-layer MODEL.CLASSIFICATION.MIT.TRANSFORMER_NORM_LAYER
                        Normalization layer in transformer. Defaults to
                        LayerNorm
  --model.classification.mit.no-fuse-local-global-features
                        Do not combine local and global features in MobileViT
                        block
  --model.classification.mit.conv-kernel-size MODEL.CLASSIFICATION.MIT.CONV_KERNEL_SIZE
                        Kernel size of Conv layers in MobileViT block
  --model.classification.mit.head-dim MODEL.CLASSIFICATION.MIT.HEAD_DIM
                        Head dimension in transformer
  --model.classification.mit.number-heads MODEL.CLASSIFICATION.MIT.NUMBER_HEADS
                        Number of heads in transformer

MobileViTv2:
  --model.classification.mitv2.attn-dropout MODEL.CLASSIFICATION.MITV2.ATTN_DROPOUT
                        Dropout in attention layer. Defaults to 0.0
  --model.classification.mitv2.ffn-dropout MODEL.CLASSIFICATION.MITV2.FFN_DROPOUT
                        Dropout between FFN layers. Defaults to 0.0
  --model.classification.mitv2.dropout MODEL.CLASSIFICATION.MITV2.DROPOUT
                        Dropout in attention layer. Defaults to 0.0
  --model.classification.mitv2.width-multiplier MODEL.CLASSIFICATION.MITV2.WIDTH_MULTIPLIER
                        Width multiplier. Defaults to 1.0
  --model.classification.mitv2.attn-norm-layer MODEL.CLASSIFICATION.MITV2.ATTN_NORM_LAYER
                        Norm layer in attention block. Defaults to LayerNorm

RegNet:
  --model.classification.regnet.mode MODEL.CLASSIFICATION.REGNET.MODE
                        The RegNet<mode> to use. Must be one of x_200mf,
                        x_400mf, x_600mf, x_800mf, x_1.6gf, x_3.2gf, x_4.0gf,
                        x_6.4gf, x_8.0gf, x_12gf, x_16gf, x_32gf, y_200mf,
                        y_400mf, y_800mf, y_600mf, y_1.6gf, y_3.2gf, y_4.0gf,
                        y_6.4gf, y_8.0gf, y_12gf, y_16gf, y_32gf. Defaults to
                        y_4.0gf.
  --model.classification.regnet.stochastic-depth-prob MODEL.CLASSIFICATION.REGNET.STOCHASTIC_DEPTH_PROB
                        Stochastic depth drop probability in RegNet blocks.
                        Defaults to 0.
  --model.classification.regnet.stem-width MODEL.CLASSIFICATION.REGNET.STEM_WIDTH
                        The number of output channels of the first conv layer.
                        Defaults to 32

ResNet:
  --model.classification.resnet.depth MODEL.CLASSIFICATION.RESNET.DEPTH
  --model.classification.resnet.dropout MODEL.CLASSIFICATION.RESNET.DROPOUT
                        Dropout in Resnet blocks. Defaults to 0.
  --model.classification.resnet.stochastic-depth-prob MODEL.CLASSIFICATION.RESNET.STOCHASTIC_DEPTH_PROB
                        Stochastic depth drop probability in Resnet blocks.
                        Defaults to 0.
  --model.classification.resnet.se-resnet
                        Whether to use SE block to construct SE-ResNet model.
                        Defaults to False.

SwinTransformer:
  --model.classification.swin.mode MODEL.CLASSIFICATION.SWIN.MODE
                        SwinTransformer mode. Default is swin_t
  --model.classification.swin.stochastic-depth-prob MODEL.CLASSIFICATION.SWIN.STOCHASTIC_DEPTH_PROB
  --model.classification.swin.extract-end-point-format {nchw,nhwc}
                        End point extraction format in Swin Transformer. This
                        is useful for down-stream tasks where task-specific
                        heads are either in nhwc format or nchw format.
                        Defaults to nchw.

VisionTransformer:
  --model.classification.vit.mode {tiny,small,base,large,huge}
                        ViT mode. Default is base.
  --model.classification.vit.dropout MODEL.CLASSIFICATION.VIT.DROPOUT
                        Dropout in Transformer layers. Defaults to 0.0.
  --model.classification.vit.stochastic-dropout MODEL.CLASSIFICATION.VIT.STOCHASTIC_DROPOUT
                        Stochastic Dropout in Transformer layers. Defaults to
                        0.0.
  --model.classification.vit.norm-layer MODEL.CLASSIFICATION.VIT.NORM_LAYER
                        Normalization layer to be used in Transformer layer.
                        Defaults to LayerNorm.
  --model.classification.vit.sinusoidal-pos-emb
                        Use sinusoidal instead of learnable positional
                        embedding. Defaults to False.
  --model.classification.vit.no-cls-token
                        Do not use classification token. Defaults to False.
  --model.classification.vit.use-simple-fpn
                        Add simple FPN for down-stream tasks (e.g.,
                        detection). Defaults to False.
  --model.classification.vit.use-flash-attention
                        Use Transformer layers with flash attention for
                        efficiently computing scaled dot-product attention.
                        Defaults to False.

BaseDetection:
  --model.detection.name MODEL.DETECTION.NAME
                        Detection model name
  --model.detection.n-classes MODEL.DETECTION.N_CLASSES
                        Number of classes in the dataset. Defaults to 80.
  --model.detection.pretrained MODEL.DETECTION.PRETRAINED
                        Path of the pretrained detection model. Defaults to
                        None.
  --model.detection.output-stride MODEL.DETECTION.OUTPUT_STRIDE
                        Output stride of the classification network. Defaults
                        to None.
  --model.detection.replace-stride-with-dilation
                        Replace stride with dilation
  --model.detection.freeze-batch-norm
                        Freeze batch norm layers in detection model. Defaults
                        to False.

MaskRCNNDetector:
  --model.detection.mask-rcnn.backbone-projection-channels MODEL.DETECTION.MASK_RCNN.BACKBONE_PROJECTION_CHANNELS
                        Projection channels for the encoder in Mask-RCNN
  --model.detection.mask-rcnn.backbone-lr-multiplier MODEL.DETECTION.MASK_RCNN.BACKBONE_LR_MULTIPLIER
                        LR multiplier for MASK RCNN head
  --model.detection.mask-rcnn.output-strides MODEL.DETECTION.MASK_RCNN.OUTPUT_STRIDES [MODEL.DETECTION.MASK_RCNN.OUTPUT_STRIDES ...]
                        Extract backbone feature maps from these output
                        strides. If output stride is greater than 32, extra
                        layers are added.
  --model.detection.mask-rcnn.anchor-sizes MODEL.DETECTION.MASK_RCNN.ANCHOR_SIZES [MODEL.DETECTION.MASK_RCNN.ANCHOR_SIZES ...]
                        Anchor sizes at each output stride
  --model.detection.mask-rcnn.aspect-ratio MODEL.DETECTION.MASK_RCNN.ASPECT_RATIO [MODEL.DETECTION.MASK_RCNN.ASPECT_RATIO ...]
                        Aspect ratios. These are the same for all feature maps
  --model.detection.mask-rcnn.bbox-head-fm-size MODEL.DETECTION.MASK_RCNN.BBOX_HEAD_FM_SIZE
                        Feature map size for the box head
  --model.detection.mask-rcnn.mask-head-fm-size MODEL.DETECTION.MASK_RCNN.MASK_HEAD_FM_SIZE
                        Feature map size for the max head
  --model.detection.mask-rcnn.representation-size MODEL.DETECTION.MASK_RCNN.REPRESENTATION_SIZE
                        Size of the intermediate representation in Mask RCNN
  --model.detection.mask-rcnn.box-fm-size-conv-layer MODEL.DETECTION.MASK_RCNN.BOX_FM_SIZE_CONV_LAYER [MODEL.DETECTION.MASK_RCNN.BOX_FM_SIZE_CONV_LAYER ...]
                        Feature dim of each Convolution layer in the Faster
                        RCNN head. Defaults to [256, 256, 256, 256]
  --model.detection.mask-rcnn.mask-fm-size-conv-layer MODEL.DETECTION.MASK_RCNN.MASK_FM_SIZE_CONV_LAYER [MODEL.DETECTION.MASK_RCNN.MASK_FM_SIZE_CONV_LAYER ...]
                        Feature dim of each Convolution layer in the Mask RCNN
                        head. Defaults to [256, 256, 256, 256]
  --model.detection.mask-rcnn.mask-dilation MODEL.DETECTION.MASK_RCNN.MASK_DILATION
                        Dilation rate in Mask RCNN head. Defaults to 1
  --model.detection.mask-rcnn.rpn-pre-nms-top-n-train MODEL.DETECTION.MASK_RCNN.RPN_PRE_NMS_TOP_N_TRAIN
                        Number of proposals to keep before applying NMS during
                        training
  --model.detection.mask-rcnn.rpn-pre-nms-top-n-test MODEL.DETECTION.MASK_RCNN.RPN_PRE_NMS_TOP_N_TEST
                        Number of proposals to keep before applying NMS during
                        test
  --model.detection.mask-rcnn.rpn-post-nms-top-n-train MODEL.DETECTION.MASK_RCNN.RPN_POST_NMS_TOP_N_TRAIN
                        Number of proposals to keep after applying NMS during
                        training
  --model.detection.mask-rcnn.rpn-post-nms-top-n-test MODEL.DETECTION.MASK_RCNN.RPN_POST_NMS_TOP_N_TEST
                        Number of proposals to keep after applying NMS during
                        test
  --model.detection.mask-rcnn.rpn-nms-thresh MODEL.DETECTION.MASK_RCNN.RPN_NMS_THRESH
                        NMS threshold used for postprocessing the RPN
                        proposals
  --model.detection.mask-rcnn.rpn-fg-iou-thresh MODEL.DETECTION.MASK_RCNN.RPN_FG_IOU_THRESH
                        minimum IoU between the anchor and the GT box so that
                        they can be considered as positive during training of
                        the RPN.
  --model.detection.mask-rcnn.rpn-bg-iou-thresh MODEL.DETECTION.MASK_RCNN.RPN_BG_IOU_THRESH
                        minimum IoU between the anchor and the GT box so that
                        they can be considered as negative during training of
                        the RPN.
  --model.detection.mask-rcnn.rpn-batch-size-per-image MODEL.DETECTION.MASK_RCNN.RPN_BATCH_SIZE_PER_IMAGE
                        Number of anchors that are sampled during training of
                        the RPN for computing the loss
  --model.detection.mask-rcnn.rpn-positive-fraction MODEL.DETECTION.MASK_RCNN.RPN_POSITIVE_FRACTION
                        Proportion of positive anchors in a mini-batch during
                        training of the RPN
  --model.detection.mask-rcnn.rpn-score-thresh MODEL.DETECTION.MASK_RCNN.RPN_SCORE_THRESH
                        During inference, only return proposals with a
                        classification score greater than rpn_score_thresh
  --model.detection.mask-rcnn.box-score-thresh MODEL.DETECTION.MASK_RCNN.BOX_SCORE_THRESH
                        During inference, only return proposals with a
                        classification score greater than box_score_thresh
  --model.detection.mask-rcnn.box-nms-thresh MODEL.DETECTION.MASK_RCNN.BOX_NMS_THRESH
                        During inference, NMS threshold for the prediction
                        head.
  --model.detection.mask-rcnn.box-detections-per-img MODEL.DETECTION.MASK_RCNN.BOX_DETECTIONS_PER_IMG
                        Maximum number of detections per image, for all
                        classes
  --model.detection.mask-rcnn.box-fg-iou-thresh MODEL.DETECTION.MASK_RCNN.BOX_FG_IOU_THRESH
                        Minimum IoU between the proposals and the GT box so
                        that they can be considered as positive during
                        training of the classification head
  --model.detection.mask-rcnn.box-bg-iou-thresh MODEL.DETECTION.MASK_RCNN.BOX_BG_IOU_THRESH
                        Minimum IoU between the proposals and the GT box so
                        that they can be considered as negative during
                        training of the classification head
  --model.detection.mask-rcnn.box-batch-size-per-image MODEL.DETECTION.MASK_RCNN.BOX_BATCH_SIZE_PER_IMAGE
                        Number of proposals that are sampled during training
                        of the classification head
  --model.detection.mask-rcnn.box-positive-fraction MODEL.DETECTION.MASK_RCNN.BOX_POSITIVE_FRACTION
                        Proportion of positive proposals in a mini-batch
                        during training of the classification head
  --model.detection.mask-rcnn.norm-layer MODEL.DETECTION.MASK_RCNN.NORM_LAYER
                        Mask RCNN Norm layer
  --model.detection.mask-rcnn.disable-fpn
                        Do not use FPN

SingleShotMaskDetector:
  --model.detection.ssd.anchors-aspect-ratio MODEL.DETECTION.SSD.ANCHORS_ASPECT_RATIO [MODEL.DETECTION.SSD.ANCHORS_ASPECT_RATIO ...]
                        Anchors aspect ratio in each feature map obtained at
                        different output strides.
  --model.detection.ssd.output-strides MODEL.DETECTION.SSD.OUTPUT_STRIDES [MODEL.DETECTION.SSD.OUTPUT_STRIDES ...]
                        Extract feature maps from these output strides.
  --model.detection.ssd.proj-channels MODEL.DETECTION.SSD.PROJ_CHANNELS [MODEL.DETECTION.SSD.PROJ_CHANNELS ...]
                        Projection channels for feature map obtained at each
                        output stride
  --model.detection.ssd.min-box-size MODEL.DETECTION.SSD.MIN_BOX_SIZE
                        Min. box size. Value between 0 and 1. Good default
                        value is 0.1
  --model.detection.ssd.max-box-size MODEL.DETECTION.SSD.MAX_BOX_SIZE
                        Max. box size. Value between 0 and 1. Good default
                        value is 1.05
  --model.detection.ssd.center-variance MODEL.DETECTION.SSD.CENTER_VARIANCE
                        Center variance.
  --model.detection.ssd.size-variance MODEL.DETECTION.SSD.SIZE_VARIANCE
                        Size variance.
  --model.detection.ssd.iou-threshold MODEL.DETECTION.SSD.IOU_THRESHOLD
                        IOU Threshold.
  --model.detection.ssd.conf-threshold MODEL.DETECTION.SSD.CONF_THRESHOLD
                        Confidence threshold. For evaluation on COCO, set to
                        0.01, so that we can compute mAP
  --model.detection.ssd.top-k MODEL.DETECTION.SSD.TOP_K
                        Keep only top-k objects before NMS
  --model.detection.ssd.objects-per-image MODEL.DETECTION.SSD.OBJECTS_PER_IMAGE
                        Keep only these many objects after NMS
  --model.detection.ssd.nms-iou-threshold MODEL.DETECTION.SSD.NMS_IOU_THRESHOLD
                        NMS IoU threshold
  --model.detection.ssd.fpn-out-channels MODEL.DETECTION.SSD.FPN_OUT_CHANNELS
                        Number of output channels in FPN
  --model.detection.ssd.use-fpn
                        Use SSD with FPN

BaseLanguageModel:
  --model.language-modeling.name MODEL.LANGUAGE_MODELING.NAME
                        Name of the language model. Defaults to None (i.e.,
                        user need to specify the model name).
  --model.language-modeling.pretrained MODEL.LANGUAGE_MODELING.PRETRAINED
                        Path of the pre-trained model. Defaults to None (i.e.,
                        user needs to specify the path of pre-trained model).

GeneralGPTModel:
  --model.language-modeling.general-gpt.model-name {gpt-test,gpt-1_3B,OpenELM-270M,OpenELM-450M,OpenELM-1_1B,OpenELM-3B}
                        Name of the generative transformer-based LM model.
                        Defaults to None (i.e., user need to specify the model
                        name.).
  --model.language-modeling.general-gpt.max-context-length MODEL.LANGUAGE_MODELING.GENERAL_GPT.MAX_CONTEXT_LENGTH
                        Maximum context length. Defaults to None (i.e., user
                        needs to specify the maximum contenxt length value.).
  --model.language-modeling.general-gpt.vocab-size MODEL.LANGUAGE_MODELING.GENERAL_GPT.VOCAB_SIZE
                        Vocabulary size. Defaults to None (i.e., user needs to
                        specify the vocabulary size.).
  --model.language-modeling.general-gpt.padding-index MODEL.LANGUAGE_MODELING.GENERAL_GPT.PADDING_INDEX
                        Padding index. Defaults to None (i.e., no padding).

KVPredictionLLM:
  --model.language-modeling.kv-prediction.auxkv-num-layers-to-basekv-num-layers MODEL.LANGUAGE_MODELING.KV_PREDICTION.AUXKV_NUM_LAYERS_TO_BASEKV_NUM_LAYERS
                        The mapping from auxiliary layers to base model
                        layers. The element at index i is used to tell which
                        Auxiliary layer is used to predict the KV cache at
                        Base layer i.
  --model.language-modeling.kv-prediction.base-model MODEL.LANGUAGE_MODELING.KV_PREDICTION.BASE_MODEL
                        A config for the base model.
  --model.language-modeling.kv-prediction.auxiliary-model MODEL.LANGUAGE_MODELING.KV_PREDICTION.AUXILIARY_MODEL
                        A config for the auxiliary model.

BaseMultiModalImageText:
  --model.multi-modal-image-text.name MODEL.MULTI_MODAL_IMAGE_TEXT.NAME
                        Name of the multi-modal image-text model
  --model.multi-modal-image-text.lr-multiplier-img-encoder MODEL.MULTI_MODAL_IMAGE_TEXT.LR_MULTIPLIER_IMG_ENCODER
                        LR multiplier for the image encoder in
                        BaseMultiModalImageText
  --model.multi-modal-image-text.lr-multiplier-text-encoder MODEL.MULTI_MODAL_IMAGE_TEXT.LR_MULTIPLIER_TEXT_ENCODER
                        LR multiplier for the text encoder in
                        BaseMultiModalImageText
  --model.multi-modal-image-text.pretrained MODEL.MULTI_MODAL_IMAGE_TEXT.PRETRAINED
                        Path of the pretrained backbone
  --model.multi-modal-image-text.freeze-batch-norm
                        Freeze batch norm layers

CLIP:
  --model.multi-modal-image-text.clip.projection-dim MODEL.MULTI_MODAL_IMAGE_TEXT.CLIP.PROJECTION_DIM
                        Project image and text features to this dimensionality

BaseSegmentation:
  --model.segmentation.name MODEL.SEGMENTATION.NAME
                        Segmentation model name. Defaults to None.
  --model.segmentation.n-classes MODEL.SEGMENTATION.N_CLASSES
                        Number of classes in the dataset. Defaults to 21.
  --model.segmentation.pretrained MODEL.SEGMENTATION.PRETRAINED
                        Path of the pretrained segmentation model. Useful for
                        evaluation
  --model.segmentation.lr-multiplier MODEL.SEGMENTATION.LR_MULTIPLIER
                        Multiply the learning rate in segmentation network
                        (e.g., decoder) by this factor.Defaults to 1.0.
  --model.segmentation.classifier-dropout MODEL.SEGMENTATION.CLASSIFIER_DROPOUT
                        Dropout rate in classifier
  --model.segmentation.use-aux-head
                        Use auxiliary output
  --model.segmentation.aux-dropout MODEL.SEGMENTATION.AUX_DROPOUT
                        Dropout in auxiliary branch
  --model.segmentation.output-stride MODEL.SEGMENTATION.OUTPUT_STRIDE
                        Output stride in classification network
  --model.segmentation.replace-stride-with-dilation
                        Replace stride with dilation
  --model.segmentation.activation.name MODEL.SEGMENTATION.ACTIVATION.NAME
                        Non-linear function type
  --model.segmentation.activation.inplace
                        Inplace non-linear functions
  --model.segmentation.activation.neg-slope MODEL.SEGMENTATION.ACTIVATION.NEG_SLOPE
                        Negative slope in leaky relu
  --model.segmentation.freeze-batch-norm
                        Freeze batch norm layers
  --model.segmentation.use-level5-exp
                        Use output of Level 5 expansion layer in base feature
                        extractor
  --model.segmentation.finetune-pretrained-model
                        Finetune a pretrained segmentation model. Defaults to
                        False.
  --model.segmentation.n-pretrained-classes MODEL.SEGMENTATION.N_PRETRAINED_CLASSES
                        Number of classes in the pre-trained segmentation
                        model. Defaults to None.
  --model.segmentation.norm-layer MODEL.SEGMENTATION.NORM_LAYER
                        Normalization layer for segmentation. Defaults to
                        batch_norm.

Segmentation head arguments:
  Segmentation head arguments

  --model.segmentation.seg-head MODEL.SEGMENTATION.SEG_HEAD
                        Segmentation head

DeeplabV3:
  --model.segmentation.deeplabv3.aspp-rates MODEL.SEGMENTATION.DEEPLABV3.ASPP_RATES
                        Atrous rates to be used in the ASPP module in
                        DeeplabV3 segmentation head. Defaults to (6, 12, 18).
  --model.segmentation.deeplabv3.aspp-out-channels MODEL.SEGMENTATION.DEEPLABV3.ASPP_OUT_CHANNELS
                        Output channels of ASPP module in DeeplabV3
                        segmentation head. Defaults to 256.
  --model.segmentation.deeplabv3.aspp-sep-conv
                        Use separable convolution in the ASPP module in
                        DeeplabV3 segmentation head. Defaults to False.
  --model.segmentation.deeplabv3.aspp-dropout MODEL.SEGMENTATION.DEEPLABV3.ASPP_DROPOUT
                        Dropout value in the ASPP module in DeeplabV3
                        segmentation head. Defaults to 0.1.

MultiScaleDeeplabV3:
  --model.segmentation.deeplabv3.aspp-in-channels MODEL.SEGMENTATION.DEEPLABV3.ASPP_IN_CHANNELS
                        Input channels of the ASPP module. This is only used
                        in MultiScaleDeeplabV3. Defaults to 512.
  --model.segmentation.deeplabv3.output-upsample-factor MODEL.SEGMENTATION.DEEPLABV3.OUTPUT_UPSAMPLE_FACTOR
                        Output stide of the image encoder. This argument is
                        used on MultiScaleDeeplabV3. Default value is None.

PSPNet:
  --model.segmentation.pspnet.psp-pool-sizes MODEL.SEGMENTATION.PSPNET.PSP_POOL_SIZES [MODEL.SEGMENTATION.PSPNET.PSP_POOL_SIZES ...]
                        Pool sizes in the PSPNet module
  --model.segmentation.pspnet.psp-out-channels MODEL.SEGMENTATION.PSPNET.PSP_OUT_CHANNELS
                        Output channels of PSPNet module
  --model.segmentation.pspnet.psp-dropout MODEL.SEGMENTATION.PSPNET.PSP_DROPOUT
                        Dropout in the PSPNet module

BaseVideoEncoder:
  --model.video-classification.classifier-dropout MODEL.VIDEO_CLASSIFICATION.CLASSIFIER_DROPOUT
                        Dropout rate in classifier
  --model.video-classification.name MODEL.VIDEO_CLASSIFICATION.NAME
                        Model name
  --model.video-classification.n-classes MODEL.VIDEO_CLASSIFICATION.N_CLASSES
                        Number of classes in the dataset
  --model.video-classification.pretrained MODEL.VIDEO_CLASSIFICATION.PRETRAINED
                        Path of the pretrained backbone
  --model.video-classification.freeze-batch-norm
                        Freeze batch norm layers
  --model.video-classification.activation.name MODEL.VIDEO_CLASSIFICATION.ACTIVATION.NAME
                        Non-linear function type
  --model.video-classification.activation.inplace
                        Inplace non-linear functions
  --model.video-classification.activation.neg-slope MODEL.VIDEO_CLASSIFICATION.ACTIVATION.NEG_SLOPE
                        Negative slope in leaky relu
  --model.video-classification.clip-out-voting-fn {sum,max}
                        How to fuse the outputs of different clips in a video
  --model.video-classification.inference-mode
                        Inference mode

FullyShardedDataParallelWrapper:
  --fsdp.sharding-strategy {full_shard,no_shard,grad_op_shard}
                        Sharding strategy for FSDP. Defaults to None.
  --fsdp.backward-prefetching {pre,post}
                        Backward prefetching. Supported modes are `pre` and
                        `post`. `pre` and `post` prefetches the next set of
                        parameters before and after the current set of
                        parameter's gradient computation respectively.
                        Defaults to `pre`.
  --fsdp.parameter-datatype {float16,float32,bfloat16}
                        Specify the data type of model parameters. See FSDP
                        documentation for details. Defaults to `bfloat16`.
  --fsdp.gradient-reduction-datatype {float16,float32,bfloat16}
                        Specify the data type for gradient reduction. See FSDP
                        documentation for details. Defaults to `bfloat16`.
  --fsdp.buffer-datatype {float16,float32,bfloat16}
                        Specify the data type for buffers. See FSDP
                        documentation for details. Defaults to `bfloat16`.
  --fsdp.limit-all-gathers
                        Enabling this flag allows FSDP to explicitly
                        synchronize the CPU threads and prevent too many in-
                        flight all-gathers. Enabling this can help lower the
                        number of CUDA malloc retries. Defaults to `False`.
                        Note: In older PyTorch versions, this flag may not be
                        available.
  --fsdp.cpu-offload    Enable CPU offloading. Defaults to `False`. Note: In
                        older PyTorch versions, this flag may not be
                        available.

GlobalPool arguments:
  GlobalPool arguments

  --model.layer.global-pool MODEL.LAYER.GLOBAL_POOL
                        Which global pooling?

_BaseConvNormActLayer:
  --model.layer.conv-init MODEL.LAYER.CONV_INIT
                        Init type for conv layers

Non-linear functions:
  Non-linear functions

  --model.activation.name MODEL.ACTIVATION.NAME
                        Non-linear function name
  --model.activation.inplace
                        Use non-linear functions inplace
  --model.activation.neg-slope MODEL.ACTIVATION.NEG_SLOPE
                        Negative slope in leaky relu function

Normalization layers:
  Normalization layers

  --model.normalization.name MODEL.NORMALIZATION.NAME
                        Normalization layer. Defaults to 'batch_norm'.
  --model.normalization.groups MODEL.NORMALIZATION.GROUPS
                        Number of groups in group normalization layer.
                        Defaults to 1.
  --model.normalization.momentum MODEL.NORMALIZATION.MOMENTUM
                        Momentum in normalization layers. Defaults to 0.1

EMA:
  Exponential moving average arguments

  --ema.enable          Use exponential moving average
  --ema.momentum EMA.MOMENTUM
                        EMA momentum. Defaults to 0.0001.

Anchor generator:
  Anchor generator

  --anchor-generator.name ANCHOR_GENERATOR.NAME
                        Name of the anchor generator

SSDAnchorGenerator:
  --anchor-generator.ssd.output-strides ANCHOR_GENERATOR.SSD.OUTPUT_STRIDES [ANCHOR_GENERATOR.SSD.OUTPUT_STRIDES ...]
                        Output strides of the feature maps for which we want
                        to generate anchors
  --anchor-generator.ssd.aspect-ratios ANCHOR_GENERATOR.SSD.ASPECT_RATIOS [ANCHOR_GENERATOR.SSD.ASPECT_RATIOS ...]
                        Aspect ratios at each output stride
  --anchor-generator.ssd.min-scale-ratio ANCHOR_GENERATOR.SSD.MIN_SCALE_RATIO
                        Min. scale ratio
  --anchor-generator.ssd.max-scale-ratio ANCHOR_GENERATOR.SSD.MAX_SCALE_RATIO
                        Max. scale ratio
  --anchor-generator.ssd.no-clipping
                        Don't clip the anchors
  --anchor-generator.ssd.step ANCHOR_GENERATOR.SSD.STEP [ANCHOR_GENERATOR.SSD.STEP ...]
                        Step between pixels

Matcher:
  Matcher

  --matcher.name MATCHER.NAME
                        Name of the matcher. Matcher matches anchors with GT
                        box coordinates

SSDMatcher:
  --matcher.ssd.center-variance MATCHER.SSD.CENTER_VARIANCE
                        Center variance for matching
  --matcher.ssd.size-variance MATCHER.SSD.SIZE_VARIANCE
                        Size variance.
  --matcher.ssd.iou-threshold MATCHER.SSD.IOU_THRESHOLD
                        IOU Threshold.

BaseNeuralAugmentor:
  --model.learn-augmentation.mode {basic,distribution}
                        Neural augmentation mode
  --model.learn-augmentation.brightness
                        Learn parameters for brightness
  --model.learn-augmentation.contrast
                        Learn parameters for contrast
  --model.learn-augmentation.noise
                        Learn parameters for noise
  --model.learn-augmentation.lr-multiplier MODEL.LEARN_AUGMENTATION.LR_MULTIPLIER
                        LR multiplier for neural aug parameters

BatchSampler:
  --sampler.bs.crop-size-width SAMPLER.BS.CROP_SIZE_WIDTH
                        Base crop size (along width) during training
  --sampler.bs.crop-size-height SAMPLER.BS.CROP_SIZE_HEIGHT
                        Base crop size (along height) during training

ChainSampler:
  --sampler.chain-sampler SAMPLER.CHAIN_SAMPLER
  --sampler.chain-sampler-mode {sequential,interleave}
                        Chain sampler mode. Defaults to sequential.

MultiScaleSampler:
  --sampler.msc.crop-size-width SAMPLER.MSC.CROP_SIZE_WIDTH
                        Base crop size (along width) during training. Defaults
                        to 256.
  --sampler.msc.crop-size-height SAMPLER.MSC.CROP_SIZE_HEIGHT
                        Base crop size (along height) during training.
                        Defaults to 256.
  --sampler.msc.min-crop-size-width SAMPLER.MSC.MIN_CROP_SIZE_WIDTH
                        Min. crop size along width during training. Defaults
                        to 160.
  --sampler.msc.max-crop-size-width SAMPLER.MSC.MAX_CROP_SIZE_WIDTH
                        Max. crop size along width during training. Defaults
                        to 320.
  --sampler.msc.min-crop-size-height SAMPLER.MSC.MIN_CROP_SIZE_HEIGHT
                        Min. crop size along height during training. Defaults
                        to 160.
  --sampler.msc.max-crop-size-height SAMPLER.MSC.MAX_CROP_SIZE_HEIGHT
                        Max. crop size along height during training. Defaults
                        to 320.
  --sampler.msc.max-n-scales SAMPLER.MSC.MAX_N_SCALES
                        Max. scales in variable batch sampler. Defaults to 5.
  --sampler.msc.check-scale SAMPLER.MSC.CHECK_SCALE
                        Image scales should be divisible by this factor.
                        Defaults to 32.
  --sampler.msc.scale-inc
                        Increase image scales during training. Defaults to
                        False.

VariableBatchSampler:
  --sampler.vbs.crop-size-width SAMPLER.VBS.CROP_SIZE_WIDTH
                        Base crop size (along width) during training. Defaults
                        to 256.
  --sampler.vbs.crop-size-height SAMPLER.VBS.CROP_SIZE_HEIGHT
                        Base crop size (along height) during training.
                        Defaults to 256.
  --sampler.vbs.min-crop-size-width SAMPLER.VBS.MIN_CROP_SIZE_WIDTH
                        Min. crop size along width during training. Defaults
                        to 160.
  --sampler.vbs.max-crop-size-width SAMPLER.VBS.MAX_CROP_SIZE_WIDTH
                        Max. crop size along width during training. Defaults
                        to 320.
  --sampler.vbs.min-crop-size-height SAMPLER.VBS.MIN_CROP_SIZE_HEIGHT
                        Min. crop size along height during training. Defaults
                        to 160.
  --sampler.vbs.max-crop-size-height SAMPLER.VBS.MAX_CROP_SIZE_HEIGHT
                        Max. crop size along height during training. Defaults
                        to 320.
  --sampler.vbs.max-n-scales SAMPLER.VBS.MAX_N_SCALES
                        Max. scales in variable batch sampler. Defaults to 5.
  --sampler.vbs.check-scale SAMPLER.VBS.CHECK_SCALE
                        Image scales should be divisible by this factor.
                        Defaults to 32.
  --sampler.vbs.ep-intervals SAMPLER.VBS.EP_INTERVALS
                        Epoch intervals at which scales should be adjusted.
                        Defaults to 40.
  --sampler.vbs.min-scale-inc-factor SAMPLER.VBS.MIN_SCALE_INC_FACTOR
                        Factor by which we should increase the minimum scale.
                        Defaults to 1.0
  --sampler.vbs.max-scale-inc-factor SAMPLER.VBS.MAX_SCALE_INC_FACTOR
                        Factor by which we should increase the maximum scale.
                        Defaults to 1.0
  --sampler.vbs.scale-inc
                        Increase image scales during training. Defaults to
                        False.

VideoBatchSampler:
  --sampler.bs.num-frames-per-clip SAMPLER.BS.NUM_FRAMES_PER_CLIP
                        Number of frames per video clip. Defaults to 8.
  --sampler.bs.clips-per-video SAMPLER.BS.CLIPS_PER_VIDEO
                        Number of clips per video. Defaults to 1.

Batch sampler for videos:
  Arguments related to variable batch sampler

  --sampler.vcbs.num-frames-per-clip SAMPLER.VCBS.NUM_FRAMES_PER_CLIP
                        Number of frames per video clip. Default to 8.
  --sampler.vcbs.video-fps SAMPLER.VCBS.VIDEO_FPS
                        The desired frame rate of the clip. Default to 8.
  --sampler.vcbs.audio-fps SAMPLER.VCBS.AUDIO_FPS
                        The frame rate of audio. Default to 16000.
  --sampler.vcbs.min-clip-fps-scale SAMPLER.VCBS.MIN_CLIP_FPS_SCALE
                        The minimal scale to apply to the desired video/audio
                        frame rate of the clip. Default to 1.
  --sampler.vcbs.max-clip-fps-scale SAMPLER.VCBS.MAX_CLIP_FPS_SCALE
                        The maximal scale to apply to desired video/audio
                        frame rate of the clip. Default to 2.5.
  --sampler.vcbs.video-fps-num-scales SAMPLER.VCBS.VIDEO_FPS_NUM_SCALES
                        The maximal scale to apply to desired frame rate of
                        the clip. Default to 5.
  --sampler.vcbs.num-clips-per-second-train SAMPLER.VCBS.NUM_CLIPS_PER_SECOND_TRAIN
                        The number of clips per second for training, default
                        to 1. This is used to determine the frequency to
                        sample.
  --sampler.vcbs.num-clips-per-second-val SAMPLER.VCBS.NUM_CLIPS_PER_SECOND_VAL
                        The number of clips per second for validation, default
                        to 4. This isused to determine the frequency to
                        sample.
  --sampler.vcbs.max-num-clips-per-batch SAMPLER.VCBS.MAX_NUM_CLIPS_PER_BATCH
                        The maximal number of clips per batch, default to 50.
                        This is used to avoid memory leak if videos are too
                        long.
  --sampler.vcbs.num-samples-per-clip SAMPLER.VCBS.NUM_SAMPLES_PER_CLIP
                        The number of samples to generate for each clip at
                        training time. Default to 1.

VideoVariableSeqSampler:
  --sampler.vbs.num-frames-per-clip SAMPLER.VBS.NUM_FRAMES_PER_CLIP
                        Default frames per video. Defaults to 8
  --sampler.vbs.random-video-clips
                        Sample number of clips per video randomly during
                        training between min and max values specified using
                        --sampler.vbs.min-clips-per-video and
                        --sampler.vbs.max-clips-per-video arguments
                        respectively
  --sampler.vbs.min-clips-per-video SAMPLER.VBS.MIN_CLIPS_PER_VIDEO
                        Minimum number of clips per video. Used only for
                        training. Defaults to 1.
  --sampler.vbs.max-clips-per-video SAMPLER.VBS.MAX_CLIPS_PER_VIDEO
                        Maximum number of clips per video. Used only for
                        training. Defaults to 5.
  --sampler.vbs.clips-per-video SAMPLER.VBS.CLIPS_PER_VIDEO
                        Number of clips per video. Defaults to 1.
  --sampler.vbs.min-frames-per-clip SAMPLER.VBS.MIN_FRAMES_PER_CLIP
                        Minimum number of frames per clip. Defaults to 8.

BaseSampler:
  --sampler.name SAMPLER.NAME
                        Name of the sampler. Defaults to None (i.e., user
                        needs to specify the sampler if using MAP-style
                        datasets).Note that this argument is not applicable to
                        iterable datasets.
  --sampler.num-repeats SAMPLER.NUM_REPEATS
                        Repeat the training dataset samples by this factor in
                        each epoch (aka repeated augmentation). This
                        effectively increases samples per epoch. As an
                        example, if dataset has 10000 samples and
                        sampler.num_repeats is set to 2, then total samples in
                        each epoch would be 20000. Defaults to 1.
  --sampler.truncated-repeat-aug-sampler
                        When enabled, it restricts the sampler to load a
                        subset of the training dataset such thatnumber of
                        samples obtained after repetition are the same as the
                        original dataset.As an example, if dataset has 10000
                        samples, sampler.num_repeats is set to 2, and
                        sampler.truncated_repeat_aug_sampler is enabled, then
                        the sampler would sample 10000 samples in each epoch.
                        Defaults to False.
  --sampler.start-shuffling-from-epoch SAMPLER.START_SHUFFLING_FROM_EPOCH
                        Shuffle data indices during training from this epoch
                        onwards. Defaults to 0 (i.e., shuffle from the first
                        epoch).

BaseSamplerDDP:
  --sampler.use-shards  Use data sharding. Only applicable to DDP. Defaults to
                        False.
  --sampler.disable-shuffle-sharding
                        Disable shuffling while sharding for extremely large
                        datasets. Defaults to False.

Collate function arguments:
  --dataset.collate-fn-name-train DATASET.COLLATE_FN_NAME_TRAIN
                        Name of collate function for training. Defaults to
                        pytorch_default_collate_fn.
  --dataset.collate-fn-name-val DATASET.COLLATE_FN_NAME_VAL
                        Name of collate function for validation. Defaults to
                        pytorch_default_collate_fn.
  --dataset.collate-fn-name-test DATASET.COLLATE_FN_NAME_TEST
                        Name of collate function used for evaluation. Default
                        is pytorch_default_collate_fn.

FixedSizeCrop:
  --image-augmentation.fixed-size-crop.enable
                        use FixedSizeCrop. This flag is useful when you want
                        to study the effect of different transforms.
  --image-augmentation.fixed-size-crop.size IMAGE_AUGMENTATION.FIXED_SIZE_CROP.SIZE [IMAGE_AUGMENTATION.FIXED_SIZE_CROP.SIZE ...]
                        Image size either as an int or (int, int).
  --image-augmentation.fixed-size-crop.fill IMAGE_AUGMENTATION.FIXED_SIZE_CROP.FILL
                        Fill value to be used during padding operation.
                        Defaults to 0.
  --image-augmentation.fixed-size-crop.padding-mode IMAGE_AUGMENTATION.FIXED_SIZE_CROP.PADDING_MODE
                        Padding modes. Defaults to constant

ScaleJitter:
  --image-augmentation.scale-jitter.enable
                        use ScaleJitter. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.scale-jitter.interpolation IMAGE_AUGMENTATION.SCALE_JITTER.INTERPOLATION
                        Interpolation method. Defaults to bilinear
                        interpolation
  --image-augmentation.scale-jitter.target-size IMAGE_AUGMENTATION.SCALE_JITTER.TARGET_SIZE [IMAGE_AUGMENTATION.SCALE_JITTER.TARGET_SIZE ...]
                        Target image size either as an int or (int, int).
  --image-augmentation.scale-jitter.scale-range IMAGE_AUGMENTATION.SCALE_JITTER.SCALE_RANGE [IMAGE_AUGMENTATION.SCALE_JITTER.SCALE_RANGE ...]
                        Scale range as (float, float).

RandomResizedCrop:
  --image-augmentation.random-resized-crop.enable
                        use RandomResizedCrop. This flag is useful when you
                        want to study the effect of different transforms.
  --image-augmentation.random-resized-crop.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Interpolation method for resizing. Defaults to
                        bilinear.
  --image-augmentation.random-resized-crop.scale IMAGE_AUGMENTATION.RANDOM_RESIZED_CROP.SCALE
                        Specifies the lower and upper bounds for the random
                        area of the crop, before resizing. The scale is
                        defined with respect to the area of the original
                        image. Defaults to (0.08, 1.0)
  --image-augmentation.random-resized-crop.aspect-ratio IMAGE_AUGMENTATION.RANDOM_RESIZED_CROP.ASPECT_RATIO
                        lower and upper bounds for the random aspect ratio of
                        the crop, before resizing. Defaults to (3./4., 4./3.)

AutoAugment:
  --image-augmentation.auto-augment.enable
                        use AutoAugment. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.auto-augment.policy IMAGE_AUGMENTATION.AUTO_AUGMENT.POLICY
                        Auto-augment policy name. Defaults to imagenet.
  --image-augmentation.auto-augment.interpolation IMAGE_AUGMENTATION.AUTO_AUGMENT.INTERPOLATION
                        Auto-augment interpolation method. Defaults to
                        bilinear interpolation

RandAugment:
  --image-augmentation.rand-augment.enable
                        Use RandAugment. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.rand-augment.num-ops IMAGE_AUGMENTATION.RAND_AUGMENT.NUM_OPS
                        Number of augmentation transformations to apply
                        sequentially. Defaults to 2.
  --image-augmentation.rand-augment.magnitude IMAGE_AUGMENTATION.RAND_AUGMENT.MAGNITUDE
                        Magnitude for all the transformations. Defaults to 9
  --image-augmentation.rand-augment.num-magnitude-bins IMAGE_AUGMENTATION.RAND_AUGMENT.NUM_MAGNITUDE_BINS
                        The number of different magnitude values. Defaults to
                        31.
  --image-augmentation.rand-augment.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method. Defaults to bilinear

TrivialAugmentWide:
  --image-augmentation.trivial-augment-wide.enable
                        Use TrivialAugmentWide. This flag is useful when you
                        want to study the effect of different transforms.
  --image-augmentation.trivial-augment-wide.num-magnitude-bins IMAGE_AUGMENTATION.TRIVIAL_AUGMENT_WIDE.NUM_MAGNITUDE_BINS
                        The number of different magnitude values. Defaults to
                        31.
  --image-augmentation.trivial-augment-wide.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method. Defaults to bilinear

RandomHorizontalFlip:
  --image-augmentation.random-horizontal-flip.enable
                        use RandomHorizontalFlip. This flag is useful when you
                        want to study the effect of different transforms.
  --image-augmentation.random-horizontal-flip.p IMAGE_AUGMENTATION.RANDOM_HORIZONTAL_FLIP.P
                        Probability for applying random horizontal flip

RandomRotate:
  --image-augmentation.random-rotate.enable
                        use RandomRotate. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.random-rotate.angle IMAGE_AUGMENTATION.RANDOM_ROTATE.ANGLE
                        Angle for rotation. Defaults to 10. The angle is
                        sampled uniformly from [-angle, angle]
  --image-augmentation.random-rotate.mask-fill IMAGE_AUGMENTATION.RANDOM_ROTATE.MASK_FILL
                        Fill value for the segmentation mask. Defaults to 0.

Resize:
  --image-augmentation.resize.enable
                        use Resize. This flag is useful when you want to study
                        the effect of different transforms.
  --image-augmentation.resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method for resizing. Defaults to
                        bilinear
  --image-augmentation.resize.size IMAGE_AUGMENTATION.RESIZE.SIZE [IMAGE_AUGMENTATION.RESIZE.SIZE ...]
                        Resize image to the specified size. If int is passed,
                        then shorter side is resizedto the specified size and
                        longest side is resized while maintaining aspect
                        ratio.Defaults to None.

CenterCrop:
  --image-augmentation.center-crop.enable
                        use CenterCrop. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.center-crop.size IMAGE_AUGMENTATION.CENTER_CROP.SIZE [IMAGE_AUGMENTATION.CENTER_CROP.SIZE ...]
                        Center crop size. Defaults to None.

SSDCroping:
  --image-augmentation.ssd-crop.enable
                        use SSDCroping. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.ssd-crop.iou-thresholds IMAGE_AUGMENTATION.SSD_CROP.IOU_THRESHOLDS [IMAGE_AUGMENTATION.SSD_CROP.IOU_THRESHOLDS ...]
                        IoU thresholds for SSD cropping. Defaults to [0.0,
                        0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
  --image-augmentation.ssd-crop.n-trials IMAGE_AUGMENTATION.SSD_CROP.N_TRIALS
                        Number of trials for SSD cropping. Defaults to 40
  --image-augmentation.ssd-crop.min-aspect-ratio IMAGE_AUGMENTATION.SSD_CROP.MIN_ASPECT_RATIO
                        Min. aspect ratio in SSD Cropping. Defaults to 0.5
  --image-augmentation.ssd-crop.max-aspect-ratio IMAGE_AUGMENTATION.SSD_CROP.MAX_ASPECT_RATIO
                        Max. aspect ratio in SSD Cropping. Defaults to 2.0

PhotometricDistort:
  --image-augmentation.photo-metric-distort.enable
                        use PhotometricDistort. This flag is useful when you
                        want to study the effect of different transforms.
  --image-augmentation.photo-metric-distort.alpha-min IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.ALPHA_MIN
                        Min. alpha value for contrast. Should be > 0. Defaults
                        to 0.5
  --image-augmentation.photo-metric-distort.alpha-max IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.ALPHA_MAX
                        Max. alpha value for contrast. Should be > 0. Defaults
                        to 1.5
  --image-augmentation.photo-metric-distort.beta-min IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.BETA_MIN
                        Min. beta value for brightness. Should be > 0.
                        Defaults to 0.8
  --image-augmentation.photo-metric-distort.beta-max IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.BETA_MAX
                        Max. beta value for brightness. Should be > 0.
                        Defaults to 1.2
  --image-augmentation.photo-metric-distort.gamma-min IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.GAMMA_MIN
                        Min. gamma value for saturation. Should be > 0.
                        Defaults to 0.5
  --image-augmentation.photo-metric-distort.gamma-max IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.GAMMA_MAX
                        Max. gamma value for saturation. Should be > 0.
                        Defaults to 1.5
  --image-augmentation.photo-metric-distort.delta-min IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.DELTA_MIN
                        Min. delta value for Hue. Should be between -1 and 1.
                        Defaults to -0.05
  --image-augmentation.photo-metric-distort.delta-max IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.DELTA_MAX
                        Max. delta value for Hue. Should be between -1 and 1.
                        Defaults to 0.05
  --image-augmentation.photo-metric-distort.p IMAGE_AUGMENTATION.PHOTO_METRIC_DISTORT.P
                        Probability for applying a distortion. Defaults to 0.5

RandomResize:
  --image-augmentation.random-resize.enable
                        use RandomResize. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.random-resize.max-scale-long-edge IMAGE_AUGMENTATION.RANDOM_RESIZE.MAX_SCALE_LONG_EDGE
                        Max. value along the longest edge. Defaults to None
  --image-augmentation.random-resize.max-scale-short-edge IMAGE_AUGMENTATION.RANDOM_RESIZE.MAX_SCALE_SHORT_EDGE
                        Max. value along the shortest edge. Defaults to None.
  --image-augmentation.random-resize.min-ratio IMAGE_AUGMENTATION.RANDOM_RESIZE.MIN_RATIO
                        Min ratio for random resizing. Defaults to 0.5
  --image-augmentation.random-resize.max-ratio IMAGE_AUGMENTATION.RANDOM_RESIZE.MAX_RATIO
                        Max ratio for random resizing. Defaults to 2.0
  --image-augmentation.random-resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method. Defaults to bilinear.

RandomShortSizeResize:
  --image-augmentation.random-short-size-resize.enable
                        use RandomShortSizeResize. This flag is useful when
                        you want to study the effect of different transforms.
  --image-augmentation.random-short-size-resize.short-side-min IMAGE_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.SHORT_SIDE_MIN
                        Minimum value for image's shortest side. Defaults to
                        None.
  --image-augmentation.random-short-size-resize.short-side-max IMAGE_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.SHORT_SIDE_MAX
                        Maximum value for image's shortest side. Defaults to
                        None.
  --image-augmentation.random-short-size-resize.interpolation {nearest,bilinear,bicubic,cubic,box,hamming,lanczos}
                        Desired interpolation method. Defaults to bicubic
  --image-augmentation.random-short-size-resize.max-img-dim IMAGE_AUGMENTATION.RANDOM_SHORT_SIZE_RESIZE.MAX_IMG_DIM
                        Max. image dimension. Defaults to None.

RandomErasing:
  --image-augmentation.random-erase.enable
                        use RandomErasing. This flag is useful when you want
                        to study the effect of different transforms.
  --image-augmentation.random-erase.p IMAGE_AUGMENTATION.RANDOM_ERASE.P
                        Probability that random erasing operation will be
                        applied. Defaults to 0.5

RandomGaussianBlur:
  --image-augmentation.random-gaussian-noise.enable
                        use RandomGaussianBlur. This flag is useful when you
                        want to study the effect of different transforms.
  --image-augmentation.random-gaussian-noise.p IMAGE_AUGMENTATION.RANDOM_GAUSSIAN_NOISE.P
                        Probability for applying RandomGaussianBlur

RandomCrop:
  --image-augmentation.random-crop.enable
                        use RandomCrop. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.random-crop.seg-class-max-ratio IMAGE_AUGMENTATION.RANDOM_CROP.SEG_CLASS_MAX_RATIO
                        Max. ratio that single segmentation class can occupy.
                        Defaults to None
  --image-augmentation.random-crop.pad-if-needed
                        Pad images if needed. Defaults to False, i.e.,
                        resizing will be performed
  --image-augmentation.random-crop.mask-fill IMAGE_AUGMENTATION.RANDOM_CROP.MASK_FILL
                        Value to fill in segmentation mask in case of padding.
                        Defaults to 255. Generally, this value is the same as
                        background or undefined class id.

RandomOrder:
  --image-augmentation.random-order.enable
                        use RandomOrder. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.random-order.apply-k IMAGE_AUGMENTATION.RANDOM_ORDER.APPLY_K
                        Apply K percent of transforms randomly. Value between
                        0 and 1. Defaults to 1 (i.e., apply all transforms in
                        random order).

RandAugmentTimm:
  --image-augmentation.rand-augment.use-timm-library
                        Use timm library for randaugment over PyTorch's
                        implementation
  --image-augmentation.rand-augment.timm-config-str IMAGE_AUGMENTATION.RAND_AUGMENT.TIMM_CONFIG_STR
                        Number of augmentation transformations to apply
                        sequentially. Defaults to 2.

RandomMixup:
  --image-augmentation.mixup.enable
                        use RandomMixup. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.mixup.alpha IMAGE_AUGMENTATION.MIXUP.ALPHA
                        Alpha for MixUp augmentation. Defaults to 0.2
  --image-augmentation.mixup.p IMAGE_AUGMENTATION.MIXUP.P
                        Probability for applying MixUp augmentation. Defaults
                        to 1.0 . If both MixUp and CutMix are enabled, one is
                        used with probability 0.5 per batch.
  --image-augmentation.mixup.inplace
                        Apply MixUp augmentation inplace. Defaults to False.
  --image-augmentation.mixup.sample-key IMAGE_AUGMENTATION.MIXUP.SAMPLE_KEY
                        Name of the key if input is a dictionart. Defaults to
                        None.
  --image-augmentation.mixup.target-key IMAGE_AUGMENTATION.MIXUP.TARGET_KEY
                        Name of the key if target is a dictionary. Defaults to
                        None.

RandomCutmix:
  --image-augmentation.cutmix.enable
                        use RandomCutmix. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.cutmix.alpha IMAGE_AUGMENTATION.CUTMIX.ALPHA
                        Alpha for cutmix augmentation. Defaults to 1.0
  --image-augmentation.cutmix.p IMAGE_AUGMENTATION.CUTMIX.P
                        Probability for applying cutmix augmentation. Defaults
                        to 1.0 If both MixUp and CutMix are enabled, one is
                        used with probability 0.5 per batch.
  --image-augmentation.cutmix.inplace
                        Apply cutmix operation inplace. Defaults to False
  --image-augmentation.cutmix.sample-key IMAGE_AUGMENTATION.CUTMIX.SAMPLE_KEY
                        Name of the key if input is a dictionary. Defaults to
                        None.
  --image-augmentation.cutmix.target-key IMAGE_AUGMENTATION.CUTMIX.TARGET_KEY
                        Name of the key if target is a dictionary. Defaults to
                        None.

Gain:
  --audio-augmentation.gain.enable
                        Use Gain. This flag is useful when you want to study
                        the effect of different transforms. Defaults to False.
  --audio-augmentation.gain.levels AUDIO_AUGMENTATION.GAIN.LEVELS [AUDIO_AUGMENTATION.GAIN.LEVELS ...]
                        Gain levels to use for augmentation (in dB). Defaults
                        to [0] (no gain).
  --audio-augmentation.gain.share-clip-params
                        Pick the same gain levels for each clip in the batch
                        if set True.

Noise:
  --audio-augmentation.noise.enable
                        Use Noise. This flag is useful when you want to study
                        the effect of different transforms. Defaults to False.
  --audio-augmentation.noise.levels AUDIO_AUGMENTATION.NOISE.LEVELS [AUDIO_AUGMENTATION.NOISE.LEVELS ...]
                        Gain levels to use for noise augmentation (in dB).
                        Defaults to [-100], means almost no augmentation
                        (10^-5 noise signal).
  --audio-augmentation.noise.cache-size AUDIO_AUGMENTATION.NOISE.CACHE_SIZE
                        Number of augmentation noises to cache. Defaults to
                        10.
  --audio-augmentation.noise.files-dir AUDIO_AUGMENTATION.NOISE.FILES_DIR
                        Directory path that stores the noise files to be
                        added. Defaults to None.
  --audio-augmentation.noise.refresh-freq AUDIO_AUGMENTATION.NOISE.REFRESH_FREQ
                        Frequency to refresh noise files (default 0 means
                        never refresh).

SetFixedLength:
  --audio-augmentation.set-fixed-length.enable
                        Use SetFixedLength. This flag is useful when you want
                        to study the effect of different transforms. Defaults
                        to False.
  --audio-augmentation.set-fixed-length.length AUDIO_AUGMENTATION.SET_FIXED_LENGTH.LENGTH
                        Length to which to trim or pad the audio buffer.
                        Defaults to 16000.

Roll:
  --audio-augmentation.roll.enable
                        Use Roll. This flag is useful when you want to study
                        the effect of different transforms. Defaults to False.
  --audio-augmentation.roll.window AUDIO_AUGMENTATION.ROLL.WINDOW
                        Maximum fraction of the audio buffer to move. Defaults
                        to 0.1.

MFCCs:
  --audio-augmentation.mfccs.num-mfccs AUDIO_AUGMENTATION.MFCCS.NUM_MFCCS
                        Number of MFCC features. Defaults to 20.
  --audio-augmentation.mfccs.window-length AUDIO_AUGMENTATION.MFCCS.WINDOW_LENGTH
                        Window length (unit: seconds) for MFCC calculation.
                        Defaults to 0.023.
  --audio-augmentation.mfccs.num-frames AUDIO_AUGMENTATION.MFCCS.NUM_FRAMES
                        Number of sub-time-slice temporal components. This
                        argument is used for splitting the temporal dimension
                        of the spectrogram into frames. Defaults to 8.

AudioResample:
  --audio-augmentation.audio-resample.enable
                        Use AudioResample. This flag is useful when you want
                        to study the effect of different transforms. Defaults
                        to False.
  --audio-augmentation.audio-resample.audio-fps AUDIO_AUGMENTATION.AUDIO_RESAMPLE.AUDIO_FPS
                        Frames per second in the incoming audio stream.
                        Defaults to 16000.

StandardizeChannels:
  --audio-augmentation.standardize-channels.num-channels AUDIO_AUGMENTATION.STANDARDIZE_CHANNELS.NUM_CHANNELS
                        Number of output audio channels. Defaults to 2.
  --audio-augmentation.standardize-channels.enable
                        Use StandardizeChannels. This flag is useful when you
                        want to study the effect of different transforms.
                        Defaults to False.

GaussianAudioNoise:
  --audio-augmentation.gaussian-noise.enable
                        Use GaussianAudioNoise. This flag is useful when you
                        want to study the effect of different transforms.
                        Defaults to False.
  --audio-augmentation.gaussian-noise.audio-noise-scale-range AUDIO_AUGMENTATION.GAUSSIAN_NOISE.AUDIO_NOISE_SCALE_RANGE AUDIO_AUGMENTATION.GAUSSIAN_NOISE.AUDIO_NOISE_SCALE_RANGE
                        The standard deviation of the noise will be between
                        the high and low end of this scale. Defaults to
                        (0.000, 0.005)

TorchaudioSave:
  --audio-augmentation.torchaudio-save.enable
                        Use TorchaudioSave. This flag is useful when you want
                        to study the effect of different transforms.
  --audio-augmentation.torchaudio-save.encoding-dtype {float32,int32,int16,uint8}
                        The data type used in the audio encoding. Defaults to
                        float32.
  --audio-augmentation.torchaudio-save.format {wav,mp3}
                        The format in which to save the audio. Defaults to
                        wav.
  --audio-augmentation.torchaudio-save.backend {ffmpeg,sox,soundfile}
                        The I/O backend to use for save the audio. Defaults to
                        sox, which was the default backend in the earlier
                        torchaudio versions.

PILSave:
  --image-augmentation.pil-save.enable
                        Use PILSave. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.pil-save.file-encoding {fCHW,fHWC,TIFF,PNG,JPEG}
                        The type of file encoding to use. Defaults to TIFF.
  --image-augmentation.pil-save.quality IMAGE_AUGMENTATION.PIL_SAVE.QUALITY
                        JPEG quality if using JPEG encoding. Defaults to 100.

ShuffleBytes:
  --image-augmentation.shuffle-bytes.enable
                        Use ShuffleBytes. This flag is useful when you want to
                        study the effect of different transforms.
  --image-augmentation.shuffle-bytes.mode {reverse,random_shuffle,cyclic_half_length,stride,window_shuffle}
                        The mode to use when shuffling bytes. Defaults to
                        'reverse'.
  --image-augmentation.shuffle-bytes.stride IMAGE_AUGMENTATION.SHUFFLE_BYTES.STRIDE
                        The stride of the window used in shuffling operations
                        that are windowed. Defaults to 1024.
  --image-augmentation.shuffle-bytes.window-size IMAGE_AUGMENTATION.SHUFFLE_BYTES.WINDOW_SIZE
                        The size of the window used in shuffling operations
                        that are windowed. Defaults to 1024.

MaskPositions:
  --image-augmentation.mask-positions.enable
                        Use MaskPositions. This flag is useful when you want
                        to study the effect of different transforms.
  --image-augmentation.mask-positions.keep-frac IMAGE_AUGMENTATION.MASK_POSITIONS.KEEP_FRAC
                        The fraction of bytes to keep. Defaults to 0.5.

BytePermutation:
  --image-augmentation.byte-permutation.enable
                        Use BytePermutation. This flag is useful when you want
                        to study the effect of different transforms.

RandomUniformNoise:
  --image-augmentation.random-uniform.enable
                        Use RandomUniformNoise. This flag is useful when you
                        want to study the effect of different transforms.
  --image-augmentation.random-uniform.width-range IMAGE_AUGMENTATION.RANDOM_UNIFORM.WIDTH_RANGE IMAGE_AUGMENTATION.RANDOM_UNIFORM.WIDTH_RANGE
                        The range of values from which to add noise. It is
                        specified as [low, high] (inclusive). Defaults to [-5,
                        5].

SaveInputs:
  --video-augmentation.save-inputs.save-dir VIDEO_AUGMENTATION.SAVE_INPUTS.SAVE_DIR
                        Path to the folder for saving output debugging videos.
                        Defaults to None.
  --video-augmentation.save-inputs.add-labels
                        If set, write the class label on each frame of the
                        video. Defaults to False.
  --video-augmentation.save-inputs.enable
                        Use SaveInputs. This flag is useful when you want to
                        study the effect of different transforms. Defaults to
                        False.
  --video-augmentation.save-inputs.symlink-to-original
                        If True, a symlink to original video sample will be
                        created besidesthe saved inputs for easier debugging.
                        Defaults to False.

RandomResizedCrop:
  --video-augmentation.random-resized-crop.enable
                        Use RandomResizedCrop. This flag is useful when you
                        want to study the effect of different transforms.
                        Defaults to False.
  --video-augmentation.random-resized-crop.interpolation {nearest,bilinear,bicubic}
                        Desired interpolation method. Defaults to bilinear
  --video-augmentation.random-resized-crop.scale VIDEO_AUGMENTATION.RANDOM_RESIZED_CROP.SCALE
                        Specifies the lower and upper bounds for the random
                        area of the crop, before resizing. The scale is
                        defined with respect to the area of the original
                        image. Defaults to (0.08, 1.0).
  --video-augmentation.random-resized-crop.aspect-ratio VIDEO_AUGMENTATION.RANDOM_RESIZED_CROP.ASPECT_RATIO
                        lower and upper bounds for the random aspect ratio of
                        the crop, before resizing. Defaults to (3./4., 4./3.).

RandomShortSizeResizeCrop:
  --video-augmentation.random-short-side-resize-crop.enable
                        Use RandomShortSizeResizeCrop. This flag is useful
                        when you want to study the effect of different
                        transforms. Defaults to False.
  --video-augmentation.random-short-side-resize-crop.interpolation {nearest,bilinear,bicubic}
                        Desired interpolation method. Defaults to bilinear
  --video-augmentation.random-short-side-resize-crop.short-side-min VIDEO_AUGMENTATION.RANDOM_SHORT_SIDE_RESIZE_CROP.SHORT_SIDE_MIN
                        Minimum value for video's shortest side. Defaults to
                        None.
  --video-augmentation.random-short-side-resize-crop.short-side-max VIDEO_AUGMENTATION.RANDOM_SHORT_SIDE_RESIZE_CROP.SHORT_SIDE_MAX
                        Maximum value for video's shortest side. Defaults to
                        None.

RandomCrop:
  --video-augmentation.random-crop.enable
                        Use RandomCrop. This flag is useful when you want to
                        study the effect of different transforms. Defaults to
                        False.

RandomHorizontalFlip:
  --video-augmentation.random-horizontal-flip.enable
                        Use RandomHorizontalFlip. This flag is useful when you
                        want to study the effect of different transforms.
                        Defaults to False.
  --video-augmentation.random-horizontal-flip.p VIDEO_AUGMENTATION.RANDOM_HORIZONTAL_FLIP.P
                        Probability for random horizontal flip. Defaults to
                        0.5.

CenterCrop:
  --video-augmentation.center-crop.enable
                        Use CenterCrop. This flag is useful when you want to
                        study the effect of different transforms. Defaults to
                        False.

Resize:
  --video-augmentation.resize.enable
                        Use Resize. This flag is useful when you want to study
                        the effect of different transforms. Defaults to False.
  --video-augmentation.resize.interpolation {nearest,bilinear,bicubic}
                        Interpolation for resizing. Defaults to bilinear
  --video-augmentation.resize.size VIDEO_AUGMENTATION.RESIZE.SIZE [VIDEO_AUGMENTATION.RESIZE.SIZE ...]
                        Resize video to the specified size. If int is passed,
                        then shorter side is resized to the specified size and
                        longest side is resized while maintaining aspect
                        ratio. Defaults to None.

CropByBoundingBox:
  --video-augmentation.crop-by-bounding-box.enable
                        Use CropByBoundingBox. This flag is useful when you
                        want to study the effect of different transforms.
                        Default to False.
  --video-augmentation.crop-by-bounding-box.image-size VIDEO_AUGMENTATION.CROP_BY_BOUNDING_BOX.IMAGE_SIZE
                        Sizes [height, width] of the video frames after
                        cropping. Defaults to None
  --video-augmentation.crop-by-bounding-box.channel-first
                        If true, the video shape is [N, C, T, H, W].
                        Otherwise: [N, T, C, H, W]. Defaults to False.
  --video-augmentation.crop-by-bounding-box.multiplier-range VIDEO_AUGMENTATION.CROP_BY_BOUNDING_BOX.MULTIPLIER_RANGE VIDEO_AUGMENTATION.CROP_BY_BOUNDING_BOX.MULTIPLIER_RANGE
                        The bounding boxes get randomly expanded within the
                        range before cropping. Useful for zooming in/out.
                        Default None means no expansion of the bounding box.
  --video-augmentation.crop-by-bounding-box.multiplier VIDEO_AUGMENTATION.CROP_BY_BOUNDING_BOX.MULTIPLIER
                        The bounding boxes get expanded by this multiplier
                        before cropping. Useful for zooming in/out. Defaults
                        to 1.
  --video-augmentation.crop-by-bounding-box.interpolation {nearest,bilinear,bicubic}
                        Desired interpolation method. Defaults to bilinear.

ShuffleAudios:
  --video-augmentation.shuffle-audios.shuffle-ratio-train VIDEO_AUGMENTATION.SHUFFLE_AUDIOS.SHUFFLE_RATIO_TRAIN
                        Ratio of training videos with shuffled audio samples.
                        Defaults to 0.5.
  --video-augmentation.shuffle-audios.shuffle-ratio-val VIDEO_AUGMENTATION.SHUFFLE_AUDIOS.SHUFFLE_RATIO_VAL
                        Ratio of validation videos with shuffled audio
                        samples. Defaults to 0.5.
  --video-augmentation.shuffle-audios.shuffle-ratio-test VIDEO_AUGMENTATION.SHUFFLE_AUDIOS.SHUFFLE_RATIO_TEST
                        Ratio of test videos with shuffled audio samples.
                        Defaults to 0.5.
  --video-augmentation.shuffle-audios.generate-frame-level-targets
                        If true, the generated targets will be 2-dimensional
                        (n_clips x n_frames). Otherwise, targets will be 1
                        dimensional (n_clips). Defaults to False.
  --video-augmentation.shuffle-audios.target-key VIDEO_AUGMENTATION.SHUFFLE_AUDIOS.TARGET_KEY
                        Defaults to 'is_shuffled'. Name of the sub-key in
                        data['targets'] to store the labels tensor. For each
                        clip index `i`, we will have
                        data['targets']['is_shuffled'][i] == 0 iff audio of
                        the clip matches the video, otherwise 1.
  --video-augmentation.shuffle-audios.debug-mode
                        If enabled, the permutation used for shuffling the
                        clip audios will be added to data['samples']['metadata
                        ']['shuffled_audio_permutation'] for debugging
                        purposes. Defaults to False.

BaseAVReader:
  --video-reader.name VIDEO_READER.NAME
                        Name of video reader.
  --video-reader.fast-video-decoding
                        Multi-threaded fast video decoding using pyav.
  --video-reader.frame-stack-format {sequence_first,channel_first}
                        Sequence first (NTCHW) or channel first (NCTHW) format
                        for stacking video frames.

BaseCriteria:
  --loss.category LOSS.CATEGORY
                        Loss function category (e.g., classification).
                        Defaults to None.

BaseClassificationCriteria:
  --loss.classification.name LOSS.CLASSIFICATION.NAME
                        Name of the loss function in
                        BaseClassificationCriteria. Defaults to None.

BinaryCrossEntropy:
  --loss.classification.binary-cross-entropy.reduction {sum,mean,none,batch_mean}
                        Specifies the reduction to apply to the output
                        (default='mean'). 'batch_mean' divides the sum of the
                        loss only by the first dimension.

CrossEntropy:
  --loss.classification.cross-entropy.class-weights
                        Use class weights in CrossEntropy. Defaults to False.
  --loss.classification.cross-entropy.ignore-index LOSS.CLASSIFICATION.CROSS_ENTROPY.IGNORE_INDEX
                        Target value that is ignored and does not contribute
                        to the input gradient in CrossEntropy. Defaults to -1.
  --loss.classification.cross-entropy.label-smoothing LOSS.CLASSIFICATION.CROSS_ENTROPY.LABEL_SMOOTHING
                        Specifies the amount of smoothing when computing the
                        loss in CrossEntropy, where 0.0 means no smoothing.
                        Defaults to 0.0.

FocalLoss:
  --loss.classification.focal-loss.gamma LOSS.CLASSIFICATION.FOCAL_LOSS.GAMMA
                        Gamma of focal loss. Defaults to 0 and it's equvilent
                        to CE loss.
  --loss.classification.focal-loss.weights [LOSS.CLASSIFICATION.FOCAL_LOSS.WEIGHTS ...]
                        Weights for FocalLoss. Defaults to None.

CompositeLoss:
  --loss.composite-loss LOSS.COMPOSITE_LOSS

BaseDetectionCriteria:
  --loss.detection.name LOSS.DETECTION.NAME
                        Name of the loss function in BaseDetectionCriteria.
                        Defaults to None.

MaskRCNNLoss:
  --loss.detection.mask-rcnn-loss.classifier-weight LOSS.DETECTION.MASK_RCNN_LOSS.CLASSIFIER_WEIGHT
                        Weight for classifier in MaskRCNNLoss. Defaults to 1.
  --loss.detection.mask-rcnn-loss.box-reg-weight LOSS.DETECTION.MASK_RCNN_LOSS.BOX_REG_WEIGHT
                        Weight for box reg in MaskRCNNLoss. Defaults to 1.
  --loss.detection.mask-rcnn-loss.mask-weight LOSS.DETECTION.MASK_RCNN_LOSS.MASK_WEIGHT
                        Weight for mask in MaskRCNNLoss. Defaults to 1.
  --loss.detection.mask-rcnn-loss.objectness-weight LOSS.DETECTION.MASK_RCNN_LOSS.OBJECTNESS_WEIGHT
                        Weight for objectness in MaskRCNNLoss. Defaults to 1.
  --loss.detection.mask-rcnn-loss.rpn-box-reg LOSS.DETECTION.MASK_RCNN_LOSS.RPN_BOX_REG
                        Weight for rpn box reg. in MaskRCNNLoss. Defaults to
                        1.

SSDLoss:
  --loss.detection.ssd-multibox-loss.neg-pos-ratio LOSS.DETECTION.SSD_MULTIBOX_LOSS.NEG_POS_RATIO
                        Negative positive ratio in SSDLoss. Defaults to 3.
  --loss.detection.ssd-multibox-loss.max-monitor-iter LOSS.DETECTION.SSD_MULTIBOX_LOSS.MAX_MONITOR_ITER
                        Number of iterations for monitoring location and
                        classification loss in SSDLoss. -1 means do not
                        monitor. Defaults to -1.
  --loss.detection.ssd-multibox-loss.update-wt-freq LOSS.DETECTION.SSD_MULTIBOX_LOSS.UPDATE_WT_FREQ
                        Update the weights after N number of iterations in
                        SSDLoss. Defaults to 200 iterations.
  --loss.detection.ssd-multibox-loss.label-smoothing LOSS.DETECTION.SSD_MULTIBOX_LOSS.LABEL_SMOOTHING
                        Specifies the amount of smoothing when computing the
                        classification loss in SSDLoss, where 0.0 means no
                        smoothing. Defaults to 0.0.

BaseDistillationCriteria:
  --loss.distillation.name LOSS.DISTILLATION.NAME
                        Name of the loss function. Defaults to None.

HardDistillationLoss:
  --loss.distillation.hard-distillation.topk LOSS.DISTILLATION.HARD_DISTILLATION.TOPK
                        Distill top-k labels from teacher when in
                        HardDistillationLoss. Defaults to 1.
  --loss.distillation.hard-distillation.label-smoothing LOSS.DISTILLATION.HARD_DISTILLATION.LABEL_SMOOTHING
                        Specifies the amount of smoothing when computing the
                        classification loss in HardDistillationLoss, where 0.0
                        means no smoothing. Defaults to 0.0.

SoftKLLoss:
  --loss.distillation.soft-kl-loss.temperature LOSS.DISTILLATION.SOFT_KL_LOSS.TEMPERATURE
                        Temperature for KL divergence loss in SoftKLLoss.
                        Defaults to 1.

BaseLanguageModelingCriteria:
  --loss.language-modeling.name LOSS.LANGUAGE_MODELING.NAME
                        Name of the loss function in
                        BaseLanguageModelingCriteria. Defaults to None.

CrossEntropyLM:
  --loss.language-modeling.cross-entropy.ignore-index LOSS.LANGUAGE_MODELING.CROSS_ENTROPY.IGNORE_INDEX
                        Target value that is ignored and does not contribute
                        to the input gradient in CrossEntropyLM. Defaults to
                        -1.
  --loss.language-modeling.cross-entropy.label-smoothing LOSS.LANGUAGE_MODELING.CROSS_ENTROPY.LABEL_SMOOTHING
                        Specifies the amount of smoothing when computing the
                        loss in CrossEntropyLM, where 0.0 means no smoothing.
                        Defaults to 0.0.
  --loss.language-modeling.cross-entropy.use-z-loss
                        Use z-loss with cross-entropy loss. Defaults to False.
  --loss.language-modeling.cross-entropy.z-loss-eps LOSS.LANGUAGE_MODELING.CROSS_ENTROPY.Z_LOSS_EPS
                        Epsilon value for z-loss. Defaults to 0.0001.

CrossEntropyForKVPrediction:
  --loss.language-modeling.cross-entropy-for-kv-prediction.ignore-index LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.IGNORE_INDEX
                        Target value that is ignored and does not contribute
                        to the input gradient in CrossEntropyForKVPrediction.
                        Defaults to -1.
  --loss.language-modeling.cross-entropy-for-kv-prediction.label-smoothing LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.LABEL_SMOOTHING
                        Specifies the amount of smoothing when computing the
                        loss in CrossEntropyForKVPrediction, where 0.0 means
                        no smoothing. Defaults to 0.0.
  --loss.language-modeling.cross-entropy-for-kv-prediction.use-z-loss
                        Use z-loss with cross-entropy loss. Defaults to False.
  --loss.language-modeling.cross-entropy-for-kv-prediction.z-loss-eps LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.Z_LOSS_EPS
                        Epsilon value for z-loss. Defaults to 0.0001.
  --loss.language-modeling.cross-entropy-for-kv-prediction.auxiliary-loss LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.AUXILIARY_LOSS
                        Multiplicative factor for auxiliary loss.
  --loss.language-modeling.cross-entropy-for-kv-prediction.base-loss LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.BASE_LOSS
                        Multiplicative factor for base loss.
  --loss.language-modeling.cross-entropy-for-kv-prediction.kv-loss LOSS.LANGUAGE_MODELING.CROSS_ENTROPY_FOR_KV_PREDICTION.KV_LOSS
                        Multiplicative factor for KV loss.

BaseMultiModalImageTextCriteria:
  --loss.multi-modal-image-text.name LOSS.MULTI_MODAL_IMAGE_TEXT.NAME
                        Name of the loss function. Defaults to None.

NeuralAugmentation:
  --loss.neural-augmentation.perceptual-metric {psnr}
                        Name of the perceptual metric to be used in
                        NeuralAugmentation.
  --loss.neural-augmentation.target-value LOSS.NEURAL_AUGMENTATION.TARGET_VALUE [LOSS.NEURAL_AUGMENTATION.TARGET_VALUE ...]
                        Target image similarity value in NeuralAugmentation.
                        Defaults to [40, 20]
  --loss.neural-augmentation.curriculum-method {linear,cosine}
                        Curriculum for varying the target image similarity
                        value in NeuralAugmentation.Supported curriculums are
                        ['psnr']. Defaults to cosine
  --loss.neural-augmentation.alpha LOSS.NEURAL_AUGMENTATION.ALPHA
                        Scale loss value by alpha value. Defaults to 100.
                        Note: When perceptual metric is PSNR, alpha value is
                        divided by 65025

BaseSegmentationCriteria:
  --loss.segmentation.name LOSS.SEGMENTATION.NAME
                        Name of the loss function. Defaults to None.

SegCrossEntropy:
  --loss.segmentation.cross-entropy.class-weights
                        Use class weights in SegCrossEntropy. Defaults to
                        False.
  --loss.segmentation.cross-entropy.ignore-index LOSS.SEGMENTATION.CROSS_ENTROPY.IGNORE_INDEX
                        Target value that is ignored and does not contribute
                        to the input gradient in SegCrossEntropy. Defaults to
                        -1.
  --loss.segmentation.cross-entropy.aux-weight LOSS.SEGMENTATION.CROSS_ENTROPY.AUX_WEIGHT
                        Weight of auxiliary segmentation loss. Defaults to
                        0.4.
  --loss.segmentation.cross-entropy.label-smoothing LOSS.SEGMENTATION.CROSS_ENTROPY.LABEL_SMOOTHING
                        Specifies the amount of smoothing when computing the
                        loss in SegCrossEntropy, where 0.0 means no smoothing.
                        Defaults to 0.0.

BaseOptim:
  --optim.name OPTIM.NAME
                        Name of the optimizer. Defaults to SGD.
  --optim.eps OPTIM.EPS
                        Optimizer epsilon value. Defaults to 1.e-8.
  --optim.weight-decay OPTIM.WEIGHT_DECAY
                        Weight decay (or L2 penalty). Defaults to 4.e-5.
  --optim.no-decay-bn-filter-bias
                        When enabled, the weight in normalization layers and
                        biases in the model are not decayed.Defaults to False.
  --optim.bypass-parameters-check
                        Bypass parameter check when creating optimizer.
                        Defaults to False

AdamOptimizer:
  --optim.adam.beta1 OPTIM.ADAM.BETA1
                        Value of Beta1 in ADAM optimizer. Defaults to 0.9.
  --optim.adam.beta2 OPTIM.ADAM.BETA2
                        Value of Beta2 in ADAM optimizer. Defaults to 0.98.
  --optim.adam.amsgrad  Use AMSGrad in ADAM. Defaults to False.
  --optim.adam.eps OPTIM.ADAM.EPS
                        Value of epsilon in Adam optimizer. Defaults to
                        None.When this value is None, the default value in
                        base optimizer is used.

AdamWOptimizer:
  --optim.adamw.beta1 OPTIM.ADAMW.BETA1
                        Value of Beta1 in AdamW optimizer. Defaults to 0.9.
  --optim.adamw.beta2 OPTIM.ADAMW.BETA2
                        Value of Beta2 in AdamW optimizer. Defaults to 0.98.
  --optim.adamw.amsgrad
                        Use AMSGrad in AdamW. Defaults to False.
  --optim.adamw.eps OPTIM.ADAMW.EPS
                        Value of epsilon in AdamW optimizer. Defaults to
                        None.When this value is None, the default value in
                        base optimizer is used.

SGDOptimizer:
  --optim.sgd.momentum OPTIM.SGD.MOMENTUM
                        The value of momemtum in SGD. Defaults to 0.9
  --optim.sgd.nesterov  Use nesterov momentum in SGD. Defaults to False.

LR scheduler arguments:
  LR scheduler arguments

  --scheduler.name SCHEDULER.NAME
                        LR scheduler name
  --scheduler.lr SCHEDULER.LR
                        Learning rate
  --scheduler.max-epochs SCHEDULER.MAX_EPOCHS
                        Max. epochs for training
  --scheduler.max-iterations SCHEDULER.MAX_ITERATIONS
                        Max. iterations for training
  --scheduler.warmup-iterations SCHEDULER.WARMUP_ITERATIONS
                        Warm-up iterations
  --scheduler.warmup-init-lr SCHEDULER.WARMUP_INIT_LR
                        Warm-up init lr
  --scheduler.is-iteration-based
                        Is iteration type or epoch type
  --scheduler.adjust-period-for-epochs
                        Adjust the period for epoch-based scheduler.

Cosine LR arguments:
  Cosine LR arguments

  --scheduler.cosine.min-lr SCHEDULER.COSINE.MIN_LR
                        Minimum LR in Cosine LR scheduler
  --scheduler.cosine.max-lr SCHEDULER.COSINE.MAX_LR
                        Maximum LR in Cosine LR scheduler

Cyclic LR arguments:
  Cyclic LR arguments

  --scheduler.cyclic.min-lr SCHEDULER.CYCLIC.MIN_LR
                        Min. lr for a cycle
  --scheduler.cyclic.last-cycle-end-lr SCHEDULER.CYCLIC.LAST_CYCLE_END_LR
                        End LR for the last cycle
  --scheduler.cyclic.total-cycles SCHEDULER.CYCLIC.TOTAL_CYCLES
                        Number of cycles. Default is 10
  --scheduler.cyclic.epochs-per-cycle SCHEDULER.CYCLIC.EPOCHS_PER_CYCLE
                        Number of epochs per cycle. Default is 5
  --scheduler.cyclic.steps SCHEDULER.CYCLIC.STEPS [SCHEDULER.CYCLIC.STEPS ...]
                        steps at which LR should be decreased
  --scheduler.cyclic.gamma SCHEDULER.CYCLIC.GAMMA
                        Factor by which LR should be decreased
  --scheduler.cyclic.last-cycle-type {cosine,linear}
                        Annealing in last cycle

Fixed LR arguments:
  Fixed LR arguments

  --scheduler.fixed.lr SCHEDULER.FIXED.LR
                        LR value

MultiStepLRScheduler arguments:
  MultiStepLRScheduler arguments

  --scheduler.multi-step.lr SCHEDULER.MULTI_STEP.LR
                        LR value
  --scheduler.multi-step.gamma SCHEDULER.MULTI_STEP.GAMMA
                        Decay LR value by this factor
  --scheduler.multi-step.milestones SCHEDULER.MULTI_STEP.MILESTONES [SCHEDULER.MULTI_STEP.MILESTONES ...]
                        Decay LR value at these epoch

Polynomial LR arguments:
  Polynomial LR arguments

  --scheduler.polynomial.power SCHEDULER.POLYNOMIAL.POWER
                        Polynomial power
  --scheduler.polynomial.start-lr SCHEDULER.POLYNOMIAL.START_LR
                        Start LR in Poly LR scheduler
  --scheduler.polynomial.end-lr SCHEDULER.POLYNOMIAL.END_LR
                        End LR in Poly LR scheduler

DDP arguments:
  --ddp.rank DDP.RANK   Node rank for distributed training. Defaults to 0.
  --ddp.world-size DDP.WORLD_SIZE
                        World size for DDP. Defaults to -1, meaning use all
                        GPUs.
  --ddp.dist-url DDP.DIST_URL
                        DDP URL. Defaults to None.
  --ddp.dist-port DDP.DIST_PORT
                        DDP Port. Only used when --ddp.dist-url is not
                        specified. Defaults to 30768.
  --ddp.device-id DDP.DEVICE_ID
                        Device ID. Defaults to None.
  --ddp.backend DDP.BACKEND
                        DDP backend. Default is nccl
  --ddp.find-unused-params
                        Find unused params in model. useful for debugging with
                        DDP. Defaults to False.
  --ddp.use-deprecated-data-parallel
                        Use Data parallel for training. This flag is not
                        recommended for training and should be used only for
                        debugging. The support for this flag will be
                        deprecating in future.

Statistics:
  Statistics

  --stats.val STATS.VAL [STATS.VAL ...]
                        Name of statistics
  --stats.train STATS.TRAIN [STATS.TRAIN ...]
                        Name of statistics
  --stats.checkpoint-metric STATS.CHECKPOINT_METRIC
                        Metric to use for saving checkpoints
  --stats.checkpoint-metric-max
                        Maximize checkpoint metric
  --stats.coco-map.iou-types {bbox,segm} [{bbox,segm} ...]
                        Types of IOU to compute for MSCoco.

Common arguments:
  Common arguments

  --taskname TASKNAME   Name of the task (can have arbitrary values)
  --common.seed COMMON.SEED
                        Random seed
  --common.config-file COMMON.CONFIG_FILE
                        Configuration file
  --common.results-loc COMMON.RESULTS_LOC
                        Directory where results will be stored. Defaults to
                        results.
  --common.logs-loc COMMON.LOGS_LOC
                        Directory where logs will be stored. Defaults to
                        results/logs.
  --common.run-label COMMON.RUN_LABEL
                        Label id for the current run
  --common.eval-stage-name COMMON.EVAL_STAGE_NAME
                        Name to be used while logging in evaluation stage.
  --common.resume COMMON.RESUME
                        Resume location
  --common.finetune COMMON.FINETUNE
                        Checkpoint location to be used for finetuning
  --common.finetune-ema COMMON.FINETUNE_EMA
                        EMA Checkpoint location to be used for finetuning
  --common.mixed-precision
                        Enable mixed precision training. Defaults to False.
                        Note that this argument is not applicable for FSDP
                        training. For mixed precision training with FSDP,
                        please see respective arguments in FSDP.
  --common.mixed-precision-dtype COMMON.MIXED_PRECISION_DTYPE
                        Mixed precision training data type
  --common.accum-freq COMMON.ACCUM_FREQ
                        Accumulate gradients for this number of iterations
  --common.accum-after-epoch COMMON.ACCUM_AFTER_EPOCH
                        Start accumulation after this many epochs
  --common.log-freq COMMON.LOG_FREQ
                        Display after these many iterations
  --common.auto-resume  Resume training from the last checkpoint
  --common.grad-clip COMMON.GRAD_CLIP
                        Gradient clipping value
  --common.k-best-checkpoints COMMON.K_BEST_CHECKPOINTS
                        Keep k-best checkpoints
  --common.save-all-checkpoints
                        If True, will save checkpoints from all epochs
  --common.channels-last
                        Use channel last format during training. Note that
                        some models may not support it, so we recommend to use
                        it with caution.
  --common.tensorboard-logging
                        Enable tensorboard logging
  --common.file-logging
                        Enable file logging.
  --common.override-kwargs [COMMON.OVERRIDE_KWARGS ...]
                        Override arguments. Example. To override the value of
                        --sampler.vbs.crop-size-width, we can pass override
                        argument as --common.override-kwargs
                        sampler.vbs.crop_size_width=512 Note that keys in
                        override arguments do not contain -- or -
  --common.enable-coreml-compatible-module
                        Use coreml compatible modules (if applicable) during
                        inference
  --common.debug-mode   You can use this flag for debugging purposes.
  --common.save-interval-freq COMMON.SAVE_INTERVAL_FREQ
                        Save checkpoints every N updates. Defaults to 0
  --common.eval-every-k-iterations COMMON.EVAL_EVERY_K_ITERATIONS
                        Evaluate model every k iterations. Defaults to 0.
  --common.set-grad-to-none
                        Set gradients to none instead of zero after
                        optimization step. This can help in reducing GPU
                        memory usage and can moderately improve training
                        speed. Defaults to False. Please be cautious when
                        computing grad_norm for debugging purposes.

BaseTextTokenizer:
  --text-tokenizer.name TEXT_TOKENIZER.NAME
                        Name of the text tokenizer (e.g., clip). Defaults to
                        None.
  --text-tokenizer.sot-token TEXT_TOKENIZER.SOT_TOKEN
                        Start of the text token. Defaults to None (i.e., users
                        must specify the value if it needs to be used.).
  --text-tokenizer.eot-token TEXT_TOKENIZER.EOT_TOKEN
                        End of the text token. Defaults to None (i.e., users
                        must specify the value if it needs to be used.).
  --text-tokenizer.pad-token TEXT_TOKENIZER.PAD_TOKEN
                        Pad token. Defaults to None (i.e., users must specify
                        the value if it needs to be used.).

ClipTokenizer:
  --text-tokenizer.clip.merges-path TEXT_TOKENIZER.CLIP.MERGES_PATH
                        Path to bpe merges file. Defaults to None.
  --text-tokenizer.clip.encoder-json-path TEXT_TOKENIZER.CLIP.ENCODER_JSON_PATH
                        Path to BPE encoder json file. This file is used to
                        infer `num_merges`. Defaults to None.

SentencePieceTokenizer:
  --text-tokenizer.sentence-piece.model-path TEXT_TOKENIZER.SENTENCE_PIECE.MODEL_PATH
                        Sentence piece model path. Defaults to None (i.e.,
                        user need to supply the model path).
  --text-tokenizer.sentence-piece.enable-nfc-normalization
                        Normalize the text using NFC normalization. This is
                        useful when pre-training. Defaults to False.
  --text-tokenizer.sentence-piece.append-sot-token
                        Append start of text token before tokenized text.
                        Defaults to False.
  --text-tokenizer.sentence-piece.append-eot-token
                        Append end of text token after tokenized text.
                        Defaults to False.

OpenAIClipTokenizer:
  --text-tokenizer.openai-clip.bpe-path TEXT_TOKENIZER.OPENAI_CLIP.BPE_PATH
                        Path to BPE file. Defaults to 'https://github.com/open
                        ai/CLIP/raw/a1d071733d7111c9c014f024669f959182114e33/c
                        lip/bpe_simple_vocab_16e6.txt.gz'.

TrainEvalPipeline:
  --train-eval-pipeline.name TRAIN_EVAL_PIPELINE.NAME
                        Name of the TrainEvalPipeline to use for constructing
                        pipeline components. Defaults to 'default' pipeline
                        (see corenet/train_eval_pipelines/train_eval.py)
